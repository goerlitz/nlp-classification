{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "21c_10kGNAD_huggingface_basic_optuna.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOONqycNLHAzb7z/eYH/F7G",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b021a4ce41b74fc4a198001bb3945f8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_503650c6ebe34c68a8190bf6927c056a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4846e4c77d3743848968dc9877cf313c",
              "IPY_MODEL_f8a13731f06e4d9096437b121b770626",
              "IPY_MODEL_e31d125213064de68c12f2eb20f18357"
            ]
          }
        },
        "503650c6ebe34c68a8190bf6927c056a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4846e4c77d3743848968dc9877cf313c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_08bd1537849d44deaf55473617c8908c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b4d18a471b5d424b958ffca7fadea962"
          }
        },
        "f8a13731f06e4d9096437b121b770626": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e8e9a79209a04509a58d1edfa62734f4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0531cf1614414ad481cea66d973aed36"
          }
        },
        "e31d125213064de68c12f2eb20f18357": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_466860e06af24e7bae6c31489c280333",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2/2 [00:00&lt;00:00, 53.57it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2fe9bf4a395d431696b2af761004d9e2"
          }
        },
        "08bd1537849d44deaf55473617c8908c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b4d18a471b5d424b958ffca7fadea962": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e8e9a79209a04509a58d1edfa62734f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0531cf1614414ad481cea66d973aed36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "466860e06af24e7bae6c31489c280333": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2fe9bf4a395d431696b2af761004d9e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goerlitz/nlp-classification/blob/main/%20nlp-classification/notebooks/10kGNAD/21c_10kGNAD_huggingface_basic_optuna.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrveYRYD2E9-"
      },
      "source": [
        "# Hyperparameter Optimization with HuggingFace Transformers\n",
        "\n",
        "Adapted from https://huggingface.co/docs/transformers/custom_datasets#sequence-classification-with-imdb-reviews\n",
        "\n",
        "Things we need\n",
        "* a tokenizer\n",
        "* tokenized input data\n",
        "* a pretrained model\n",
        "* evaluation metrics\n",
        "* training parameters\n",
        "* a Trainer instance\n",
        "\n",
        "Notes\n",
        "* [class labels can be included in the model config](https://github.com/huggingface/transformers/pull/2945#issuecomment-781986506) (a bit hacky)\n",
        "* [fp16 is disabled on tesla P100 GPU in pytorch](https://discuss.pytorch.org/t/cnn-fp16-slower-than-fp32-on-tesla-p100/12146)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JH5VyX6w9AIM"
      },
      "source": [
        "## Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2BZ9NGbmTkF"
      },
      "source": [
        "# checkpoint = \"distilbert-base-german-cased\"\n",
        "\n",
        "checkpoint = \"deepset/gbert-base\"\n",
        "\n",
        "# checkpoint = \"deepset/gelectra-base\"\n",
        "\n",
        "project_name = f'10kgnad_hf__{checkpoint.replace(\"/\", \"_\")}'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKqjLlGpWdsi"
      },
      "source": [
        "### Connect Google Drive\n",
        "\n",
        "Will be used to save results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YXxAbtZWcxA",
        "outputId": "00d5f197-f314-412f-bab5-1b79d23bfba8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# define model path\n",
        "root_path = Path('/content/gdrive/My Drive/')\n",
        "base_path = root_path / 'Colab Notebooks/nlp-classification/'\n",
        "model_path = base_path / 'models'"
      ],
      "metadata": {
        "id": "M3n8HqXwEyK6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check GPU"
      ],
      "metadata": {
        "id": "5F2oddvs2rNv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVWRNvQf1cnR",
        "outputId": "cb068d7a-5a7c-4cd3-c966-f2b718867a5a"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Dec 22 21:58:23 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P0    46W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwlw_kI3nHLW"
      },
      "source": [
        "### Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QhKj625lBso",
        "outputId": "14c85d13-e33a-4a70-8cf8-91da27bd828d"
      },
      "source": [
        "%%time\n",
        "!pip install -q -U transformers datasets >/dev/null\n",
        "!pip install -q -U optuna >/dev/null\n",
        "\n",
        "# check installed version\n",
        "!pip freeze | grep optuna        # optuna==2.10.0\n",
        "!pip freeze | grep transformers  # transformers==4.15.0\n",
        "!pip freeze | grep torch         # torch==1.10.0+cu111"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "optuna==2.10.0\n",
            "transformers==4.15.0\n",
            "torch @ https://download.pytorch.org/whl/cu111/torch-1.10.0%2Bcu111-cp37-cp37m-linux_x86_64.whl\n",
            "torchaudio @ https://download.pytorch.org/whl/cu111/torchaudio-0.10.0%2Bcu111-cp37-cp37m-linux_x86_64.whl\n",
            "torchsummary==1.5.1\n",
            "torchtext==0.11.0\n",
            "torchvision @ https://download.pytorch.org/whl/cu111/torchvision-0.11.1%2Bcu111-cp37-cp37m-linux_x86_64.whl\n",
            "CPU times: user 121 ms, sys: 45.4 ms, total: 167 ms\n",
            "Wall time: 15.6 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import logging\n",
        "\n",
        "# hide progress bar when downloading tokenizer and model (a workaround!)\n",
        "logging.get_verbosity = lambda : logging.NOTSET"
      ],
      "metadata": {
        "id": "vbpLzC7DRoR5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset"
      ],
      "metadata": {
        "id": "64Q8bFMM7UYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "gnad10k = load_dataset(\"gnad10\")\n",
        "label_names = gnad10k[\"train\"].features[\"label\"].names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "b021a4ce41b74fc4a198001bb3945f8e",
            "503650c6ebe34c68a8190bf6927c056a",
            "4846e4c77d3743848968dc9877cf313c",
            "f8a13731f06e4d9096437b121b770626",
            "e31d125213064de68c12f2eb20f18357",
            "08bd1537849d44deaf55473617c8908c",
            "b4d18a471b5d424b958ffca7fadea962",
            "e8e9a79209a04509a58d1edfa62734f4",
            "0531cf1614414ad481cea66d973aed36",
            "466860e06af24e7bae6c31489c280333",
            "2fe9bf4a395d431696b2af761004d9e2"
          ]
        },
        "id": "4aYzOljo7W9X",
        "outputId": "95ab5462-d340-47f4-b518-1e63b1bf888b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using custom data configuration default\n",
            "Reusing dataset gnad10 (/root/.cache/huggingface/datasets/gnad10/default/1.1.0/3a8445be65795ad88270af4d797034c3d99f70f8352ca658c586faf1cf960881)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b021a4ce41b74fc4a198001bb3945f8e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(gnad10k)\n",
        "print(\"labels:\", label_names)\n",
        "gnad10k[\"train\"][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkcJyc-I8bLP",
        "outputId": "02540b69-c73a-4c40-fa7a-dd32e39812a2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 9245\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 1028\n",
            "    })\n",
            "})\n",
            "labels: ['Web', 'Panorama', 'International', 'Wirtschaft', 'Sport', 'Inland', 'Etat', 'Wissenschaft', 'Kultur']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': 4,\n",
              " 'text': '21-Jähriger fällt wohl bis Saisonende aus. Wien – Rapid muss wohl bis Saisonende auf Offensivspieler Thomas Murg verzichten. Der im Winter aus Ried gekommene 21-Jährige erlitt beim 0:4-Heimdebakel gegen Admira Wacker Mödling am Samstag einen Teilriss des Innenbandes im linken Knie, wie eine Magnetresonanz-Untersuchung am Donnerstag ergab. Murg erhielt eine Schiene, muss aber nicht operiert werden. Dennoch steht ihm eine mehrwöchige Pause bevor.'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "* Loading the same Tokenizer that was used with the pretrained model.\n",
        "* Define function to tokenize the text (with truncation to max input length of model.\n",
        "* Run the tokenization"
      ],
      "metadata": {
        "id": "4EUOwFbFPXCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True)\n",
        "\n",
        "tokenized_gnad10k = gnad10k.map(preprocess_function, batched=True).remove_columns(\"text\")"
      ],
      "metadata": {
        "id": "u7AJ7SGjPaZ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16e3d641-ffc7-4c72-a0b1-e2c0ad62262a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/gnad10/default/1.1.0/3a8445be65795ad88270af4d797034c3d99f70f8352ca658c586faf1cf960881/cache-bc6a8b06dc72b2a1.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/gnad10/default/1.1.0/3a8445be65795ad88270af4d797034c3d99f70f8352ca658c586faf1cf960881/cache-2c4a7f3fbc1e560e.arrow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use Dynamic Padding\n",
        "\n",
        "Apply panding only on longest text in batch - this is more efficient than applying padding on the whole dataset."
      ],
      "metadata": {
        "id": "kdmFxnHlSV6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "DhEWeeg2SVCe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Setup\n",
        "\n",
        "We want to include the label names and save them together with the model.\n",
        "The only way to do this is to create a Config and put them in. "
      ],
      "metadata": {
        "id": "Eqe_fA8muv5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
        "\n",
        "config = AutoConfig.from_pretrained(\n",
        "        checkpoint,\n",
        "        num_labels=len(label_names),\n",
        "        id2label={i: label for i, label in enumerate(label_names)},\n",
        "        label2id={label: i for i, label in enumerate(label_names)},\n",
        "        )\n",
        "\n",
        "def model_init(trial: optuna.Trial):\n",
        "    \"\"\"A function that instantiates the model to be used.\"\"\"\n",
        "    return AutoModelForSequenceClassification.from_pretrained(checkpoint, config=config)"
      ],
      "metadata": {
        "id": "kHPXNqxCgyxQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Evaluation Metrics\n",
        "\n",
        "The funtion that computes the metrics needs to be passed to the Trainer."
      ],
      "metadata": {
        "id": "ufbNYVSIxuhv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, matthews_corrcoef\n",
        "import numpy as np\n",
        "from typing import Dict\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    \"\"\"The function that will be used to compute metrics at evaluation.\n",
        "    Must take a :class:`~transformers.EvalPrediction` and return a dictionary\n",
        "    string to metric values.\"\"\"\n",
        "    logits, labels = eval_preds\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        \"acc\": accuracy_score(labels, preds),\n",
        "        \"f1\": f1_score(labels, preds, average='macro'),\n",
        "        \"precision\": precision_score(labels, preds, average='macro'),\n",
        "        \"recall\": recall_score(labels, preds, average='macro'),\n",
        "        \"mcc\": matthews_corrcoef(labels, preds),\n",
        "        }\n",
        "\n",
        "\n",
        "def objective(metrics: Dict[str, float]):\n",
        "    \"\"\"A function computing the main optimization objective from the metrics\n",
        "    returned by the :obj:`compute_metrics` method.\n",
        "    To be used in :obj:`Trainer.hyperparameter_search`.\"\"\"\n",
        "    return metrics[\"eval_loss\"]"
      ],
      "metadata": {
        "id": "Hpj1PZYjxtqS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9HHIeJrM3_I"
      },
      "source": [
        "## Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainerCallback\n",
        "\n",
        "# https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/trainer_callback.py#L505\n",
        "\n",
        "class TrialPruningCallback(TrainerCallback):\n",
        "    \"\"\"\n",
        "    A :class:`~transformers.TrainerCallback` that handles pruning.\n",
        "    Args:\n",
        "       trial:\n",
        "            the current trial.\n",
        "       objective_metric(:obj:`str`, `optional`):\n",
        "            the metrics used for pruning.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, trial: optuna.Trial, objective_metric: str = \"eval_loss\"):\n",
        "        self.trial = trial\n",
        "        self.metric = objective_metric\n",
        "\n",
        "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
        "        self.trial.report(metrics[self.metric], step=state.global_step)\n",
        "        if self.trial.should_prune():\n",
        "            control.should_training_stop = True\n",
        "            print(\"pruning trial\")"
      ],
      "metadata": {
        "id": "3_9Al_rbBrla"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "import shutil\n",
        "\n",
        "def hp_space(trial: optuna.Trial):\n",
        "    \"\"\"A function that defines the hyperparameter search space.\n",
        "    To be used in :obj:`Trainer.hyperparameter_search`.\"\"\"\n",
        "    return {\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-4, log=True),\n",
        "        \"num_train_epochs\": trial.suggest_categorical(\"num_train_epochs\", [2]),\n",
        "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16]),\n",
        "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-3, 1e-2, log=True),\n",
        "        \"label_smoothing_factor\": trial.suggest_float(\"label_smoothing_factor\", 0.0, 0.1),\n",
        "    }\n",
        "\n",
        "best_model_dir = \"best_model_trainer\"\n",
        "\n",
        "def best_model_callback(study, trial):\n",
        "    \"\"\"Save the model from a best trial\"\"\"\n",
        "    for t in study.best_trials:\n",
        "        if t.number == trial.number:\n",
        "            print(\"This is a new besttrial\", trial.number)\n",
        "        \n",
        "            out_filename = model_path / f\"{project_name}_t{trial.number}\"\n",
        "            shutil.make_archive(out_filename, 'zip', f\"{project_name}/{best_model_dir}\")\n",
        "\n",
        "def train(trial: optuna.Trial):\n",
        "\n",
        "    # get hyperparameters choice\n",
        "    hp = hp_space(trial)\n",
        "    lr = hp[\"learning_rate\"]\n",
        "    bs = hp[\"per_device_train_batch_size\"]\n",
        "    epochs = hp[\"num_train_epochs\"]\n",
        "    weight_decay = hp[\"weight_decay\"]\n",
        "    label_smoothing_factor = hp[\"label_smoothing_factor\"]\n",
        "\n",
        "    eval_rounds_per_epoch = 5\n",
        "    eval_steps = gnad10k[\"train\"].num_rows / bs // eval_rounds_per_epoch\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=str(project_name),\n",
        "        report_to=[],\n",
        "        log_level=\"error\",\n",
        "        disable_tqdm=False,\n",
        "\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=eval_steps,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=eval_steps,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        greater_is_better=False,\n",
        "\n",
        "        # hyperparameters\n",
        "        num_train_epochs=epochs,\n",
        "        learning_rate=lr,\n",
        "        per_device_train_batch_size=bs,\n",
        "        per_device_eval_batch_size=bs,\n",
        "        weight_decay=weight_decay,\n",
        "        label_smoothing_factor=label_smoothing_factor,\n",
        "\n",
        "        # fp16=True,  # fp16 is disabled on Tesla P100 by pytorch\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model_init=model_init,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_gnad10k[\"train\"],\n",
        "        eval_dataset=tokenized_gnad10k[\"test\"],\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "        # callbacks=[TrialPruningCallback(trial)]\n",
        "    )\n",
        "\n",
        "    # train model and save best model from evaluations\n",
        "    # needs 'load_best_model_at_end=True'\n",
        "    trainer.train()\n",
        "    trainer.save_model(f\"{project_name}/{best_model_dir}\")\n",
        "\n",
        "    result = trainer.evaluate(eval_dataset=tokenized_gnad10k[\"test\"])\n",
        "\n",
        "    # store eval metrics in trial\n",
        "    for key in result.keys():\n",
        "        if key != \"epoch\":\n",
        "            trial.set_user_attr(key, result[key])\n",
        "    \n",
        "    return result[\"eval_loss\"]#, result[\"eval_mcc\"]\n",
        "\n",
        "\n",
        "db_path = \"/content/gdrive/My Drive/Colab Notebooks/nlp-classification/\"\n",
        "db_name = \"10kgnad_optuna\"\n",
        "# study_name = checkpoint + \"_multi_epoch234\"\n",
        "study_name = checkpoint + \"_epoch2_bs16\"\n",
        "\n",
        "# multi objective study\n",
        "# https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/002_multi_objective.html#sphx-glr-tutorial-20-recipes-002-multi-objective-py\n",
        "study = optuna.create_study(study_name=study_name,\n",
        "                            # directions=[\"minimize\", \"maximize\"],\n",
        "                            directions=[\"minimize\"],\n",
        "                            storage=f\"sqlite:///{db_path}{db_name}.db\",\n",
        "                            load_if_exists=True,)\n",
        "\n",
        "# give some hyperparameters that are presumably good\n",
        "study.enqueue_trial(\n",
        "    {\n",
        "        \"learning_rate\": 8e-5,\n",
        "        \"weight_decay\": 1e-3,\n",
        "        \"label_smoothing_factor\": 0.0,\n",
        "    }\n",
        ")\n",
        "study.enqueue_trial(\n",
        "    {\n",
        "        \"learning_rate\": 7e-5,\n",
        "        \"weight_decay\": 1e-3,\n",
        "        \"label_smoothing_factor\": 1e-5,\n",
        "    }\n",
        ")\n",
        "\n",
        "study.optimize(train, n_trials=100, callbacks=[best_model_callback])\n",
        "\n",
        "# study.best_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YqjuJ-2tRMYQ",
        "outputId": "6da6b61c-d12f-4339-c1e1-026be892b06c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2021-12-22 21:59:23,780]\u001b[0m Using an existing study with name 'deepset/gbert-base_epoch2_bs16' instead of creating a new one.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:108: ExperimentalWarning:\n",
            "\n",
            "enqueue_trial is experimental (supported from v1.2.0). The interface can change in the future.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/optuna/study/study.py:857: ExperimentalWarning:\n",
            "\n",
            "create_trial is experimental (supported from v2.0.0). The interface can change in the future.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/optuna/study/study.py:857: ExperimentalWarning:\n",
            "\n",
            "add_trial is experimental (supported from v2.0.0). The interface can change in the future.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:115: ExperimentalWarning:\n",
            "\n",
            "enqueue_trial is experimental (supported from v1.2.0). The interface can change in the future.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1156' max='1156' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1156/1156 20:45, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.547677</td>\n",
              "      <td>0.824903</td>\n",
              "      <td>0.820948</td>\n",
              "      <td>0.810441</td>\n",
              "      <td>0.844179</td>\n",
              "      <td>0.802195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.468286</td>\n",
              "      <td>0.862840</td>\n",
              "      <td>0.861329</td>\n",
              "      <td>0.858133</td>\n",
              "      <td>0.874075</td>\n",
              "      <td>0.844412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>345</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.430013</td>\n",
              "      <td>0.860895</td>\n",
              "      <td>0.853121</td>\n",
              "      <td>0.855927</td>\n",
              "      <td>0.859246</td>\n",
              "      <td>0.841882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.465177</td>\n",
              "      <td>0.857004</td>\n",
              "      <td>0.859499</td>\n",
              "      <td>0.871234</td>\n",
              "      <td>0.865131</td>\n",
              "      <td>0.839272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>575</td>\n",
              "      <td>0.555900</td>\n",
              "      <td>0.359662</td>\n",
              "      <td>0.889105</td>\n",
              "      <td>0.884626</td>\n",
              "      <td>0.893786</td>\n",
              "      <td>0.878359</td>\n",
              "      <td>0.873067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>0.555900</td>\n",
              "      <td>0.441582</td>\n",
              "      <td>0.884241</td>\n",
              "      <td>0.884508</td>\n",
              "      <td>0.886481</td>\n",
              "      <td>0.889913</td>\n",
              "      <td>0.868743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>805</td>\n",
              "      <td>0.555900</td>\n",
              "      <td>0.377918</td>\n",
              "      <td>0.911479</td>\n",
              "      <td>0.906674</td>\n",
              "      <td>0.900708</td>\n",
              "      <td>0.914103</td>\n",
              "      <td>0.898788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>0.555900</td>\n",
              "      <td>0.362580</td>\n",
              "      <td>0.902724</td>\n",
              "      <td>0.897148</td>\n",
              "      <td>0.898869</td>\n",
              "      <td>0.899967</td>\n",
              "      <td>0.889004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1035</td>\n",
              "      <td>0.246000</td>\n",
              "      <td>0.363838</td>\n",
              "      <td>0.909533</td>\n",
              "      <td>0.906437</td>\n",
              "      <td>0.905549</td>\n",
              "      <td>0.908925</td>\n",
              "      <td>0.896593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.246000</td>\n",
              "      <td>0.333588</td>\n",
              "      <td>0.908560</td>\n",
              "      <td>0.906827</td>\n",
              "      <td>0.905503</td>\n",
              "      <td>0.909297</td>\n",
              "      <td>0.895321</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='65' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [65/65 00:19]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2021-12-22 22:20:36,491]\u001b[0m Trial 68 finished with value: 0.33358800411224365 and parameters: {'learning_rate': 8e-05, 'num_train_epochs': 2, 'per_device_train_batch_size': 16, 'weight_decay': 0.001, 'label_smoothing_factor': 0.0}. Best is trial 23 with value: 0.3157237470149994.\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='806' max='1156' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 806/1156 14:04 < 06:07, 0.95 it/s, Epoch 1.39/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.523762</td>\n",
              "      <td>0.839494</td>\n",
              "      <td>0.835718</td>\n",
              "      <td>0.826005</td>\n",
              "      <td>0.854023</td>\n",
              "      <td>0.817741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.483270</td>\n",
              "      <td>0.851167</td>\n",
              "      <td>0.850635</td>\n",
              "      <td>0.852900</td>\n",
              "      <td>0.860573</td>\n",
              "      <td>0.831095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>345</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.399892</td>\n",
              "      <td>0.883268</td>\n",
              "      <td>0.876011</td>\n",
              "      <td>0.867338</td>\n",
              "      <td>0.891701</td>\n",
              "      <td>0.867385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.404428</td>\n",
              "      <td>0.875486</td>\n",
              "      <td>0.870166</td>\n",
              "      <td>0.872672</td>\n",
              "      <td>0.875391</td>\n",
              "      <td>0.858569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>575</td>\n",
              "      <td>0.555900</td>\n",
              "      <td>0.337040</td>\n",
              "      <td>0.891051</td>\n",
              "      <td>0.887467</td>\n",
              "      <td>0.891150</td>\n",
              "      <td>0.884768</td>\n",
              "      <td>0.875139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>0.555900</td>\n",
              "      <td>0.420522</td>\n",
              "      <td>0.884241</td>\n",
              "      <td>0.882773</td>\n",
              "      <td>0.885851</td>\n",
              "      <td>0.884322</td>\n",
              "      <td>0.868251</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16/65 00:04 < 00:14, 3.30 it/s]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lahtr 10kgnad_hf__distilbert-base-german-cased/"
      ],
      "metadata": {
        "id": "vaHTHuP9W6Dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBZsS24YbFpy"
      },
      "source": [
        "## Hyperparameter Tuning\n",
        "\n",
        "https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer.hyperparameter_search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phuJdMD6aaLP"
      },
      "source": [
        "# disable transformer warnings like \"Some weights of the model checkpoint ...\"\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=str(project_name),\n",
        "    report_to=[],\n",
        "    log_level=\"error\",\n",
        "    disable_tqdm=False,\n",
        "\n",
        "    evaluation_strategy=\"steps\",\n",
        "    # eval_steps=eval_steps,\n",
        "    save_strategy=\"steps\",\n",
        "    # save_steps=eval_steps,\n",
        "    # load_best_model_at_end=False,\n",
        "    # metric_for_best_model=\"eval_loss\",\n",
        "    # greater_is_better=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model_init=model_init,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_gnad10k[\"train\"],\n",
        "    eval_dataset=tokenized_gnad10k[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "\n",
        "# Default objective is the sum of all metrics\n",
        "# when metrics are provided, so we have to maximize it.\n",
        "# best = trainer.hyperparameter_search(\n",
        "#     hp_space=hp_space,\n",
        "#     compute_objective=objective,\n",
        "#     n_trials=2\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}