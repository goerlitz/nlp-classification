{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goerlitz/nlp-classification/blob/main/notebooks/10kGNAD/colab/21c2_10kGNAD_huggingface_optuna_config.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrveYRYD2E9-"
      },
      "source": [
        "# Hyperparameter Optimization with HuggingFace Transformers\n",
        "\n",
        "Adapted from https://huggingface.co/docs/transformers/custom_datasets#sequence-classification-with-imdb-reviews\n",
        "\n",
        "Things we need\n",
        "* a tokenizer\n",
        "* tokenized input data\n",
        "* a pretrained model\n",
        "* evaluation metrics\n",
        "* training parameters\n",
        "* a Trainer instance\n",
        "\n",
        "Notes\n",
        "* [class labels can be included in the model config](https://github.com/huggingface/transformers/pull/2945#issuecomment-781986506) (a bit hacky)\n",
        "* [fp16 is disabled on tesla P100 GPU in pytorch](https://discuss.pytorch.org/t/cnn-fp16-slower-than-fp32-on-tesla-p100/12146)\n",
        "* [comparison of GPUS (K80, T4, P100, V100)](https://www.kaggle.com/general/198232)\n",
        "* [GPU benchmark, mixed precision](https://medium.com/the-artificial-impostor/mixed-precision-training-on-tesla-t4-and-p100-d82e5d3b987d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JH5VyX6w9AIM"
      },
      "source": [
        "## Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2BZ9NGbmTkF"
      },
      "outputs": [],
      "source": [
        "# checkpoint = \"distilbert-base-german-cased\"\n",
        "checkpoint = \"deepset/gbert-base\"\n",
        "# checkpoint = \"deepset/gelectra-base\"\n",
        "# checkpoint = \"deepset/gelectra-large\"\n",
        "\n",
        "project_name = f'10kgnad_hf__{checkpoint.replace(\"/\", \"_\")}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKqjLlGpWdsi"
      },
      "source": [
        "### Connect Google Drive\n",
        "\n",
        "Will be used to save results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YXxAbtZWcxA",
        "outputId": "fc538874-b9d6-4f15-accc-436c89997eb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "M3n8HqXwEyK6"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# define model path\n",
        "root_path = Path('/content/gdrive/My Drive/')\n",
        "base_path = root_path / 'Colab Notebooks/nlp-classification/'\n",
        "model_path = base_path / 'models'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5F2oddvs2rNv"
      },
      "source": [
        "## Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVWRNvQf1cnR",
        "outputId": "4bbb4e70-7308-4a52-ecf7-2ce3915fca01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jan 30 22:50:06 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P0    35W / 250W |   3899MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnko1HAmsuly"
      },
      "source": [
        "## Install APEX\n",
        "\n",
        "https://stackoverflow.com/questions/57284345/how-to-install-nvidia-apex-on-google-colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdHFzp1yskfx",
        "outputId": "c6291c31-b86a-4035-e248-278bbde8501b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting setup.sh\n"
          ]
        }
      ],
      "source": [
        "%%writefile setup.sh\n",
        "\n",
        "git clone https://github.com/NVIDIA/apex\n",
        "pip install -q --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDdkye1_snxd",
        "outputId": "2a129d18-8c82-477f-fcb0-cbe732326260"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
            "Wall time: 7.87 µs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# !sh setup.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwlw_kI3nHLW"
      },
      "source": [
        "### Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QhKj625lBso",
        "outputId": "ac22c2d3-6146-434d-f769-47fae627b1ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "optuna==2.10.0\n",
            "transformers==4.16.1\n",
            "torch @ https://download.pytorch.org/whl/cu111/torch-1.10.0%2Bcu111-cp37-cp37m-linux_x86_64.whl\n",
            "CPU times: user 134 ms, sys: 448 ms, total: 582 ms\n",
            "Wall time: 16.4 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "!pip install -q -U transformers datasets >/dev/null\n",
        "!pip install -q -U optuna >/dev/null\n",
        "\n",
        "# check installed version\n",
        "!pip freeze | grep optuna        # optuna==2.10.0\n",
        "!pip freeze | grep transformers  # transformers==4.15.0\n",
        "!pip freeze | grep \"torch \"      # torch==1.10.0+cu111"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbpLzC7DRoR5"
      },
      "outputs": [],
      "source": [
        "from transformers import logging\n",
        "\n",
        "# hide progress bar when downloading tokenizer and model (a workaround!)\n",
        "logging.get_verbosity = lambda : logging.NOTSET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64Q8bFMM7UYg"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "2f44a82be16843898f3c900d46444283",
            "78c34095c4a7444cbb13c82fe40c43cf",
            "896fec95fec1463e8f04549e3f2896b9",
            "48bb699727e9424b93d9b9b123d3faa1",
            "9f6de51a6bde47ca94e0e3508e40a7a4",
            "00b892279f274a648322e5ec06d4cff1",
            "43f99edc070e4f9984176f1b6b846894",
            "ef0f1a2945754ec6986756e5f145d708",
            "9d6d41a6ede34027a06d8561cd62c0a7",
            "a17bdc26ac854fc98a07b3fb5dc270c1",
            "06cc5207581741d39baa9b6624b451b6"
          ]
        },
        "id": "4aYzOljo7W9X",
        "outputId": "e3d3b78d-0a7e-4cb3-aa2f-e70a3c29ceaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using custom data configuration default-0e1a53e9f937c1cf\n",
            "Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-0e1a53e9f937c1cf/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f44a82be16843898f3c900d46444283",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-0e1a53e9f937c1cf/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-7e2cd654f77312b3.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-0e1a53e9f937c1cf/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-8a8200b7f43f1260.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-0e1a53e9f937c1cf/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-575eed89dcd8fbbd.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-0e1a53e9f937c1cf/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-944af939de8444f9.arrow\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "base_url = \"https://raw.githubusercontent.com/tblock/10kGNAD/master/{}.csv\"\n",
        "data_files = {x: base_url.format(x) for x in [\"train\", \"test\"]}\n",
        "dataset = (load_dataset('csv',\n",
        "                        data_files=data_files,\n",
        "                        sep=\";\",\n",
        "                        quotechar=\"'\",\n",
        "                        names=[\"label\", \"text\"]).\n",
        "           class_encode_column(\"label\"))\n",
        "\n",
        "label_names = dataset[\"train\"].features[\"label\"].names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkcJyc-I8bLP",
        "outputId": "8acbcab3-8acc-48bb-8c5d-f880685f9ec7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['label', 'text'],\n",
            "        num_rows: 9245\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['label', 'text'],\n",
            "        num_rows: 1028\n",
            "    })\n",
            "})\n",
            "labels: ['Etat', 'Inland', 'International', 'Kultur', 'Panorama', 'Sport', 'Web', 'Wirtschaft', 'Wissenschaft']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': 5,\n",
              " 'text': '21-Jähriger fällt wohl bis Saisonende aus. Wien – Rapid muss wohl bis Saisonende auf Offensivspieler Thomas Murg verzichten. Der im Winter aus Ried gekommene 21-Jährige erlitt beim 0:4-Heimdebakel gegen Admira Wacker Mödling am Samstag einen Teilriss des Innenbandes im linken Knie, wie eine Magnetresonanz-Untersuchung am Donnerstag ergab. Murg erhielt eine Schiene, muss aber nicht operiert werden. Dennoch steht ihm eine mehrwöchige Pause bevor.'}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "print(dataset)\n",
        "print(\"labels:\", label_names)\n",
        "dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdmFxnHlSV6x"
      },
      "source": [
        "### Use Dynamic Padding\n",
        "\n",
        "Apply panding only on longest text in batch - this is more efficient than applying padding on the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhEWeeg2SVCe"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufbNYVSIxuhv"
      },
      "source": [
        "### Define Evaluation Metrics\n",
        "\n",
        "The funtion that computes the metrics needs to be passed to the Trainer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9HHIeJrM3_I"
      },
      "source": [
        "## Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqjuJ-2tRMYQ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import shutil\n",
        "\n",
        "def hp_space(trial: Trial):\n",
        "    \"\"\"A function that defines the hyperparameter search space.\n",
        "    To be used in :obj:`Trainer.hyperparameter_search`.\"\"\"\n",
        "    return {\n",
        "        # \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-4, log=True),  # distilbert/bert\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 5e-6, 5e-4, log=True),  # distilbert 1 epoch\n",
        "        # \"learning_rate\": trial.suggest_float(\"learning_rate\", 6e-5, 2e-4, log=True),  # electra\n",
        "        # \"num_train_epochs\": trial.suggest_categorical(\"num_train_epochs\", [1]),\n",
        "        \"num_train_epochs\": trial.suggest_categorical(\"num_train_epochs\", [3]),\n",
        "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32, 64, 128, 256]),\n",
        "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-3, 1e-2, log=True),\n",
        "        # \"weight_decay\": trial.suggest_categorical(\"weight_decay\", [1e-3, 0.0]),\n",
        "    }\n",
        "\n",
        "best_model_dir = \"best_model_trainer\"\n",
        "\n",
        "def best_model_callback(study, trial):\n",
        "    \"\"\"Save the model from a best trial\"\"\"\n",
        "    for t in study.best_trials:\n",
        "        if t.number == trial.number:\n",
        "            print(\"This is a new besttrial\", trial.number)\n",
        "        \n",
        "            out_filename = model_path / f\"{project_name}_t{trial.number}\"\n",
        "            shutil.make_archive(out_filename, 'zip', f\"{project_name}/{best_model_dir}\")\n",
        "\n",
        "def model_init(trial: Trial):\n",
        "    \"\"\"A function that instantiates the model to be used.\"\"\"\n",
        "\n",
        "    # We want to include the label names and save them together with the model.\n",
        "    # The only way to do this is to create a Config and put them in. \n",
        "    config = AutoConfig.from_pretrained(\n",
        "            checkpoint,\n",
        "            num_labels=len(label_names),\n",
        "            id2label={i: label for i, label in enumerate(label_names)},\n",
        "            label2id={label: i for i, label in enumerate(label_names)},\n",
        "            )\n",
        "\n",
        "    return AutoModelForSequenceClassification.from_pretrained(checkpoint, config=config)\n",
        "\n",
        "def objective(trial: Trial):\n",
        "\n",
        "    # get hyperparameters choice\n",
        "    hp = hp_space(trial)\n",
        "    lr = hp[\"learning_rate\"]\n",
        "    bs = hp[\"per_device_train_batch_size\"]\n",
        "    epochs = hp[\"num_train_epochs\"]\n",
        "    weight_decay = hp[\"weight_decay\"]\n",
        "    # label_smoothing_factor = hp[\"label_smoothing_factor\"]\n",
        "\n",
        "    # calculate gradient_accumulation_steps\n",
        "    train_batch_size = 8\n",
        "    gradient_accumulation_steps = bs // train_batch_size\n",
        "\n",
        "    eval_rounds_per_epoch = 5\n",
        "    eval_steps = dataset[\"train\"].num_rows / bs // eval_rounds_per_epoch\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=str(project_name),\n",
        "        report_to=[],\n",
        "        log_level=\"error\",\n",
        "        disable_tqdm=False,\n",
        "\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=eval_steps,\n",
        "        logging_steps=eval_steps,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=eval_steps,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        greater_is_better=False,\n",
        "\n",
        "        # hyperparameters\n",
        "        num_train_epochs=epochs,\n",
        "        learning_rate=lr,\n",
        "        per_device_train_batch_size=train_batch_size,\n",
        "        per_device_eval_batch_size=train_batch_size,\n",
        "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "        weight_decay=weight_decay,\n",
        "        # label_smoothing_factor=label_smoothing_factor,\n",
        "\n",
        "        # fp16=True,  # fp16 needs apex. but disabled on Tesla P100 by pytorch\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model_init=model_init,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset[\"train\"],\n",
        "        eval_dataset=tokenized_dataset[\"test\"],\n",
        "        tokenizer=tokenizer,\n",
        "        # data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[TrialLogAndPruningCallback(trial, objectives=[\"eval_loss\", \"eval_f1\"], min_trials=700, warmup_steps=eval_steps*3)]\n",
        "        # callbacks=[TrialPruningCallback(trial)]\n",
        "    )\n",
        "\n",
        "    # train model and save best model from evaluations\n",
        "    # needs 'load_best_model_at_end=True'\n",
        "    trainer.train()\n",
        "    trainer.save_model(f\"{project_name}/{best_model_dir}\")\n",
        "\n",
        "    result = trainer.evaluate(eval_dataset=tokenized_dataset[\"test\"])\n",
        "\n",
        "    # store eval metrics in trial\n",
        "    trial.set_user_attr(\"eval_result\", result)\n",
        "    \n",
        "    # return result[\"eval_loss\"]\n",
        "    return result[\"eval_loss\"], result[\"eval_f1\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9knsvQTUxjWi"
      },
      "source": [
        "## Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-wiraP9iLIK"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SNx_dukVxizp",
        "outputId": "25646589-195d-4ab0-f2a3-c12d57041dc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 07:46:30,994]\u001b[0m A new study created in RDB with name: deepset/gbert-base_bs8-256_ep3_len128\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:50: ExperimentalWarning: enqueue_trial is experimental (supported from v1.2.0). The interface can change in the future.\n",
            "/usr/local/lib/python3.7/dist-packages/optuna/study/study.py:857: ExperimentalWarning: create_trial is experimental (supported from v2.0.0). The interface can change in the future.\n",
            "  create_trial(state=TrialState.WAITING, system_attrs={\"fixed_params\": params})\n",
            "/usr/local/lib/python3.7/dist-packages/optuna/study/study.py:857: ExperimentalWarning: add_trial is experimental (supported from v2.0.0). The interface can change in the future.\n",
            "  create_trial(state=TrialState.WAITING, system_attrs={\"fixed_params\": params})\n",
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 16)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=345.0, min_trials=700\n",
            "params: {'learning_rate': 9.170262586219904e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 16, 'weight_decay': 0.0027599208115882986}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1734' max='1734' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1734/1734 09:26, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>1.537100</td>\n",
              "      <td>0.897237</td>\n",
              "      <td>0.756809</td>\n",
              "      <td>0.716140</td>\n",
              "      <td>0.813256</td>\n",
              "      <td>0.700500</td>\n",
              "      <td>0.721879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.738400</td>\n",
              "      <td>0.534543</td>\n",
              "      <td>0.857004</td>\n",
              "      <td>0.849835</td>\n",
              "      <td>0.853672</td>\n",
              "      <td>0.848348</td>\n",
              "      <td>0.836384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>345</td>\n",
              "      <td>0.544000</td>\n",
              "      <td>0.459638</td>\n",
              "      <td>0.860895</td>\n",
              "      <td>0.854224</td>\n",
              "      <td>0.852459</td>\n",
              "      <td>0.859615</td>\n",
              "      <td>0.841169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.473800</td>\n",
              "      <td>0.437892</td>\n",
              "      <td>0.870623</td>\n",
              "      <td>0.864380</td>\n",
              "      <td>0.862016</td>\n",
              "      <td>0.872975</td>\n",
              "      <td>0.852892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>575</td>\n",
              "      <td>0.455700</td>\n",
              "      <td>0.399327</td>\n",
              "      <td>0.887160</td>\n",
              "      <td>0.883708</td>\n",
              "      <td>0.889948</td>\n",
              "      <td>0.879312</td>\n",
              "      <td>0.870744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>0.371600</td>\n",
              "      <td>0.380467</td>\n",
              "      <td>0.877432</td>\n",
              "      <td>0.873802</td>\n",
              "      <td>0.877573</td>\n",
              "      <td>0.871174</td>\n",
              "      <td>0.859624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>805</td>\n",
              "      <td>0.303700</td>\n",
              "      <td>0.365358</td>\n",
              "      <td>0.892996</td>\n",
              "      <td>0.886700</td>\n",
              "      <td>0.887809</td>\n",
              "      <td>0.886420</td>\n",
              "      <td>0.877468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>0.311700</td>\n",
              "      <td>0.386337</td>\n",
              "      <td>0.875486</td>\n",
              "      <td>0.869010</td>\n",
              "      <td>0.875974</td>\n",
              "      <td>0.866185</td>\n",
              "      <td>0.857515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1035</td>\n",
              "      <td>0.319400</td>\n",
              "      <td>0.374001</td>\n",
              "      <td>0.892023</td>\n",
              "      <td>0.884317</td>\n",
              "      <td>0.883084</td>\n",
              "      <td>0.888582</td>\n",
              "      <td>0.876567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.344200</td>\n",
              "      <td>0.368000</td>\n",
              "      <td>0.893969</td>\n",
              "      <td>0.886957</td>\n",
              "      <td>0.890175</td>\n",
              "      <td>0.886085</td>\n",
              "      <td>0.878631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1265</td>\n",
              "      <td>0.278300</td>\n",
              "      <td>0.362749</td>\n",
              "      <td>0.895914</td>\n",
              "      <td>0.890187</td>\n",
              "      <td>0.891404</td>\n",
              "      <td>0.891021</td>\n",
              "      <td>0.880876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1380</td>\n",
              "      <td>0.250700</td>\n",
              "      <td>0.364072</td>\n",
              "      <td>0.891051</td>\n",
              "      <td>0.885987</td>\n",
              "      <td>0.885233</td>\n",
              "      <td>0.888109</td>\n",
              "      <td>0.875286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1495</td>\n",
              "      <td>0.237700</td>\n",
              "      <td>0.365809</td>\n",
              "      <td>0.889105</td>\n",
              "      <td>0.884932</td>\n",
              "      <td>0.885768</td>\n",
              "      <td>0.884947</td>\n",
              "      <td>0.872944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1610</td>\n",
              "      <td>0.200300</td>\n",
              "      <td>0.374533</td>\n",
              "      <td>0.883268</td>\n",
              "      <td>0.877966</td>\n",
              "      <td>0.881716</td>\n",
              "      <td>0.876637</td>\n",
              "      <td>0.866320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1725</td>\n",
              "      <td>0.197700</td>\n",
              "      <td>0.369482</td>\n",
              "      <td>0.887160</td>\n",
              "      <td>0.882438</td>\n",
              "      <td>0.884377</td>\n",
              "      <td>0.881724</td>\n",
              "      <td>0.870728</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 07:56:30,721]\u001b[0m Trial 0 finished with values: [0.3627486228942871, 0.8901865223926281] and parameters: {'learning_rate': 9.170262586219904e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 16, 'weight_decay': 0.0027599208115882986}. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a new besttrial 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 32)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=171.0, min_trials=700\n",
            "params: {'learning_rate': 0.00025377987451239765, 'num_train_epochs': 3, 'per_device_train_batch_size': 32, 'weight_decay': 0.0027855104091463493}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='867' max='867' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [867/867 09:03, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>1.019300</td>\n",
              "      <td>0.847159</td>\n",
              "      <td>0.741245</td>\n",
              "      <td>0.725104</td>\n",
              "      <td>0.744659</td>\n",
              "      <td>0.757156</td>\n",
              "      <td>0.714034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>0.740100</td>\n",
              "      <td>0.583273</td>\n",
              "      <td>0.822957</td>\n",
              "      <td>0.817302</td>\n",
              "      <td>0.830493</td>\n",
              "      <td>0.817661</td>\n",
              "      <td>0.798006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>0.702300</td>\n",
              "      <td>0.655447</td>\n",
              "      <td>0.810311</td>\n",
              "      <td>0.793703</td>\n",
              "      <td>0.808370</td>\n",
              "      <td>0.809643</td>\n",
              "      <td>0.785935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>228</td>\n",
              "      <td>0.663700</td>\n",
              "      <td>0.496088</td>\n",
              "      <td>0.852140</td>\n",
              "      <td>0.851229</td>\n",
              "      <td>0.856681</td>\n",
              "      <td>0.855069</td>\n",
              "      <td>0.832134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>285</td>\n",
              "      <td>0.668900</td>\n",
              "      <td>0.554016</td>\n",
              "      <td>0.845331</td>\n",
              "      <td>0.843783</td>\n",
              "      <td>0.857241</td>\n",
              "      <td>0.837049</td>\n",
              "      <td>0.823510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>342</td>\n",
              "      <td>0.514800</td>\n",
              "      <td>0.528414</td>\n",
              "      <td>0.848249</td>\n",
              "      <td>0.836174</td>\n",
              "      <td>0.844787</td>\n",
              "      <td>0.833759</td>\n",
              "      <td>0.826584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>399</td>\n",
              "      <td>0.435300</td>\n",
              "      <td>0.543301</td>\n",
              "      <td>0.860895</td>\n",
              "      <td>0.849590</td>\n",
              "      <td>0.853161</td>\n",
              "      <td>0.858970</td>\n",
              "      <td>0.841827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>456</td>\n",
              "      <td>0.421500</td>\n",
              "      <td>0.515264</td>\n",
              "      <td>0.849222</td>\n",
              "      <td>0.827968</td>\n",
              "      <td>0.841602</td>\n",
              "      <td>0.832897</td>\n",
              "      <td>0.827899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>513</td>\n",
              "      <td>0.352100</td>\n",
              "      <td>0.514429</td>\n",
              "      <td>0.874514</td>\n",
              "      <td>0.867709</td>\n",
              "      <td>0.865420</td>\n",
              "      <td>0.873051</td>\n",
              "      <td>0.856551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.416700</td>\n",
              "      <td>0.495555</td>\n",
              "      <td>0.860895</td>\n",
              "      <td>0.841587</td>\n",
              "      <td>0.860862</td>\n",
              "      <td>0.839522</td>\n",
              "      <td>0.841480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>627</td>\n",
              "      <td>0.232700</td>\n",
              "      <td>0.496759</td>\n",
              "      <td>0.890078</td>\n",
              "      <td>0.886112</td>\n",
              "      <td>0.895426</td>\n",
              "      <td>0.878681</td>\n",
              "      <td>0.874015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>684</td>\n",
              "      <td>0.235200</td>\n",
              "      <td>0.535297</td>\n",
              "      <td>0.873541</td>\n",
              "      <td>0.865684</td>\n",
              "      <td>0.877783</td>\n",
              "      <td>0.859750</td>\n",
              "      <td>0.855309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>741</td>\n",
              "      <td>0.225000</td>\n",
              "      <td>0.466223</td>\n",
              "      <td>0.890078</td>\n",
              "      <td>0.887279</td>\n",
              "      <td>0.890985</td>\n",
              "      <td>0.885496</td>\n",
              "      <td>0.874096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>798</td>\n",
              "      <td>0.147600</td>\n",
              "      <td>0.442057</td>\n",
              "      <td>0.893969</td>\n",
              "      <td>0.889782</td>\n",
              "      <td>0.894217</td>\n",
              "      <td>0.885940</td>\n",
              "      <td>0.878441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>855</td>\n",
              "      <td>0.139500</td>\n",
              "      <td>0.447401</td>\n",
              "      <td>0.895914</td>\n",
              "      <td>0.893122</td>\n",
              "      <td>0.895344</td>\n",
              "      <td>0.891659</td>\n",
              "      <td>0.880763</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 08:06:08,960]\u001b[0m Trial 1 finished with values: [0.4420566260814667, 0.8897816535322313] and parameters: {'learning_rate': 0.00025377987451239765, 'num_train_epochs': 3, 'per_device_train_batch_size': 32, 'weight_decay': 0.0027855104091463493}. \u001b[0m\n",
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 8)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=693.0, min_trials=700\n",
            "params: {'learning_rate': 1.477243933028255e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 8, 'weight_decay': 0.00965644490221287}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3468' max='3468' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3468/3468 10:20, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>231</td>\n",
              "      <td>1.081500</td>\n",
              "      <td>0.560300</td>\n",
              "      <td>0.843385</td>\n",
              "      <td>0.840096</td>\n",
              "      <td>0.838402</td>\n",
              "      <td>0.846232</td>\n",
              "      <td>0.821653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>462</td>\n",
              "      <td>0.513400</td>\n",
              "      <td>0.435483</td>\n",
              "      <td>0.871595</td>\n",
              "      <td>0.867540</td>\n",
              "      <td>0.861620</td>\n",
              "      <td>0.878601</td>\n",
              "      <td>0.853917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>693</td>\n",
              "      <td>0.470200</td>\n",
              "      <td>0.416472</td>\n",
              "      <td>0.882296</td>\n",
              "      <td>0.878207</td>\n",
              "      <td>0.877135</td>\n",
              "      <td>0.882815</td>\n",
              "      <td>0.865591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>924</td>\n",
              "      <td>0.431300</td>\n",
              "      <td>0.407243</td>\n",
              "      <td>0.871595</td>\n",
              "      <td>0.864351</td>\n",
              "      <td>0.863730</td>\n",
              "      <td>0.870106</td>\n",
              "      <td>0.853648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1155</td>\n",
              "      <td>0.443100</td>\n",
              "      <td>0.411471</td>\n",
              "      <td>0.892023</td>\n",
              "      <td>0.888496</td>\n",
              "      <td>0.891028</td>\n",
              "      <td>0.889036</td>\n",
              "      <td>0.877030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1386</td>\n",
              "      <td>0.260800</td>\n",
              "      <td>0.414936</td>\n",
              "      <td>0.892023</td>\n",
              "      <td>0.886505</td>\n",
              "      <td>0.889275</td>\n",
              "      <td>0.885957</td>\n",
              "      <td>0.876599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1617</td>\n",
              "      <td>0.265300</td>\n",
              "      <td>0.429204</td>\n",
              "      <td>0.892023</td>\n",
              "      <td>0.884773</td>\n",
              "      <td>0.883453</td>\n",
              "      <td>0.888157</td>\n",
              "      <td>0.876790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1848</td>\n",
              "      <td>0.282000</td>\n",
              "      <td>0.483657</td>\n",
              "      <td>0.883268</td>\n",
              "      <td>0.878967</td>\n",
              "      <td>0.886231</td>\n",
              "      <td>0.876810</td>\n",
              "      <td>0.866471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2079</td>\n",
              "      <td>0.284300</td>\n",
              "      <td>0.427630</td>\n",
              "      <td>0.891051</td>\n",
              "      <td>0.884700</td>\n",
              "      <td>0.883097</td>\n",
              "      <td>0.887044</td>\n",
              "      <td>0.875257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2310</td>\n",
              "      <td>0.308100</td>\n",
              "      <td>0.413849</td>\n",
              "      <td>0.898833</td>\n",
              "      <td>0.896164</td>\n",
              "      <td>0.902419</td>\n",
              "      <td>0.892017</td>\n",
              "      <td>0.884197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2541</td>\n",
              "      <td>0.178500</td>\n",
              "      <td>0.435819</td>\n",
              "      <td>0.902724</td>\n",
              "      <td>0.899581</td>\n",
              "      <td>0.902755</td>\n",
              "      <td>0.897196</td>\n",
              "      <td>0.888544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2772</td>\n",
              "      <td>0.161600</td>\n",
              "      <td>0.484680</td>\n",
              "      <td>0.890078</td>\n",
              "      <td>0.883438</td>\n",
              "      <td>0.880396</td>\n",
              "      <td>0.888543</td>\n",
              "      <td>0.874405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3003</td>\n",
              "      <td>0.159300</td>\n",
              "      <td>0.474269</td>\n",
              "      <td>0.892023</td>\n",
              "      <td>0.887852</td>\n",
              "      <td>0.889087</td>\n",
              "      <td>0.887514</td>\n",
              "      <td>0.876280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3234</td>\n",
              "      <td>0.127700</td>\n",
              "      <td>0.489893</td>\n",
              "      <td>0.892996</td>\n",
              "      <td>0.887114</td>\n",
              "      <td>0.888712</td>\n",
              "      <td>0.887670</td>\n",
              "      <td>0.877497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3465</td>\n",
              "      <td>0.127500</td>\n",
              "      <td>0.485847</td>\n",
              "      <td>0.896887</td>\n",
              "      <td>0.890947</td>\n",
              "      <td>0.890592</td>\n",
              "      <td>0.892677</td>\n",
              "      <td>0.881958</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 08:16:40,565]\u001b[0m Trial 2 finished with values: [0.4072425365447998, 0.8643513996282546] and parameters: {'learning_rate': 1.477243933028255e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 8, 'weight_decay': 0.00965644490221287}. \u001b[0m\n",
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 32)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=171.0, min_trials=700\n",
            "params: {'learning_rate': 8.865861102164478e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 32, 'weight_decay': 0.004610893951910017}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='867' max='867' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [867/867 09:00, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>1.790000</td>\n",
              "      <td>1.310157</td>\n",
              "      <td>0.628405</td>\n",
              "      <td>0.469333</td>\n",
              "      <td>0.577545</td>\n",
              "      <td>0.499204</td>\n",
              "      <td>0.576793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>1.067900</td>\n",
              "      <td>0.751542</td>\n",
              "      <td>0.814202</td>\n",
              "      <td>0.793712</td>\n",
              "      <td>0.832896</td>\n",
              "      <td>0.787286</td>\n",
              "      <td>0.787767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>0.695900</td>\n",
              "      <td>0.555820</td>\n",
              "      <td>0.849222</td>\n",
              "      <td>0.845489</td>\n",
              "      <td>0.855019</td>\n",
              "      <td>0.839391</td>\n",
              "      <td>0.827291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>228</td>\n",
              "      <td>0.562900</td>\n",
              "      <td>0.505775</td>\n",
              "      <td>0.854086</td>\n",
              "      <td>0.849252</td>\n",
              "      <td>0.847440</td>\n",
              "      <td>0.858791</td>\n",
              "      <td>0.834354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>285</td>\n",
              "      <td>0.525200</td>\n",
              "      <td>0.450872</td>\n",
              "      <td>0.869650</td>\n",
              "      <td>0.862135</td>\n",
              "      <td>0.867140</td>\n",
              "      <td>0.860356</td>\n",
              "      <td>0.850749</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>342</td>\n",
              "      <td>0.458300</td>\n",
              "      <td>0.414959</td>\n",
              "      <td>0.874514</td>\n",
              "      <td>0.870510</td>\n",
              "      <td>0.871114</td>\n",
              "      <td>0.870769</td>\n",
              "      <td>0.856316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>399</td>\n",
              "      <td>0.371800</td>\n",
              "      <td>0.400745</td>\n",
              "      <td>0.876459</td>\n",
              "      <td>0.869203</td>\n",
              "      <td>0.871656</td>\n",
              "      <td>0.869392</td>\n",
              "      <td>0.858533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>456</td>\n",
              "      <td>0.383100</td>\n",
              "      <td>0.401695</td>\n",
              "      <td>0.879377</td>\n",
              "      <td>0.872886</td>\n",
              "      <td>0.870227</td>\n",
              "      <td>0.878862</td>\n",
              "      <td>0.862252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>513</td>\n",
              "      <td>0.374900</td>\n",
              "      <td>0.381597</td>\n",
              "      <td>0.879377</td>\n",
              "      <td>0.872931</td>\n",
              "      <td>0.870816</td>\n",
              "      <td>0.877462</td>\n",
              "      <td>0.862093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.395000</td>\n",
              "      <td>0.383328</td>\n",
              "      <td>0.889105</td>\n",
              "      <td>0.884042</td>\n",
              "      <td>0.888943</td>\n",
              "      <td>0.880802</td>\n",
              "      <td>0.872950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>627</td>\n",
              "      <td>0.336100</td>\n",
              "      <td>0.372574</td>\n",
              "      <td>0.880350</td>\n",
              "      <td>0.874858</td>\n",
              "      <td>0.873213</td>\n",
              "      <td>0.878671</td>\n",
              "      <td>0.863228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>684</td>\n",
              "      <td>0.338000</td>\n",
              "      <td>0.368732</td>\n",
              "      <td>0.882296</td>\n",
              "      <td>0.877969</td>\n",
              "      <td>0.879723</td>\n",
              "      <td>0.877017</td>\n",
              "      <td>0.865122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>741</td>\n",
              "      <td>0.314100</td>\n",
              "      <td>0.372121</td>\n",
              "      <td>0.881323</td>\n",
              "      <td>0.876762</td>\n",
              "      <td>0.880297</td>\n",
              "      <td>0.874755</td>\n",
              "      <td>0.864014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>798</td>\n",
              "      <td>0.268600</td>\n",
              "      <td>0.367286</td>\n",
              "      <td>0.883268</td>\n",
              "      <td>0.877471</td>\n",
              "      <td>0.881617</td>\n",
              "      <td>0.874977</td>\n",
              "      <td>0.866256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>855</td>\n",
              "      <td>0.271400</td>\n",
              "      <td>0.366016</td>\n",
              "      <td>0.882296</td>\n",
              "      <td>0.877448</td>\n",
              "      <td>0.879871</td>\n",
              "      <td>0.875953</td>\n",
              "      <td>0.865101</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 08:25:54,330]\u001b[0m Trial 3 finished with values: [0.36601579189300537, 0.8774477535603779] and parameters: {'learning_rate': 8.865861102164478e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 32, 'weight_decay': 0.004610893951910017}. \u001b[0m\n",
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 32)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=171.0, min_trials=700\n",
            "params: {'learning_rate': 9.332422126559672e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 32, 'weight_decay': 0.003930827533604793}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='867' max='867' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [867/867 09:00, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.915700</td>\n",
              "      <td>0.559776</td>\n",
              "      <td>0.831712</td>\n",
              "      <td>0.824993</td>\n",
              "      <td>0.851876</td>\n",
              "      <td>0.821130</td>\n",
              "      <td>0.809747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>0.519000</td>\n",
              "      <td>0.449155</td>\n",
              "      <td>0.851167</td>\n",
              "      <td>0.824476</td>\n",
              "      <td>0.845263</td>\n",
              "      <td>0.827201</td>\n",
              "      <td>0.830528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>0.481100</td>\n",
              "      <td>0.425011</td>\n",
              "      <td>0.866732</td>\n",
              "      <td>0.858558</td>\n",
              "      <td>0.852967</td>\n",
              "      <td>0.871659</td>\n",
              "      <td>0.848311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>228</td>\n",
              "      <td>0.439700</td>\n",
              "      <td>0.438610</td>\n",
              "      <td>0.859922</td>\n",
              "      <td>0.858908</td>\n",
              "      <td>0.861055</td>\n",
              "      <td>0.867057</td>\n",
              "      <td>0.841260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>285</td>\n",
              "      <td>0.445500</td>\n",
              "      <td>0.403228</td>\n",
              "      <td>0.873541</td>\n",
              "      <td>0.870829</td>\n",
              "      <td>0.889054</td>\n",
              "      <td>0.858319</td>\n",
              "      <td>0.855422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>342</td>\n",
              "      <td>0.275800</td>\n",
              "      <td>0.367296</td>\n",
              "      <td>0.892023</td>\n",
              "      <td>0.890413</td>\n",
              "      <td>0.887556</td>\n",
              "      <td>0.895201</td>\n",
              "      <td>0.876615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>399</td>\n",
              "      <td>0.212000</td>\n",
              "      <td>0.387461</td>\n",
              "      <td>0.892996</td>\n",
              "      <td>0.886157</td>\n",
              "      <td>0.890431</td>\n",
              "      <td>0.883284</td>\n",
              "      <td>0.877391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>456</td>\n",
              "      <td>0.227200</td>\n",
              "      <td>0.393353</td>\n",
              "      <td>0.884241</td>\n",
              "      <td>0.880822</td>\n",
              "      <td>0.884960</td>\n",
              "      <td>0.880261</td>\n",
              "      <td>0.867750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>513</td>\n",
              "      <td>0.215500</td>\n",
              "      <td>0.429472</td>\n",
              "      <td>0.890078</td>\n",
              "      <td>0.883410</td>\n",
              "      <td>0.883167</td>\n",
              "      <td>0.891791</td>\n",
              "      <td>0.875090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.264900</td>\n",
              "      <td>0.351835</td>\n",
              "      <td>0.893969</td>\n",
              "      <td>0.887789</td>\n",
              "      <td>0.885117</td>\n",
              "      <td>0.892162</td>\n",
              "      <td>0.878854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>627</td>\n",
              "      <td>0.105500</td>\n",
              "      <td>0.381949</td>\n",
              "      <td>0.903696</td>\n",
              "      <td>0.899415</td>\n",
              "      <td>0.900199</td>\n",
              "      <td>0.899026</td>\n",
              "      <td>0.889708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>684</td>\n",
              "      <td>0.093800</td>\n",
              "      <td>0.416008</td>\n",
              "      <td>0.898833</td>\n",
              "      <td>0.894873</td>\n",
              "      <td>0.892815</td>\n",
              "      <td>0.898673</td>\n",
              "      <td>0.884303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>741</td>\n",
              "      <td>0.086600</td>\n",
              "      <td>0.429647</td>\n",
              "      <td>0.900778</td>\n",
              "      <td>0.894969</td>\n",
              "      <td>0.892564</td>\n",
              "      <td>0.900002</td>\n",
              "      <td>0.886602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>798</td>\n",
              "      <td>0.070100</td>\n",
              "      <td>0.420419</td>\n",
              "      <td>0.901751</td>\n",
              "      <td>0.897877</td>\n",
              "      <td>0.899516</td>\n",
              "      <td>0.896946</td>\n",
              "      <td>0.887477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>855</td>\n",
              "      <td>0.069800</td>\n",
              "      <td>0.415128</td>\n",
              "      <td>0.900778</td>\n",
              "      <td>0.895539</td>\n",
              "      <td>0.895959</td>\n",
              "      <td>0.895570</td>\n",
              "      <td>0.886332</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 08:35:06,497]\u001b[0m Trial 4 finished with values: [0.3518349528312683, 0.8877889552706211] and parameters: {'learning_rate': 9.332422126559672e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 32, 'weight_decay': 0.003930827533604793}. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a new besttrial 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 128)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=42.0, min_trials=700\n",
            "params: {'learning_rate': 0.00041050509467014317, 'num_train_epochs': 3, 'per_device_train_batch_size': 128, 'weight_decay': 0.0011671045013154776}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='216' max='216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [216/216 08:40, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.459400</td>\n",
              "      <td>0.763391</td>\n",
              "      <td>0.785992</td>\n",
              "      <td>0.765838</td>\n",
              "      <td>0.776774</td>\n",
              "      <td>0.782524</td>\n",
              "      <td>0.757014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.718100</td>\n",
              "      <td>0.604803</td>\n",
              "      <td>0.821984</td>\n",
              "      <td>0.814266</td>\n",
              "      <td>0.821329</td>\n",
              "      <td>0.812472</td>\n",
              "      <td>0.796648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.678800</td>\n",
              "      <td>0.526314</td>\n",
              "      <td>0.848249</td>\n",
              "      <td>0.843916</td>\n",
              "      <td>0.840697</td>\n",
              "      <td>0.854574</td>\n",
              "      <td>0.827490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.585400</td>\n",
              "      <td>0.558474</td>\n",
              "      <td>0.834630</td>\n",
              "      <td>0.830480</td>\n",
              "      <td>0.846141</td>\n",
              "      <td>0.834079</td>\n",
              "      <td>0.814127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.622000</td>\n",
              "      <td>0.500237</td>\n",
              "      <td>0.853113</td>\n",
              "      <td>0.847902</td>\n",
              "      <td>0.864267</td>\n",
              "      <td>0.840061</td>\n",
              "      <td>0.831979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.430700</td>\n",
              "      <td>0.540657</td>\n",
              "      <td>0.847276</td>\n",
              "      <td>0.838883</td>\n",
              "      <td>0.838308</td>\n",
              "      <td>0.850132</td>\n",
              "      <td>0.826701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.347300</td>\n",
              "      <td>0.539931</td>\n",
              "      <td>0.849222</td>\n",
              "      <td>0.842080</td>\n",
              "      <td>0.842229</td>\n",
              "      <td>0.847322</td>\n",
              "      <td>0.828547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.378100</td>\n",
              "      <td>0.456767</td>\n",
              "      <td>0.859922</td>\n",
              "      <td>0.852043</td>\n",
              "      <td>0.866657</td>\n",
              "      <td>0.846990</td>\n",
              "      <td>0.840447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>0.341900</td>\n",
              "      <td>0.473319</td>\n",
              "      <td>0.858949</td>\n",
              "      <td>0.851470</td>\n",
              "      <td>0.859673</td>\n",
              "      <td>0.852179</td>\n",
              "      <td>0.839353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.370000</td>\n",
              "      <td>0.464985</td>\n",
              "      <td>0.869650</td>\n",
              "      <td>0.863679</td>\n",
              "      <td>0.861594</td>\n",
              "      <td>0.870510</td>\n",
              "      <td>0.851392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>0.251100</td>\n",
              "      <td>0.453321</td>\n",
              "      <td>0.879377</td>\n",
              "      <td>0.873629</td>\n",
              "      <td>0.873274</td>\n",
              "      <td>0.875486</td>\n",
              "      <td>0.862143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>0.191700</td>\n",
              "      <td>0.480038</td>\n",
              "      <td>0.876459</td>\n",
              "      <td>0.872061</td>\n",
              "      <td>0.873759</td>\n",
              "      <td>0.873726</td>\n",
              "      <td>0.859041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>0.178100</td>\n",
              "      <td>0.442370</td>\n",
              "      <td>0.879377</td>\n",
              "      <td>0.875359</td>\n",
              "      <td>0.878098</td>\n",
              "      <td>0.874693</td>\n",
              "      <td>0.861877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>0.136800</td>\n",
              "      <td>0.445360</td>\n",
              "      <td>0.887160</td>\n",
              "      <td>0.880385</td>\n",
              "      <td>0.881276</td>\n",
              "      <td>0.882251</td>\n",
              "      <td>0.871093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.137700</td>\n",
              "      <td>0.437313</td>\n",
              "      <td>0.883268</td>\n",
              "      <td>0.874834</td>\n",
              "      <td>0.878459</td>\n",
              "      <td>0.872671</td>\n",
              "      <td>0.866345</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 08:44:22,835]\u001b[0m Trial 5 finished with values: [0.4373129606246948, 0.8748336346218838] and parameters: {'learning_rate': 0.00041050509467014317, 'num_train_epochs': 3, 'per_device_train_batch_size': 128, 'weight_decay': 0.0011671045013154776}. \u001b[0m\n",
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 256)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=21.0, min_trials=700\n",
            "params: {'learning_rate': 1.3980474740304256e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 256, 'weight_decay': 0.0039606645867351345}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='108' max='108' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [108/108 08:34, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.083600</td>\n",
              "      <td>1.925694</td>\n",
              "      <td>0.463035</td>\n",
              "      <td>0.309615</td>\n",
              "      <td>0.413138</td>\n",
              "      <td>0.352162</td>\n",
              "      <td>0.384702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.851200</td>\n",
              "      <td>1.664102</td>\n",
              "      <td>0.563230</td>\n",
              "      <td>0.372737</td>\n",
              "      <td>0.335689</td>\n",
              "      <td>0.432836</td>\n",
              "      <td>0.500177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.617000</td>\n",
              "      <td>1.420909</td>\n",
              "      <td>0.634241</td>\n",
              "      <td>0.479870</td>\n",
              "      <td>0.567738</td>\n",
              "      <td>0.508253</td>\n",
              "      <td>0.581262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.376400</td>\n",
              "      <td>1.221086</td>\n",
              "      <td>0.672179</td>\n",
              "      <td>0.541829</td>\n",
              "      <td>0.810594</td>\n",
              "      <td>0.554148</td>\n",
              "      <td>0.625154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.214700</td>\n",
              "      <td>1.041985</td>\n",
              "      <td>0.746109</td>\n",
              "      <td>0.660264</td>\n",
              "      <td>0.829345</td>\n",
              "      <td>0.650016</td>\n",
              "      <td>0.710464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.078500</td>\n",
              "      <td>0.911290</td>\n",
              "      <td>0.787938</td>\n",
              "      <td>0.745801</td>\n",
              "      <td>0.826540</td>\n",
              "      <td>0.736779</td>\n",
              "      <td>0.757354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.903700</td>\n",
              "      <td>0.806528</td>\n",
              "      <td>0.813230</td>\n",
              "      <td>0.786793</td>\n",
              "      <td>0.852364</td>\n",
              "      <td>0.770100</td>\n",
              "      <td>0.786662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.818700</td>\n",
              "      <td>0.733761</td>\n",
              "      <td>0.822957</td>\n",
              "      <td>0.802746</td>\n",
              "      <td>0.842309</td>\n",
              "      <td>0.788039</td>\n",
              "      <td>0.797036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.749700</td>\n",
              "      <td>0.678391</td>\n",
              "      <td>0.831712</td>\n",
              "      <td>0.817796</td>\n",
              "      <td>0.851763</td>\n",
              "      <td>0.804499</td>\n",
              "      <td>0.807333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.733200</td>\n",
              "      <td>0.627279</td>\n",
              "      <td>0.846304</td>\n",
              "      <td>0.833607</td>\n",
              "      <td>0.855023</td>\n",
              "      <td>0.824240</td>\n",
              "      <td>0.823812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.688600</td>\n",
              "      <td>0.603742</td>\n",
              "      <td>0.845331</td>\n",
              "      <td>0.834105</td>\n",
              "      <td>0.858450</td>\n",
              "      <td>0.822585</td>\n",
              "      <td>0.822698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.641600</td>\n",
              "      <td>0.576303</td>\n",
              "      <td>0.857004</td>\n",
              "      <td>0.847691</td>\n",
              "      <td>0.865540</td>\n",
              "      <td>0.836961</td>\n",
              "      <td>0.836009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.633900</td>\n",
              "      <td>0.561245</td>\n",
              "      <td>0.859922</td>\n",
              "      <td>0.849414</td>\n",
              "      <td>0.863216</td>\n",
              "      <td>0.841993</td>\n",
              "      <td>0.839470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.582300</td>\n",
              "      <td>0.553538</td>\n",
              "      <td>0.856031</td>\n",
              "      <td>0.845179</td>\n",
              "      <td>0.859294</td>\n",
              "      <td>0.837208</td>\n",
              "      <td>0.834982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.548300</td>\n",
              "      <td>0.549358</td>\n",
              "      <td>0.854086</td>\n",
              "      <td>0.843582</td>\n",
              "      <td>0.857749</td>\n",
              "      <td>0.835457</td>\n",
              "      <td>0.832714</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 08:53:11,172]\u001b[0m Trial 6 finished with values: [0.5493583679199219, 0.8435817589319067] and parameters: {'learning_rate': 1.3980474740304256e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 256, 'weight_decay': 0.0039606645867351345}. \u001b[0m\n",
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 64)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=84.0, min_trials=700\n",
            "params: {'learning_rate': 1.9839175247031734e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 64, 'weight_decay': 0.005227926427303831}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='432' max='432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [432/432 08:48, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.708600</td>\n",
              "      <td>1.150119</td>\n",
              "      <td>0.678016</td>\n",
              "      <td>0.547772</td>\n",
              "      <td>0.817171</td>\n",
              "      <td>0.557889</td>\n",
              "      <td>0.634344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.922100</td>\n",
              "      <td>0.624375</td>\n",
              "      <td>0.832685</td>\n",
              "      <td>0.818441</td>\n",
              "      <td>0.844124</td>\n",
              "      <td>0.811205</td>\n",
              "      <td>0.808468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.591900</td>\n",
              "      <td>0.489699</td>\n",
              "      <td>0.854086</td>\n",
              "      <td>0.847539</td>\n",
              "      <td>0.853667</td>\n",
              "      <td>0.844534</td>\n",
              "      <td>0.832897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.504000</td>\n",
              "      <td>0.419624</td>\n",
              "      <td>0.874514</td>\n",
              "      <td>0.868419</td>\n",
              "      <td>0.867372</td>\n",
              "      <td>0.871146</td>\n",
              "      <td>0.856504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.492900</td>\n",
              "      <td>0.415450</td>\n",
              "      <td>0.872568</td>\n",
              "      <td>0.866709</td>\n",
              "      <td>0.867942</td>\n",
              "      <td>0.868863</td>\n",
              "      <td>0.854631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>0.406100</td>\n",
              "      <td>0.378423</td>\n",
              "      <td>0.885214</td>\n",
              "      <td>0.880989</td>\n",
              "      <td>0.878387</td>\n",
              "      <td>0.883987</td>\n",
              "      <td>0.868584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>0.319700</td>\n",
              "      <td>0.373419</td>\n",
              "      <td>0.886187</td>\n",
              "      <td>0.878579</td>\n",
              "      <td>0.876293</td>\n",
              "      <td>0.883851</td>\n",
              "      <td>0.869900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>224</td>\n",
              "      <td>0.328200</td>\n",
              "      <td>0.363974</td>\n",
              "      <td>0.885214</td>\n",
              "      <td>0.880946</td>\n",
              "      <td>0.881016</td>\n",
              "      <td>0.882122</td>\n",
              "      <td>0.868638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>252</td>\n",
              "      <td>0.313800</td>\n",
              "      <td>0.362317</td>\n",
              "      <td>0.887160</td>\n",
              "      <td>0.883535</td>\n",
              "      <td>0.889868</td>\n",
              "      <td>0.879382</td>\n",
              "      <td>0.870731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.338700</td>\n",
              "      <td>0.356850</td>\n",
              "      <td>0.892023</td>\n",
              "      <td>0.885170</td>\n",
              "      <td>0.887760</td>\n",
              "      <td>0.885019</td>\n",
              "      <td>0.876428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>308</td>\n",
              "      <td>0.297800</td>\n",
              "      <td>0.347543</td>\n",
              "      <td>0.893969</td>\n",
              "      <td>0.887359</td>\n",
              "      <td>0.887887</td>\n",
              "      <td>0.888296</td>\n",
              "      <td>0.878687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>336</td>\n",
              "      <td>0.260500</td>\n",
              "      <td>0.349042</td>\n",
              "      <td>0.895914</td>\n",
              "      <td>0.890523</td>\n",
              "      <td>0.893592</td>\n",
              "      <td>0.888833</td>\n",
              "      <td>0.880827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>364</td>\n",
              "      <td>0.246000</td>\n",
              "      <td>0.353439</td>\n",
              "      <td>0.890078</td>\n",
              "      <td>0.882789</td>\n",
              "      <td>0.883179</td>\n",
              "      <td>0.884743</td>\n",
              "      <td>0.874195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>392</td>\n",
              "      <td>0.214000</td>\n",
              "      <td>0.349053</td>\n",
              "      <td>0.891051</td>\n",
              "      <td>0.884843</td>\n",
              "      <td>0.885838</td>\n",
              "      <td>0.885505</td>\n",
              "      <td>0.875320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.205000</td>\n",
              "      <td>0.351159</td>\n",
              "      <td>0.889105</td>\n",
              "      <td>0.882644</td>\n",
              "      <td>0.885055</td>\n",
              "      <td>0.881584</td>\n",
              "      <td>0.872980</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 09:02:11,473]\u001b[0m Trial 7 finished with values: [0.34754306077957153, 0.8873589941298792] and parameters: {'learning_rate': 1.9839175247031734e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 64, 'weight_decay': 0.005227926427303831}. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a new besttrial 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 256)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=21.0, min_trials=700\n",
            "params: {'learning_rate': 0.00019445902615149377, 'num_train_epochs': 3, 'per_device_train_batch_size': 256, 'weight_decay': 0.0013622259121800415}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='108' max='108' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [108/108 08:37, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.721700</td>\n",
              "      <td>0.930834</td>\n",
              "      <td>0.755837</td>\n",
              "      <td>0.722563</td>\n",
              "      <td>0.777028</td>\n",
              "      <td>0.718739</td>\n",
              "      <td>0.723868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.757500</td>\n",
              "      <td>0.499476</td>\n",
              "      <td>0.839494</td>\n",
              "      <td>0.825407</td>\n",
              "      <td>0.817273</td>\n",
              "      <td>0.843101</td>\n",
              "      <td>0.817734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.532300</td>\n",
              "      <td>0.490282</td>\n",
              "      <td>0.855058</td>\n",
              "      <td>0.846839</td>\n",
              "      <td>0.854288</td>\n",
              "      <td>0.853505</td>\n",
              "      <td>0.835805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.548300</td>\n",
              "      <td>0.444164</td>\n",
              "      <td>0.868677</td>\n",
              "      <td>0.865579</td>\n",
              "      <td>0.883054</td>\n",
              "      <td>0.852849</td>\n",
              "      <td>0.849730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.521300</td>\n",
              "      <td>0.424248</td>\n",
              "      <td>0.871595</td>\n",
              "      <td>0.864654</td>\n",
              "      <td>0.866530</td>\n",
              "      <td>0.868794</td>\n",
              "      <td>0.853469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.333600</td>\n",
              "      <td>0.356539</td>\n",
              "      <td>0.885214</td>\n",
              "      <td>0.880169</td>\n",
              "      <td>0.881189</td>\n",
              "      <td>0.881133</td>\n",
              "      <td>0.868659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.233900</td>\n",
              "      <td>0.341159</td>\n",
              "      <td>0.896887</td>\n",
              "      <td>0.888712</td>\n",
              "      <td>0.890353</td>\n",
              "      <td>0.887332</td>\n",
              "      <td>0.881827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.266700</td>\n",
              "      <td>0.354486</td>\n",
              "      <td>0.889105</td>\n",
              "      <td>0.884220</td>\n",
              "      <td>0.885375</td>\n",
              "      <td>0.885642</td>\n",
              "      <td>0.873361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.257700</td>\n",
              "      <td>0.350029</td>\n",
              "      <td>0.885214</td>\n",
              "      <td>0.880049</td>\n",
              "      <td>0.887442</td>\n",
              "      <td>0.875805</td>\n",
              "      <td>0.868720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.277400</td>\n",
              "      <td>0.334589</td>\n",
              "      <td>0.895914</td>\n",
              "      <td>0.892959</td>\n",
              "      <td>0.891801</td>\n",
              "      <td>0.895231</td>\n",
              "      <td>0.880890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.193600</td>\n",
              "      <td>0.339212</td>\n",
              "      <td>0.894942</td>\n",
              "      <td>0.891770</td>\n",
              "      <td>0.891838</td>\n",
              "      <td>0.892520</td>\n",
              "      <td>0.879672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>0.356910</td>\n",
              "      <td>0.894942</td>\n",
              "      <td>0.891906</td>\n",
              "      <td>0.896158</td>\n",
              "      <td>0.888601</td>\n",
              "      <td>0.879636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.131100</td>\n",
              "      <td>0.365188</td>\n",
              "      <td>0.896887</td>\n",
              "      <td>0.892723</td>\n",
              "      <td>0.892027</td>\n",
              "      <td>0.894244</td>\n",
              "      <td>0.881924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.108900</td>\n",
              "      <td>0.370245</td>\n",
              "      <td>0.897860</td>\n",
              "      <td>0.893781</td>\n",
              "      <td>0.895753</td>\n",
              "      <td>0.892911</td>\n",
              "      <td>0.882986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.096300</td>\n",
              "      <td>0.368861</td>\n",
              "      <td>0.900778</td>\n",
              "      <td>0.896819</td>\n",
              "      <td>0.901251</td>\n",
              "      <td>0.893396</td>\n",
              "      <td>0.886308</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 09:11:27,311]\u001b[0m Trial 8 finished with values: [0.3345893919467926, 0.8929587984285268] and parameters: {'learning_rate': 0.00019445902615149377, 'num_train_epochs': 3, 'per_device_train_batch_size': 256, 'weight_decay': 0.0013622259121800415}. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a new besttrial 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 16)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=345.0, min_trials=700\n",
            "params: {'learning_rate': 7.825169186318606e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 16, 'weight_decay': 0.007546565647762932}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1734' max='1734' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1734/1734 09:30, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>1.617300</td>\n",
              "      <td>1.014582</td>\n",
              "      <td>0.727626</td>\n",
              "      <td>0.664512</td>\n",
              "      <td>0.813271</td>\n",
              "      <td>0.649374</td>\n",
              "      <td>0.688747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.822500</td>\n",
              "      <td>0.581985</td>\n",
              "      <td>0.847276</td>\n",
              "      <td>0.836764</td>\n",
              "      <td>0.844410</td>\n",
              "      <td>0.833571</td>\n",
              "      <td>0.825160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>345</td>\n",
              "      <td>0.579600</td>\n",
              "      <td>0.483564</td>\n",
              "      <td>0.856031</td>\n",
              "      <td>0.848642</td>\n",
              "      <td>0.847230</td>\n",
              "      <td>0.853517</td>\n",
              "      <td>0.835610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.495600</td>\n",
              "      <td>0.453510</td>\n",
              "      <td>0.859922</td>\n",
              "      <td>0.855453</td>\n",
              "      <td>0.854114</td>\n",
              "      <td>0.864705</td>\n",
              "      <td>0.840951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>575</td>\n",
              "      <td>0.472900</td>\n",
              "      <td>0.410801</td>\n",
              "      <td>0.881323</td>\n",
              "      <td>0.877435</td>\n",
              "      <td>0.879990</td>\n",
              "      <td>0.876191</td>\n",
              "      <td>0.864160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>0.396500</td>\n",
              "      <td>0.388723</td>\n",
              "      <td>0.874514</td>\n",
              "      <td>0.870081</td>\n",
              "      <td>0.873734</td>\n",
              "      <td>0.867474</td>\n",
              "      <td>0.856264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>805</td>\n",
              "      <td>0.324600</td>\n",
              "      <td>0.375944</td>\n",
              "      <td>0.888132</td>\n",
              "      <td>0.881876</td>\n",
              "      <td>0.881395</td>\n",
              "      <td>0.883415</td>\n",
              "      <td>0.871924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>0.333700</td>\n",
              "      <td>0.389852</td>\n",
              "      <td>0.875486</td>\n",
              "      <td>0.868038</td>\n",
              "      <td>0.876171</td>\n",
              "      <td>0.865039</td>\n",
              "      <td>0.857648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1035</td>\n",
              "      <td>0.339700</td>\n",
              "      <td>0.376305</td>\n",
              "      <td>0.889105</td>\n",
              "      <td>0.881228</td>\n",
              "      <td>0.879708</td>\n",
              "      <td>0.885603</td>\n",
              "      <td>0.873220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.361300</td>\n",
              "      <td>0.374152</td>\n",
              "      <td>0.890078</td>\n",
              "      <td>0.884905</td>\n",
              "      <td>0.889565</td>\n",
              "      <td>0.882985</td>\n",
              "      <td>0.874181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1265</td>\n",
              "      <td>0.301300</td>\n",
              "      <td>0.367005</td>\n",
              "      <td>0.891051</td>\n",
              "      <td>0.885855</td>\n",
              "      <td>0.886732</td>\n",
              "      <td>0.887133</td>\n",
              "      <td>0.875377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1380</td>\n",
              "      <td>0.275500</td>\n",
              "      <td>0.365831</td>\n",
              "      <td>0.889105</td>\n",
              "      <td>0.884248</td>\n",
              "      <td>0.882917</td>\n",
              "      <td>0.887366</td>\n",
              "      <td>0.873133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1495</td>\n",
              "      <td>0.264000</td>\n",
              "      <td>0.367583</td>\n",
              "      <td>0.889105</td>\n",
              "      <td>0.884205</td>\n",
              "      <td>0.885093</td>\n",
              "      <td>0.884520</td>\n",
              "      <td>0.872962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1610</td>\n",
              "      <td>0.226500</td>\n",
              "      <td>0.374013</td>\n",
              "      <td>0.883268</td>\n",
              "      <td>0.878062</td>\n",
              "      <td>0.881010</td>\n",
              "      <td>0.877366</td>\n",
              "      <td>0.866317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1725</td>\n",
              "      <td>0.219900</td>\n",
              "      <td>0.368886</td>\n",
              "      <td>0.887160</td>\n",
              "      <td>0.881593</td>\n",
              "      <td>0.882394</td>\n",
              "      <td>0.882025</td>\n",
              "      <td>0.870747</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:05]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 09:21:34,076]\u001b[0m Trial 9 finished with values: [0.3658309578895569, 0.8842479162722792] and parameters: {'learning_rate': 7.825169186318606e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 16, 'weight_decay': 0.007546565647762932}. \u001b[0m\n",
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 64)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=84.0, min_trials=700\n",
            "params: {'learning_rate': 1.0477519000433616e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 64, 'weight_decay': 0.009508215679227439}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='432' max='432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [432/432 08:50, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.920100</td>\n",
              "      <td>1.577964</td>\n",
              "      <td>0.566148</td>\n",
              "      <td>0.375861</td>\n",
              "      <td>0.443247</td>\n",
              "      <td>0.435404</td>\n",
              "      <td>0.503658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>1.367800</td>\n",
              "      <td>1.046987</td>\n",
              "      <td>0.742218</td>\n",
              "      <td>0.677397</td>\n",
              "      <td>0.821775</td>\n",
              "      <td>0.661510</td>\n",
              "      <td>0.705316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.934100</td>\n",
              "      <td>0.728717</td>\n",
              "      <td>0.830739</td>\n",
              "      <td>0.813792</td>\n",
              "      <td>0.847918</td>\n",
              "      <td>0.800662</td>\n",
              "      <td>0.806066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.699200</td>\n",
              "      <td>0.585476</td>\n",
              "      <td>0.844358</td>\n",
              "      <td>0.832775</td>\n",
              "      <td>0.855934</td>\n",
              "      <td>0.822099</td>\n",
              "      <td>0.821660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.634800</td>\n",
              "      <td>0.521177</td>\n",
              "      <td>0.857004</td>\n",
              "      <td>0.849893</td>\n",
              "      <td>0.867443</td>\n",
              "      <td>0.840754</td>\n",
              "      <td>0.836691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>0.548000</td>\n",
              "      <td>0.458500</td>\n",
              "      <td>0.874514</td>\n",
              "      <td>0.867235</td>\n",
              "      <td>0.874861</td>\n",
              "      <td>0.862608</td>\n",
              "      <td>0.856204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>0.446800</td>\n",
              "      <td>0.437498</td>\n",
              "      <td>0.876459</td>\n",
              "      <td>0.869606</td>\n",
              "      <td>0.871122</td>\n",
              "      <td>0.870422</td>\n",
              "      <td>0.858539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>224</td>\n",
              "      <td>0.441700</td>\n",
              "      <td>0.421277</td>\n",
              "      <td>0.881323</td>\n",
              "      <td>0.876645</td>\n",
              "      <td>0.879510</td>\n",
              "      <td>0.875143</td>\n",
              "      <td>0.864007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>252</td>\n",
              "      <td>0.420000</td>\n",
              "      <td>0.410310</td>\n",
              "      <td>0.880350</td>\n",
              "      <td>0.875258</td>\n",
              "      <td>0.884876</td>\n",
              "      <td>0.868756</td>\n",
              "      <td>0.862850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.435100</td>\n",
              "      <td>0.403156</td>\n",
              "      <td>0.880350</td>\n",
              "      <td>0.872311</td>\n",
              "      <td>0.876801</td>\n",
              "      <td>0.871096</td>\n",
              "      <td>0.862978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>308</td>\n",
              "      <td>0.414900</td>\n",
              "      <td>0.392666</td>\n",
              "      <td>0.880350</td>\n",
              "      <td>0.875251</td>\n",
              "      <td>0.880930</td>\n",
              "      <td>0.871196</td>\n",
              "      <td>0.862849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>336</td>\n",
              "      <td>0.384000</td>\n",
              "      <td>0.389303</td>\n",
              "      <td>0.879377</td>\n",
              "      <td>0.874869</td>\n",
              "      <td>0.881087</td>\n",
              "      <td>0.870333</td>\n",
              "      <td>0.861738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>364</td>\n",
              "      <td>0.371100</td>\n",
              "      <td>0.386530</td>\n",
              "      <td>0.879377</td>\n",
              "      <td>0.871452</td>\n",
              "      <td>0.871871</td>\n",
              "      <td>0.872815</td>\n",
              "      <td>0.861868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>392</td>\n",
              "      <td>0.328600</td>\n",
              "      <td>0.383073</td>\n",
              "      <td>0.882296</td>\n",
              "      <td>0.875882</td>\n",
              "      <td>0.878006</td>\n",
              "      <td>0.874792</td>\n",
              "      <td>0.865118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.318100</td>\n",
              "      <td>0.383237</td>\n",
              "      <td>0.881323</td>\n",
              "      <td>0.875516</td>\n",
              "      <td>0.878566</td>\n",
              "      <td>0.873576</td>\n",
              "      <td>0.863988</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 09:30:36,621]\u001b[0m Trial 10 finished with values: [0.38307270407676697, 0.8758820664217609] and parameters: {'learning_rate': 1.0477519000433616e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 64, 'weight_decay': 0.009508215679227439}. \u001b[0m\n",
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 128)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=42.0, min_trials=700\n",
            "params: {'learning_rate': 0.0002494710113539506, 'num_train_epochs': 3, 'per_device_train_batch_size': 128, 'weight_decay': 0.0015729870867858232}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='216' max='216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [216/216 08:41, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.200400</td>\n",
              "      <td>0.651215</td>\n",
              "      <td>0.806420</td>\n",
              "      <td>0.794221</td>\n",
              "      <td>0.819898</td>\n",
              "      <td>0.795029</td>\n",
              "      <td>0.781097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.551300</td>\n",
              "      <td>0.501216</td>\n",
              "      <td>0.841440</td>\n",
              "      <td>0.842194</td>\n",
              "      <td>0.851620</td>\n",
              "      <td>0.839923</td>\n",
              "      <td>0.819992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.508500</td>\n",
              "      <td>0.454084</td>\n",
              "      <td>0.868677</td>\n",
              "      <td>0.867301</td>\n",
              "      <td>0.879274</td>\n",
              "      <td>0.860367</td>\n",
              "      <td>0.850064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.459900</td>\n",
              "      <td>0.508519</td>\n",
              "      <td>0.839494</td>\n",
              "      <td>0.832903</td>\n",
              "      <td>0.849342</td>\n",
              "      <td>0.833049</td>\n",
              "      <td>0.818108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.522000</td>\n",
              "      <td>0.484665</td>\n",
              "      <td>0.861868</td>\n",
              "      <td>0.854714</td>\n",
              "      <td>0.853139</td>\n",
              "      <td>0.866511</td>\n",
              "      <td>0.843174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.342600</td>\n",
              "      <td>0.396572</td>\n",
              "      <td>0.878405</td>\n",
              "      <td>0.869356</td>\n",
              "      <td>0.867819</td>\n",
              "      <td>0.875239</td>\n",
              "      <td>0.861334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.213600</td>\n",
              "      <td>0.370860</td>\n",
              "      <td>0.889105</td>\n",
              "      <td>0.885171</td>\n",
              "      <td>0.887076</td>\n",
              "      <td>0.886531</td>\n",
              "      <td>0.873452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.258200</td>\n",
              "      <td>0.463097</td>\n",
              "      <td>0.864786</td>\n",
              "      <td>0.862746</td>\n",
              "      <td>0.870137</td>\n",
              "      <td>0.870446</td>\n",
              "      <td>0.847314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>0.260100</td>\n",
              "      <td>0.362699</td>\n",
              "      <td>0.876459</td>\n",
              "      <td>0.870016</td>\n",
              "      <td>0.873334</td>\n",
              "      <td>0.870020</td>\n",
              "      <td>0.858996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.298900</td>\n",
              "      <td>0.349826</td>\n",
              "      <td>0.890078</td>\n",
              "      <td>0.884106</td>\n",
              "      <td>0.883875</td>\n",
              "      <td>0.884838</td>\n",
              "      <td>0.874132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>0.162600</td>\n",
              "      <td>0.382599</td>\n",
              "      <td>0.893969</td>\n",
              "      <td>0.888527</td>\n",
              "      <td>0.887564</td>\n",
              "      <td>0.893514</td>\n",
              "      <td>0.878874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>0.117900</td>\n",
              "      <td>0.405544</td>\n",
              "      <td>0.889105</td>\n",
              "      <td>0.889063</td>\n",
              "      <td>0.899060</td>\n",
              "      <td>0.882302</td>\n",
              "      <td>0.873224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>0.100600</td>\n",
              "      <td>0.398936</td>\n",
              "      <td>0.902724</td>\n",
              "      <td>0.898319</td>\n",
              "      <td>0.897139</td>\n",
              "      <td>0.903090</td>\n",
              "      <td>0.888997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>0.084500</td>\n",
              "      <td>0.401455</td>\n",
              "      <td>0.892023</td>\n",
              "      <td>0.890010</td>\n",
              "      <td>0.887203</td>\n",
              "      <td>0.895892</td>\n",
              "      <td>0.876627</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.084200</td>\n",
              "      <td>0.396232</td>\n",
              "      <td>0.894942</td>\n",
              "      <td>0.889451</td>\n",
              "      <td>0.890902</td>\n",
              "      <td>0.890188</td>\n",
              "      <td>0.879759</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 09:39:31,443]\u001b[0m Trial 11 finished with values: [0.34982600808143616, 0.8841063654586251] and parameters: {'learning_rate': 0.0002494710113539506, 'num_train_epochs': 3, 'per_device_train_batch_size': 128, 'weight_decay': 0.0015729870867858232}. \u001b[0m\n",
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 8)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=693.0, min_trials=700\n",
            "params: {'learning_rate': 0.00014655188624811084, 'num_train_epochs': 3, 'per_device_train_batch_size': 8, 'weight_decay': 0.0010617745061379659}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3468' max='3468' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3468/3468 10:18, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>231</td>\n",
              "      <td>1.055300</td>\n",
              "      <td>0.923973</td>\n",
              "      <td>0.731518</td>\n",
              "      <td>0.729669</td>\n",
              "      <td>0.749733</td>\n",
              "      <td>0.744515</td>\n",
              "      <td>0.701663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>462</td>\n",
              "      <td>0.997700</td>\n",
              "      <td>0.913319</td>\n",
              "      <td>0.731518</td>\n",
              "      <td>0.676095</td>\n",
              "      <td>0.691602</td>\n",
              "      <td>0.676751</td>\n",
              "      <td>0.692937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>693</td>\n",
              "      <td>0.877000</td>\n",
              "      <td>0.815250</td>\n",
              "      <td>0.784047</td>\n",
              "      <td>0.729187</td>\n",
              "      <td>0.776449</td>\n",
              "      <td>0.753816</td>\n",
              "      <td>0.755463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>924</td>\n",
              "      <td>0.793600</td>\n",
              "      <td>0.680532</td>\n",
              "      <td>0.837549</td>\n",
              "      <td>0.828279</td>\n",
              "      <td>0.822098</td>\n",
              "      <td>0.844417</td>\n",
              "      <td>0.815997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1155</td>\n",
              "      <td>0.727400</td>\n",
              "      <td>0.698633</td>\n",
              "      <td>0.818093</td>\n",
              "      <td>0.820687</td>\n",
              "      <td>0.835581</td>\n",
              "      <td>0.820754</td>\n",
              "      <td>0.794759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1386</td>\n",
              "      <td>0.564400</td>\n",
              "      <td>0.671614</td>\n",
              "      <td>0.844358</td>\n",
              "      <td>0.839103</td>\n",
              "      <td>0.840765</td>\n",
              "      <td>0.848327</td>\n",
              "      <td>0.823777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1617</td>\n",
              "      <td>0.531700</td>\n",
              "      <td>0.794205</td>\n",
              "      <td>0.841440</td>\n",
              "      <td>0.834110</td>\n",
              "      <td>0.862845</td>\n",
              "      <td>0.821085</td>\n",
              "      <td>0.819807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1848</td>\n",
              "      <td>0.541700</td>\n",
              "      <td>0.679851</td>\n",
              "      <td>0.860895</td>\n",
              "      <td>0.853230</td>\n",
              "      <td>0.859297</td>\n",
              "      <td>0.851285</td>\n",
              "      <td>0.840842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2079</td>\n",
              "      <td>0.545000</td>\n",
              "      <td>0.622204</td>\n",
              "      <td>0.847276</td>\n",
              "      <td>0.835203</td>\n",
              "      <td>0.858092</td>\n",
              "      <td>0.823682</td>\n",
              "      <td>0.825687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2310</td>\n",
              "      <td>0.519200</td>\n",
              "      <td>0.532936</td>\n",
              "      <td>0.878405</td>\n",
              "      <td>0.874476</td>\n",
              "      <td>0.883216</td>\n",
              "      <td>0.868360</td>\n",
              "      <td>0.860800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2541</td>\n",
              "      <td>0.370200</td>\n",
              "      <td>0.580077</td>\n",
              "      <td>0.876459</td>\n",
              "      <td>0.868499</td>\n",
              "      <td>0.868396</td>\n",
              "      <td>0.871837</td>\n",
              "      <td>0.858916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2772</td>\n",
              "      <td>0.333700</td>\n",
              "      <td>0.592976</td>\n",
              "      <td>0.877432</td>\n",
              "      <td>0.871985</td>\n",
              "      <td>0.872818</td>\n",
              "      <td>0.872436</td>\n",
              "      <td>0.859730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3003</td>\n",
              "      <td>0.304100</td>\n",
              "      <td>0.598603</td>\n",
              "      <td>0.877432</td>\n",
              "      <td>0.873441</td>\n",
              "      <td>0.871960</td>\n",
              "      <td>0.876287</td>\n",
              "      <td>0.859878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3234</td>\n",
              "      <td>0.240700</td>\n",
              "      <td>0.573324</td>\n",
              "      <td>0.891051</td>\n",
              "      <td>0.888353</td>\n",
              "      <td>0.891366</td>\n",
              "      <td>0.886225</td>\n",
              "      <td>0.875116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3465</td>\n",
              "      <td>0.244400</td>\n",
              "      <td>0.552538</td>\n",
              "      <td>0.892996</td>\n",
              "      <td>0.885715</td>\n",
              "      <td>0.886868</td>\n",
              "      <td>0.885210</td>\n",
              "      <td>0.877467</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 09:50:00,546]\u001b[0m Trial 12 finished with values: [0.5329359769821167, 0.8744756071184393] and parameters: {'learning_rate': 0.00014655188624811084, 'num_train_epochs': 3, 'per_device_train_batch_size': 8, 'weight_decay': 0.0010617745061379659}. \u001b[0m\n",
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 16)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=345.0, min_trials=700\n",
            "params: {'learning_rate': 0.00043802337191544045, 'num_train_epochs': 3, 'per_device_train_batch_size': 16, 'weight_decay': 0.0012533394094352084}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1734' max='1734' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1734/1734 09:31, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>2.224500</td>\n",
              "      <td>2.148651</td>\n",
              "      <td>0.163424</td>\n",
              "      <td>0.031215</td>\n",
              "      <td>0.018158</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>2.194000</td>\n",
              "      <td>2.202052</td>\n",
              "      <td>0.146887</td>\n",
              "      <td>0.028461</td>\n",
              "      <td>0.016321</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>345</td>\n",
              "      <td>2.185000</td>\n",
              "      <td>2.177348</td>\n",
              "      <td>0.137160</td>\n",
              "      <td>0.026804</td>\n",
              "      <td>0.015240</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>2.160500</td>\n",
              "      <td>2.158997</td>\n",
              "      <td>0.163424</td>\n",
              "      <td>0.031215</td>\n",
              "      <td>0.018158</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>575</td>\n",
              "      <td>2.155800</td>\n",
              "      <td>2.156488</td>\n",
              "      <td>0.137160</td>\n",
              "      <td>0.026804</td>\n",
              "      <td>0.015240</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>2.177300</td>\n",
              "      <td>2.150413</td>\n",
              "      <td>0.163424</td>\n",
              "      <td>0.031215</td>\n",
              "      <td>0.018158</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>805</td>\n",
              "      <td>2.165300</td>\n",
              "      <td>2.179559</td>\n",
              "      <td>0.163424</td>\n",
              "      <td>0.031215</td>\n",
              "      <td>0.018158</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>2.163300</td>\n",
              "      <td>2.153965</td>\n",
              "      <td>0.163424</td>\n",
              "      <td>0.031215</td>\n",
              "      <td>0.018158</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1035</td>\n",
              "      <td>2.144200</td>\n",
              "      <td>2.128993</td>\n",
              "      <td>0.163424</td>\n",
              "      <td>0.031215</td>\n",
              "      <td>0.018158</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>2.147300</td>\n",
              "      <td>2.135439</td>\n",
              "      <td>0.163424</td>\n",
              "      <td>0.031215</td>\n",
              "      <td>0.018158</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1265</td>\n",
              "      <td>2.150200</td>\n",
              "      <td>2.125899</td>\n",
              "      <td>0.163424</td>\n",
              "      <td>0.031215</td>\n",
              "      <td>0.018158</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1380</td>\n",
              "      <td>2.133000</td>\n",
              "      <td>2.131771</td>\n",
              "      <td>0.146887</td>\n",
              "      <td>0.028461</td>\n",
              "      <td>0.016321</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1495</td>\n",
              "      <td>2.140900</td>\n",
              "      <td>2.126368</td>\n",
              "      <td>0.163424</td>\n",
              "      <td>0.031215</td>\n",
              "      <td>0.018158</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1610</td>\n",
              "      <td>2.136300</td>\n",
              "      <td>2.124024</td>\n",
              "      <td>0.146887</td>\n",
              "      <td>0.028461</td>\n",
              "      <td>0.016321</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1725</td>\n",
              "      <td>2.131100</td>\n",
              "      <td>2.120209</td>\n",
              "      <td>0.163424</td>\n",
              "      <td>0.031215</td>\n",
              "      <td>0.018158</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "\u001b[32m[I 2022-01-29 09:59:44,814]\u001b[0m Trial 13 finished with values: [2.120208501815796, 0.031215161649944256] and parameters: {'learning_rate': 0.00043802337191544045, 'num_train_epochs': 3, 'per_device_train_batch_size': 16, 'weight_decay': 0.0012533394094352084}. \u001b[0m\n",
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 32)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=171.0, min_trials=700\n",
            "params: {'learning_rate': 0.0003233565932492428, 'num_train_epochs': 3, 'per_device_train_batch_size': 32, 'weight_decay': 0.003471968825336658}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='867' max='867' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [867/867 09:03, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>1.101600</td>\n",
              "      <td>0.836243</td>\n",
              "      <td>0.760700</td>\n",
              "      <td>0.747116</td>\n",
              "      <td>0.775057</td>\n",
              "      <td>0.781374</td>\n",
              "      <td>0.735428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>0.860400</td>\n",
              "      <td>0.783594</td>\n",
              "      <td>0.767510</td>\n",
              "      <td>0.723125</td>\n",
              "      <td>0.762475</td>\n",
              "      <td>0.739758</td>\n",
              "      <td>0.737682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>0.833600</td>\n",
              "      <td>0.894776</td>\n",
              "      <td>0.759728</td>\n",
              "      <td>0.740624</td>\n",
              "      <td>0.769781</td>\n",
              "      <td>0.750264</td>\n",
              "      <td>0.729723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>228</td>\n",
              "      <td>0.860000</td>\n",
              "      <td>0.833949</td>\n",
              "      <td>0.790856</td>\n",
              "      <td>0.788645</td>\n",
              "      <td>0.819206</td>\n",
              "      <td>0.792657</td>\n",
              "      <td>0.767097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>285</td>\n",
              "      <td>0.846400</td>\n",
              "      <td>0.736023</td>\n",
              "      <td>0.806420</td>\n",
              "      <td>0.801909</td>\n",
              "      <td>0.821856</td>\n",
              "      <td>0.801148</td>\n",
              "      <td>0.782143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>342</td>\n",
              "      <td>0.685900</td>\n",
              "      <td>0.885707</td>\n",
              "      <td>0.783074</td>\n",
              "      <td>0.777901</td>\n",
              "      <td>0.790188</td>\n",
              "      <td>0.795911</td>\n",
              "      <td>0.757697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>399</td>\n",
              "      <td>0.617200</td>\n",
              "      <td>0.816222</td>\n",
              "      <td>0.818093</td>\n",
              "      <td>0.806368</td>\n",
              "      <td>0.822414</td>\n",
              "      <td>0.811661</td>\n",
              "      <td>0.794055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>456</td>\n",
              "      <td>0.610300</td>\n",
              "      <td>0.672030</td>\n",
              "      <td>0.820039</td>\n",
              "      <td>0.815581</td>\n",
              "      <td>0.824549</td>\n",
              "      <td>0.814700</td>\n",
              "      <td>0.794786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>513</td>\n",
              "      <td>0.548100</td>\n",
              "      <td>0.660491</td>\n",
              "      <td>0.848249</td>\n",
              "      <td>0.840172</td>\n",
              "      <td>0.847402</td>\n",
              "      <td>0.841948</td>\n",
              "      <td>0.826610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.554800</td>\n",
              "      <td>0.585458</td>\n",
              "      <td>0.842412</td>\n",
              "      <td>0.839169</td>\n",
              "      <td>0.852542</td>\n",
              "      <td>0.830851</td>\n",
              "      <td>0.819988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>627</td>\n",
              "      <td>0.378700</td>\n",
              "      <td>0.578437</td>\n",
              "      <td>0.861868</td>\n",
              "      <td>0.861868</td>\n",
              "      <td>0.858912</td>\n",
              "      <td>0.868624</td>\n",
              "      <td>0.842560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>684</td>\n",
              "      <td>0.368000</td>\n",
              "      <td>0.550697</td>\n",
              "      <td>0.858949</td>\n",
              "      <td>0.859661</td>\n",
              "      <td>0.857611</td>\n",
              "      <td>0.868243</td>\n",
              "      <td>0.840003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>741</td>\n",
              "      <td>0.333800</td>\n",
              "      <td>0.538871</td>\n",
              "      <td>0.865759</td>\n",
              "      <td>0.861884</td>\n",
              "      <td>0.856513</td>\n",
              "      <td>0.871382</td>\n",
              "      <td>0.846705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>798</td>\n",
              "      <td>0.297600</td>\n",
              "      <td>0.500763</td>\n",
              "      <td>0.872568</td>\n",
              "      <td>0.871642</td>\n",
              "      <td>0.871031</td>\n",
              "      <td>0.873148</td>\n",
              "      <td>0.854114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>855</td>\n",
              "      <td>0.276600</td>\n",
              "      <td>0.482706</td>\n",
              "      <td>0.876459</td>\n",
              "      <td>0.876133</td>\n",
              "      <td>0.872036</td>\n",
              "      <td>0.881225</td>\n",
              "      <td>0.858738</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 10:09:00,460]\u001b[0m Trial 14 finished with values: [0.4827063977718353, 0.8761334664911256] and parameters: {'learning_rate': 0.0003233565932492428, 'num_train_epochs': 3, 'per_device_train_batch_size': 32, 'weight_decay': 0.003471968825336658}. \u001b[0m\n",
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 64)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=84.0, min_trials=700\n",
            "params: {'learning_rate': 5.5493780650768304e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 64, 'weight_decay': 0.002569654858971682}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='432' max='432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [432/432 08:50, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.230600</td>\n",
              "      <td>0.583036</td>\n",
              "      <td>0.824903</td>\n",
              "      <td>0.811167</td>\n",
              "      <td>0.835182</td>\n",
              "      <td>0.800775</td>\n",
              "      <td>0.800409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.541000</td>\n",
              "      <td>0.452381</td>\n",
              "      <td>0.852140</td>\n",
              "      <td>0.850881</td>\n",
              "      <td>0.857240</td>\n",
              "      <td>0.854669</td>\n",
              "      <td>0.832474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.460000</td>\n",
              "      <td>0.393792</td>\n",
              "      <td>0.878405</td>\n",
              "      <td>0.873061</td>\n",
              "      <td>0.878601</td>\n",
              "      <td>0.871568</td>\n",
              "      <td>0.861008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.422400</td>\n",
              "      <td>0.381267</td>\n",
              "      <td>0.880350</td>\n",
              "      <td>0.876925</td>\n",
              "      <td>0.878811</td>\n",
              "      <td>0.880306</td>\n",
              "      <td>0.863760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.451500</td>\n",
              "      <td>0.375862</td>\n",
              "      <td>0.882296</td>\n",
              "      <td>0.877641</td>\n",
              "      <td>0.875747</td>\n",
              "      <td>0.882337</td>\n",
              "      <td>0.865494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>0.276800</td>\n",
              "      <td>0.357285</td>\n",
              "      <td>0.877432</td>\n",
              "      <td>0.874290</td>\n",
              "      <td>0.875464</td>\n",
              "      <td>0.874978</td>\n",
              "      <td>0.859831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>0.217200</td>\n",
              "      <td>0.354184</td>\n",
              "      <td>0.894942</td>\n",
              "      <td>0.888815</td>\n",
              "      <td>0.884587</td>\n",
              "      <td>0.893886</td>\n",
              "      <td>0.879787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>224</td>\n",
              "      <td>0.238000</td>\n",
              "      <td>0.356710</td>\n",
              "      <td>0.893969</td>\n",
              "      <td>0.888736</td>\n",
              "      <td>0.885248</td>\n",
              "      <td>0.894593</td>\n",
              "      <td>0.878844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>252</td>\n",
              "      <td>0.228600</td>\n",
              "      <td>0.331659</td>\n",
              "      <td>0.896887</td>\n",
              "      <td>0.892923</td>\n",
              "      <td>0.897899</td>\n",
              "      <td>0.889343</td>\n",
              "      <td>0.881911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.253100</td>\n",
              "      <td>0.335413</td>\n",
              "      <td>0.904669</td>\n",
              "      <td>0.898692</td>\n",
              "      <td>0.900288</td>\n",
              "      <td>0.897812</td>\n",
              "      <td>0.890782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>308</td>\n",
              "      <td>0.173800</td>\n",
              "      <td>0.324848</td>\n",
              "      <td>0.905642</td>\n",
              "      <td>0.900895</td>\n",
              "      <td>0.901059</td>\n",
              "      <td>0.901872</td>\n",
              "      <td>0.892036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>336</td>\n",
              "      <td>0.114800</td>\n",
              "      <td>0.353647</td>\n",
              "      <td>0.904669</td>\n",
              "      <td>0.900682</td>\n",
              "      <td>0.902415</td>\n",
              "      <td>0.900377</td>\n",
              "      <td>0.890999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>364</td>\n",
              "      <td>0.114800</td>\n",
              "      <td>0.358079</td>\n",
              "      <td>0.899805</td>\n",
              "      <td>0.895170</td>\n",
              "      <td>0.895556</td>\n",
              "      <td>0.895451</td>\n",
              "      <td>0.885231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>392</td>\n",
              "      <td>0.098600</td>\n",
              "      <td>0.360817</td>\n",
              "      <td>0.893969</td>\n",
              "      <td>0.888375</td>\n",
              "      <td>0.889179</td>\n",
              "      <td>0.888346</td>\n",
              "      <td>0.878546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.097800</td>\n",
              "      <td>0.364827</td>\n",
              "      <td>0.897860</td>\n",
              "      <td>0.892429</td>\n",
              "      <td>0.893721</td>\n",
              "      <td>0.892234</td>\n",
              "      <td>0.883054</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 10:18:03,141]\u001b[0m Trial 15 finished with values: [0.32484832406044006, 0.9008951799368575] and parameters: {'learning_rate': 5.5493780650768304e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 64, 'weight_decay': 0.002569654858971682}. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a new besttrial 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 8)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=693.0, min_trials=700\n",
            "params: {'learning_rate': 5.753701304398057e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 8, 'weight_decay': 0.003686267331294336}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3468' max='3468' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3468/3468 10:27, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>231</td>\n",
              "      <td>0.835500</td>\n",
              "      <td>0.569053</td>\n",
              "      <td>0.826848</td>\n",
              "      <td>0.819821</td>\n",
              "      <td>0.817811</td>\n",
              "      <td>0.833299</td>\n",
              "      <td>0.803613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>462</td>\n",
              "      <td>0.570000</td>\n",
              "      <td>0.490939</td>\n",
              "      <td>0.857004</td>\n",
              "      <td>0.843325</td>\n",
              "      <td>0.838746</td>\n",
              "      <td>0.853056</td>\n",
              "      <td>0.837129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>693</td>\n",
              "      <td>0.572400</td>\n",
              "      <td>0.497278</td>\n",
              "      <td>0.860895</td>\n",
              "      <td>0.859116</td>\n",
              "      <td>0.855760</td>\n",
              "      <td>0.864209</td>\n",
              "      <td>0.840878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>924</td>\n",
              "      <td>0.504000</td>\n",
              "      <td>0.467499</td>\n",
              "      <td>0.871595</td>\n",
              "      <td>0.866891</td>\n",
              "      <td>0.867236</td>\n",
              "      <td>0.872363</td>\n",
              "      <td>0.853567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1155</td>\n",
              "      <td>0.545300</td>\n",
              "      <td>0.473322</td>\n",
              "      <td>0.871595</td>\n",
              "      <td>0.870758</td>\n",
              "      <td>0.872994</td>\n",
              "      <td>0.872289</td>\n",
              "      <td>0.853677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1386</td>\n",
              "      <td>0.305800</td>\n",
              "      <td>0.561848</td>\n",
              "      <td>0.882296</td>\n",
              "      <td>0.878322</td>\n",
              "      <td>0.884681</td>\n",
              "      <td>0.877951</td>\n",
              "      <td>0.866081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1617</td>\n",
              "      <td>0.259900</td>\n",
              "      <td>0.506452</td>\n",
              "      <td>0.896887</td>\n",
              "      <td>0.891150</td>\n",
              "      <td>0.891135</td>\n",
              "      <td>0.891834</td>\n",
              "      <td>0.881911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1848</td>\n",
              "      <td>0.315200</td>\n",
              "      <td>0.504459</td>\n",
              "      <td>0.890078</td>\n",
              "      <td>0.883756</td>\n",
              "      <td>0.883677</td>\n",
              "      <td>0.887632</td>\n",
              "      <td>0.874455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2079</td>\n",
              "      <td>0.299000</td>\n",
              "      <td>0.546737</td>\n",
              "      <td>0.882296</td>\n",
              "      <td>0.879635</td>\n",
              "      <td>0.880337</td>\n",
              "      <td>0.880929</td>\n",
              "      <td>0.865537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2310</td>\n",
              "      <td>0.307100</td>\n",
              "      <td>0.491970</td>\n",
              "      <td>0.892996</td>\n",
              "      <td>0.891925</td>\n",
              "      <td>0.892990</td>\n",
              "      <td>0.892873</td>\n",
              "      <td>0.877763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2541</td>\n",
              "      <td>0.121400</td>\n",
              "      <td>0.549364</td>\n",
              "      <td>0.902724</td>\n",
              "      <td>0.898816</td>\n",
              "      <td>0.898194</td>\n",
              "      <td>0.900006</td>\n",
              "      <td>0.888641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2772</td>\n",
              "      <td>0.126200</td>\n",
              "      <td>0.577792</td>\n",
              "      <td>0.893969</td>\n",
              "      <td>0.888291</td>\n",
              "      <td>0.885109</td>\n",
              "      <td>0.892968</td>\n",
              "      <td>0.878696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3003</td>\n",
              "      <td>0.123100</td>\n",
              "      <td>0.599613</td>\n",
              "      <td>0.892023</td>\n",
              "      <td>0.887583</td>\n",
              "      <td>0.885962</td>\n",
              "      <td>0.890055</td>\n",
              "      <td>0.876346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3234</td>\n",
              "      <td>0.112300</td>\n",
              "      <td>0.598305</td>\n",
              "      <td>0.893969</td>\n",
              "      <td>0.888861</td>\n",
              "      <td>0.889198</td>\n",
              "      <td>0.890080</td>\n",
              "      <td>0.878575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3465</td>\n",
              "      <td>0.093700</td>\n",
              "      <td>0.590788</td>\n",
              "      <td>0.893969</td>\n",
              "      <td>0.889847</td>\n",
              "      <td>0.889635</td>\n",
              "      <td>0.890913</td>\n",
              "      <td>0.878561</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 10:29:06,903]\u001b[0m Trial 16 finished with values: [0.4674991965293884, 0.8668905491969076] and parameters: {'learning_rate': 5.753701304398057e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 8, 'weight_decay': 0.003686267331294336}. \u001b[0m\n",
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 64)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=84.0, min_trials=700\n",
            "params: {'learning_rate': 6.847655329697433e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 64, 'weight_decay': 0.002097878125077771}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='432' max='432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [432/432 08:47, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>2.014900</td>\n",
              "      <td>1.790944</td>\n",
              "      <td>0.525292</td>\n",
              "      <td>0.348995</td>\n",
              "      <td>0.322584</td>\n",
              "      <td>0.404050</td>\n",
              "      <td>0.457951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>1.642500</td>\n",
              "      <td>1.380370</td>\n",
              "      <td>0.607977</td>\n",
              "      <td>0.452251</td>\n",
              "      <td>0.559578</td>\n",
              "      <td>0.484974</td>\n",
              "      <td>0.551437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>1.271400</td>\n",
              "      <td>1.046559</td>\n",
              "      <td>0.743191</td>\n",
              "      <td>0.685175</td>\n",
              "      <td>0.820142</td>\n",
              "      <td>0.670657</td>\n",
              "      <td>0.706290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.977200</td>\n",
              "      <td>0.825887</td>\n",
              "      <td>0.804475</td>\n",
              "      <td>0.773185</td>\n",
              "      <td>0.848543</td>\n",
              "      <td>0.754936</td>\n",
              "      <td>0.776448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.841900</td>\n",
              "      <td>0.699804</td>\n",
              "      <td>0.827821</td>\n",
              "      <td>0.811426</td>\n",
              "      <td>0.851929</td>\n",
              "      <td>0.797798</td>\n",
              "      <td>0.803333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>0.733400</td>\n",
              "      <td>0.607090</td>\n",
              "      <td>0.843385</td>\n",
              "      <td>0.829907</td>\n",
              "      <td>0.849021</td>\n",
              "      <td>0.821162</td>\n",
              "      <td>0.820506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>0.603900</td>\n",
              "      <td>0.556377</td>\n",
              "      <td>0.851167</td>\n",
              "      <td>0.843846</td>\n",
              "      <td>0.856123</td>\n",
              "      <td>0.836845</td>\n",
              "      <td>0.829461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>224</td>\n",
              "      <td>0.574500</td>\n",
              "      <td>0.521970</td>\n",
              "      <td>0.856031</td>\n",
              "      <td>0.849924</td>\n",
              "      <td>0.860033</td>\n",
              "      <td>0.843679</td>\n",
              "      <td>0.834923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>252</td>\n",
              "      <td>0.544600</td>\n",
              "      <td>0.501139</td>\n",
              "      <td>0.861868</td>\n",
              "      <td>0.856722</td>\n",
              "      <td>0.873529</td>\n",
              "      <td>0.845981</td>\n",
              "      <td>0.841702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.546700</td>\n",
              "      <td>0.479985</td>\n",
              "      <td>0.869650</td>\n",
              "      <td>0.860749</td>\n",
              "      <td>0.869679</td>\n",
              "      <td>0.856032</td>\n",
              "      <td>0.850671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>308</td>\n",
              "      <td>0.528700</td>\n",
              "      <td>0.463395</td>\n",
              "      <td>0.866732</td>\n",
              "      <td>0.860874</td>\n",
              "      <td>0.869234</td>\n",
              "      <td>0.854687</td>\n",
              "      <td>0.847189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>336</td>\n",
              "      <td>0.494600</td>\n",
              "      <td>0.456919</td>\n",
              "      <td>0.871595</td>\n",
              "      <td>0.864771</td>\n",
              "      <td>0.871860</td>\n",
              "      <td>0.860078</td>\n",
              "      <td>0.852820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>364</td>\n",
              "      <td>0.489500</td>\n",
              "      <td>0.448558</td>\n",
              "      <td>0.871595</td>\n",
              "      <td>0.864620</td>\n",
              "      <td>0.868697</td>\n",
              "      <td>0.862660</td>\n",
              "      <td>0.852895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>392</td>\n",
              "      <td>0.441400</td>\n",
              "      <td>0.445792</td>\n",
              "      <td>0.871595</td>\n",
              "      <td>0.865986</td>\n",
              "      <td>0.872889</td>\n",
              "      <td>0.861201</td>\n",
              "      <td>0.852813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.419300</td>\n",
              "      <td>0.444755</td>\n",
              "      <td>0.873541</td>\n",
              "      <td>0.867992</td>\n",
              "      <td>0.875324</td>\n",
              "      <td>0.862792</td>\n",
              "      <td>0.855048</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 10:38:08,394]\u001b[0m Trial 17 finished with values: [0.44475504755973816, 0.8679923613492703] and parameters: {'learning_rate': 6.847655329697433e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 64, 'weight_decay': 0.002097878125077771}. \u001b[0m\n",
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 128)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=42.0, min_trials=700\n",
            "params: {'learning_rate': 0.0001657259967540948, 'num_train_epochs': 3, 'per_device_train_batch_size': 128, 'weight_decay': 0.002449348556800101}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='216' max='216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [216/216 08:42, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.231800</td>\n",
              "      <td>0.688053</td>\n",
              "      <td>0.772374</td>\n",
              "      <td>0.744273</td>\n",
              "      <td>0.813902</td>\n",
              "      <td>0.727693</td>\n",
              "      <td>0.743985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.563500</td>\n",
              "      <td>0.473883</td>\n",
              "      <td>0.847276</td>\n",
              "      <td>0.844023</td>\n",
              "      <td>0.849226</td>\n",
              "      <td>0.847569</td>\n",
              "      <td>0.826930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.505400</td>\n",
              "      <td>0.462308</td>\n",
              "      <td>0.857004</td>\n",
              "      <td>0.850328</td>\n",
              "      <td>0.848818</td>\n",
              "      <td>0.862751</td>\n",
              "      <td>0.837808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.443900</td>\n",
              "      <td>0.427550</td>\n",
              "      <td>0.865759</td>\n",
              "      <td>0.859000</td>\n",
              "      <td>0.859021</td>\n",
              "      <td>0.867777</td>\n",
              "      <td>0.847954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.499400</td>\n",
              "      <td>0.397725</td>\n",
              "      <td>0.881323</td>\n",
              "      <td>0.874990</td>\n",
              "      <td>0.874083</td>\n",
              "      <td>0.880102</td>\n",
              "      <td>0.864609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.292800</td>\n",
              "      <td>0.351229</td>\n",
              "      <td>0.879377</td>\n",
              "      <td>0.872462</td>\n",
              "      <td>0.871951</td>\n",
              "      <td>0.875836</td>\n",
              "      <td>0.862069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.190900</td>\n",
              "      <td>0.382538</td>\n",
              "      <td>0.883268</td>\n",
              "      <td>0.877818</td>\n",
              "      <td>0.875766</td>\n",
              "      <td>0.881348</td>\n",
              "      <td>0.866426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.260100</td>\n",
              "      <td>0.366226</td>\n",
              "      <td>0.891051</td>\n",
              "      <td>0.884148</td>\n",
              "      <td>0.887001</td>\n",
              "      <td>0.883405</td>\n",
              "      <td>0.875389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>0.227900</td>\n",
              "      <td>0.350769</td>\n",
              "      <td>0.901751</td>\n",
              "      <td>0.895837</td>\n",
              "      <td>0.896713</td>\n",
              "      <td>0.898863</td>\n",
              "      <td>0.888014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.244400</td>\n",
              "      <td>0.346802</td>\n",
              "      <td>0.895914</td>\n",
              "      <td>0.891504</td>\n",
              "      <td>0.890571</td>\n",
              "      <td>0.894765</td>\n",
              "      <td>0.881034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>0.154600</td>\n",
              "      <td>0.357728</td>\n",
              "      <td>0.894942</td>\n",
              "      <td>0.889346</td>\n",
              "      <td>0.887199</td>\n",
              "      <td>0.892400</td>\n",
              "      <td>0.879753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>0.096300</td>\n",
              "      <td>0.372246</td>\n",
              "      <td>0.900778</td>\n",
              "      <td>0.897029</td>\n",
              "      <td>0.901917</td>\n",
              "      <td>0.893890</td>\n",
              "      <td>0.886495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>0.104100</td>\n",
              "      <td>0.398740</td>\n",
              "      <td>0.887160</td>\n",
              "      <td>0.881839</td>\n",
              "      <td>0.881720</td>\n",
              "      <td>0.884443</td>\n",
              "      <td>0.870871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>0.093500</td>\n",
              "      <td>0.385039</td>\n",
              "      <td>0.892996</td>\n",
              "      <td>0.889170</td>\n",
              "      <td>0.888980</td>\n",
              "      <td>0.889935</td>\n",
              "      <td>0.877459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.071500</td>\n",
              "      <td>0.387542</td>\n",
              "      <td>0.895914</td>\n",
              "      <td>0.891913</td>\n",
              "      <td>0.893741</td>\n",
              "      <td>0.890758</td>\n",
              "      <td>0.880720</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 10:47:03,432]\u001b[0m Trial 18 finished with values: [0.3468015193939209, 0.8915038847375549] and parameters: {'learning_rate': 0.0001657259967540948, 'num_train_epochs': 3, 'per_device_train_batch_size': 128, 'weight_decay': 0.002449348556800101}. \u001b[0m\n",
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 256)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=21.0, min_trials=700\n",
            "params: {'learning_rate': 9.749898871510996e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 256, 'weight_decay': 0.001550196862429288}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='108' max='108' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [108/108 08:35, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.117600</td>\n",
              "      <td>2.003444</td>\n",
              "      <td>0.396887</td>\n",
              "      <td>0.265767</td>\n",
              "      <td>0.300071</td>\n",
              "      <td>0.300116</td>\n",
              "      <td>0.313118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.949200</td>\n",
              "      <td>1.825149</td>\n",
              "      <td>0.507782</td>\n",
              "      <td>0.337440</td>\n",
              "      <td>0.318265</td>\n",
              "      <td>0.388211</td>\n",
              "      <td>0.436204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.797700</td>\n",
              "      <td>1.638196</td>\n",
              "      <td>0.567121</td>\n",
              "      <td>0.382722</td>\n",
              "      <td>0.423992</td>\n",
              "      <td>0.437096</td>\n",
              "      <td>0.503051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.601400</td>\n",
              "      <td>1.471394</td>\n",
              "      <td>0.613813</td>\n",
              "      <td>0.445226</td>\n",
              "      <td>0.559541</td>\n",
              "      <td>0.483255</td>\n",
              "      <td>0.557461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.465500</td>\n",
              "      <td>1.316723</td>\n",
              "      <td>0.655642</td>\n",
              "      <td>0.507265</td>\n",
              "      <td>0.584670</td>\n",
              "      <td>0.529842</td>\n",
              "      <td>0.606360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.354700</td>\n",
              "      <td>1.187138</td>\n",
              "      <td>0.686770</td>\n",
              "      <td>0.562360</td>\n",
              "      <td>0.682349</td>\n",
              "      <td>0.573182</td>\n",
              "      <td>0.641939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>1.179300</td>\n",
              "      <td>1.077290</td>\n",
              "      <td>0.737354</td>\n",
              "      <td>0.650754</td>\n",
              "      <td>0.824202</td>\n",
              "      <td>0.640032</td>\n",
              "      <td>0.699845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>1.096500</td>\n",
              "      <td>0.988802</td>\n",
              "      <td>0.769455</td>\n",
              "      <td>0.710980</td>\n",
              "      <td>0.831139</td>\n",
              "      <td>0.693240</td>\n",
              "      <td>0.736339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>1.004600</td>\n",
              "      <td>0.921725</td>\n",
              "      <td>0.788911</td>\n",
              "      <td>0.746648</td>\n",
              "      <td>0.838257</td>\n",
              "      <td>0.729274</td>\n",
              "      <td>0.758444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.979700</td>\n",
              "      <td>0.861530</td>\n",
              "      <td>0.808366</td>\n",
              "      <td>0.776703</td>\n",
              "      <td>0.846542</td>\n",
              "      <td>0.759548</td>\n",
              "      <td>0.780623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.923500</td>\n",
              "      <td>0.823283</td>\n",
              "      <td>0.817121</td>\n",
              "      <td>0.788919</td>\n",
              "      <td>0.848588</td>\n",
              "      <td>0.774055</td>\n",
              "      <td>0.790585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.871400</td>\n",
              "      <td>0.788719</td>\n",
              "      <td>0.826848</td>\n",
              "      <td>0.801860</td>\n",
              "      <td>0.855743</td>\n",
              "      <td>0.787472</td>\n",
              "      <td>0.801691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.864800</td>\n",
              "      <td>0.766540</td>\n",
              "      <td>0.824903</td>\n",
              "      <td>0.801681</td>\n",
              "      <td>0.851766</td>\n",
              "      <td>0.788281</td>\n",
              "      <td>0.799419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.811100</td>\n",
              "      <td>0.753057</td>\n",
              "      <td>0.826848</td>\n",
              "      <td>0.804497</td>\n",
              "      <td>0.850399</td>\n",
              "      <td>0.791170</td>\n",
              "      <td>0.801634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.765300</td>\n",
              "      <td>0.746491</td>\n",
              "      <td>0.827821</td>\n",
              "      <td>0.806660</td>\n",
              "      <td>0.851928</td>\n",
              "      <td>0.792401</td>\n",
              "      <td>0.802759</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 10:55:53,857]\u001b[0m Trial 19 finished with values: [0.7464913129806519, 0.8066600481539498] and parameters: {'learning_rate': 9.749898871510996e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 256, 'weight_decay': 0.001550196862429288}. \u001b[0m\n",
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 64)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=84.0, min_trials=700\n",
            "params: {'learning_rate': 0.0003037332858230308, 'num_train_epochs': 3, 'per_device_train_batch_size': 64, 'weight_decay': 0.0027847250456737114}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='432' max='432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [432/432 08:48, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.247300</td>\n",
              "      <td>0.852071</td>\n",
              "      <td>0.703307</td>\n",
              "      <td>0.696037</td>\n",
              "      <td>0.819718</td>\n",
              "      <td>0.689803</td>\n",
              "      <td>0.677776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.688700</td>\n",
              "      <td>0.704442</td>\n",
              "      <td>0.787938</td>\n",
              "      <td>0.782815</td>\n",
              "      <td>0.796108</td>\n",
              "      <td>0.791335</td>\n",
              "      <td>0.760095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.738400</td>\n",
              "      <td>0.625297</td>\n",
              "      <td>0.835603</td>\n",
              "      <td>0.831170</td>\n",
              "      <td>0.832211</td>\n",
              "      <td>0.837319</td>\n",
              "      <td>0.812852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.616200</td>\n",
              "      <td>0.522565</td>\n",
              "      <td>0.843385</td>\n",
              "      <td>0.835585</td>\n",
              "      <td>0.836123</td>\n",
              "      <td>0.845419</td>\n",
              "      <td>0.822268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.605200</td>\n",
              "      <td>0.582338</td>\n",
              "      <td>0.841440</td>\n",
              "      <td>0.835354</td>\n",
              "      <td>0.860964</td>\n",
              "      <td>0.818817</td>\n",
              "      <td>0.818234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>0.475900</td>\n",
              "      <td>0.524533</td>\n",
              "      <td>0.856031</td>\n",
              "      <td>0.841953</td>\n",
              "      <td>0.848916</td>\n",
              "      <td>0.842331</td>\n",
              "      <td>0.835800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>0.371000</td>\n",
              "      <td>0.514829</td>\n",
              "      <td>0.859922</td>\n",
              "      <td>0.855911</td>\n",
              "      <td>0.864592</td>\n",
              "      <td>0.852268</td>\n",
              "      <td>0.839773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>224</td>\n",
              "      <td>0.365300</td>\n",
              "      <td>0.509692</td>\n",
              "      <td>0.843385</td>\n",
              "      <td>0.841710</td>\n",
              "      <td>0.854512</td>\n",
              "      <td>0.843238</td>\n",
              "      <td>0.822418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>252</td>\n",
              "      <td>0.331800</td>\n",
              "      <td>0.498224</td>\n",
              "      <td>0.860895</td>\n",
              "      <td>0.853491</td>\n",
              "      <td>0.841831</td>\n",
              "      <td>0.872643</td>\n",
              "      <td>0.842056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.375500</td>\n",
              "      <td>0.427407</td>\n",
              "      <td>0.879377</td>\n",
              "      <td>0.875154</td>\n",
              "      <td>0.874369</td>\n",
              "      <td>0.879854</td>\n",
              "      <td>0.862483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>308</td>\n",
              "      <td>0.233200</td>\n",
              "      <td>0.513316</td>\n",
              "      <td>0.870623</td>\n",
              "      <td>0.868716</td>\n",
              "      <td>0.880459</td>\n",
              "      <td>0.864350</td>\n",
              "      <td>0.853081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>336</td>\n",
              "      <td>0.193600</td>\n",
              "      <td>0.488428</td>\n",
              "      <td>0.867704</td>\n",
              "      <td>0.859949</td>\n",
              "      <td>0.853767</td>\n",
              "      <td>0.869029</td>\n",
              "      <td>0.848894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>364</td>\n",
              "      <td>0.157900</td>\n",
              "      <td>0.468846</td>\n",
              "      <td>0.886187</td>\n",
              "      <td>0.882997</td>\n",
              "      <td>0.886854</td>\n",
              "      <td>0.881156</td>\n",
              "      <td>0.869747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>392</td>\n",
              "      <td>0.136400</td>\n",
              "      <td>0.481449</td>\n",
              "      <td>0.885214</td>\n",
              "      <td>0.878023</td>\n",
              "      <td>0.878780</td>\n",
              "      <td>0.881558</td>\n",
              "      <td>0.868765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.117600</td>\n",
              "      <td>0.454866</td>\n",
              "      <td>0.888132</td>\n",
              "      <td>0.886265</td>\n",
              "      <td>0.885226</td>\n",
              "      <td>0.888151</td>\n",
              "      <td>0.871929</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 11:04:54,847]\u001b[0m Trial 20 finished with values: [0.4274071753025055, 0.8751540410253789] and parameters: {'learning_rate': 0.0003037332858230308, 'num_train_epochs': 3, 'per_device_train_batch_size': 64, 'weight_decay': 0.0027847250456737114}. \u001b[0m\n",
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 128)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=42.0, min_trials=700\n",
            "params: {'learning_rate': 3.3705205014236195e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 128, 'weight_decay': 0.0015353540566317835}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='216' max='216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [216/216 08:41, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.746700</td>\n",
              "      <td>1.201802</td>\n",
              "      <td>0.657588</td>\n",
              "      <td>0.530257</td>\n",
              "      <td>0.807584</td>\n",
              "      <td>0.541821</td>\n",
              "      <td>0.611140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.980800</td>\n",
              "      <td>0.667858</td>\n",
              "      <td>0.824903</td>\n",
              "      <td>0.806958</td>\n",
              "      <td>0.840651</td>\n",
              "      <td>0.798366</td>\n",
              "      <td>0.799782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.622800</td>\n",
              "      <td>0.535765</td>\n",
              "      <td>0.837549</td>\n",
              "      <td>0.833475</td>\n",
              "      <td>0.841987</td>\n",
              "      <td>0.831569</td>\n",
              "      <td>0.814763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.529200</td>\n",
              "      <td>0.443990</td>\n",
              "      <td>0.864786</td>\n",
              "      <td>0.859316</td>\n",
              "      <td>0.872104</td>\n",
              "      <td>0.850203</td>\n",
              "      <td>0.845209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.500300</td>\n",
              "      <td>0.413560</td>\n",
              "      <td>0.873541</td>\n",
              "      <td>0.865987</td>\n",
              "      <td>0.871901</td>\n",
              "      <td>0.865044</td>\n",
              "      <td>0.855451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.403300</td>\n",
              "      <td>0.380163</td>\n",
              "      <td>0.885214</td>\n",
              "      <td>0.878215</td>\n",
              "      <td>0.877934</td>\n",
              "      <td>0.880300</td>\n",
              "      <td>0.868639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.310600</td>\n",
              "      <td>0.365028</td>\n",
              "      <td>0.884241</td>\n",
              "      <td>0.877830</td>\n",
              "      <td>0.879936</td>\n",
              "      <td>0.877592</td>\n",
              "      <td>0.867439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.314800</td>\n",
              "      <td>0.357572</td>\n",
              "      <td>0.888132</td>\n",
              "      <td>0.882476</td>\n",
              "      <td>0.883761</td>\n",
              "      <td>0.881888</td>\n",
              "      <td>0.871903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>0.300400</td>\n",
              "      <td>0.359294</td>\n",
              "      <td>0.888132</td>\n",
              "      <td>0.880857</td>\n",
              "      <td>0.888564</td>\n",
              "      <td>0.875423</td>\n",
              "      <td>0.871751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.338100</td>\n",
              "      <td>0.355810</td>\n",
              "      <td>0.891051</td>\n",
              "      <td>0.882733</td>\n",
              "      <td>0.881536</td>\n",
              "      <td>0.886129</td>\n",
              "      <td>0.875393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>0.288700</td>\n",
              "      <td>0.343761</td>\n",
              "      <td>0.893969</td>\n",
              "      <td>0.887929</td>\n",
              "      <td>0.890125</td>\n",
              "      <td>0.886395</td>\n",
              "      <td>0.878496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>0.245300</td>\n",
              "      <td>0.341571</td>\n",
              "      <td>0.896887</td>\n",
              "      <td>0.891803</td>\n",
              "      <td>0.892902</td>\n",
              "      <td>0.891397</td>\n",
              "      <td>0.881902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>0.225800</td>\n",
              "      <td>0.347159</td>\n",
              "      <td>0.892996</td>\n",
              "      <td>0.885451</td>\n",
              "      <td>0.885284</td>\n",
              "      <td>0.887394</td>\n",
              "      <td>0.877537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>0.200200</td>\n",
              "      <td>0.348325</td>\n",
              "      <td>0.896887</td>\n",
              "      <td>0.890760</td>\n",
              "      <td>0.894162</td>\n",
              "      <td>0.889157</td>\n",
              "      <td>0.881945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.193900</td>\n",
              "      <td>0.349096</td>\n",
              "      <td>0.895914</td>\n",
              "      <td>0.890115</td>\n",
              "      <td>0.895278</td>\n",
              "      <td>0.886579</td>\n",
              "      <td>0.880778</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 11:13:48,746]\u001b[0m Trial 21 finished with values: [0.34157106280326843, 0.891803054735122] and parameters: {'learning_rate': 3.3705205014236195e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 128, 'weight_decay': 0.0015353540566317835}. \u001b[0m\n",
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 16)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=345.0, min_trials=700\n",
            "params: {'learning_rate': 0.00010385370797127183, 'num_train_epochs': 3, 'per_device_train_batch_size': 16, 'weight_decay': 0.004271677459604317}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1734' max='1734' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1734/1734 09:26, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.836400</td>\n",
              "      <td>0.604095</td>\n",
              "      <td>0.819066</td>\n",
              "      <td>0.805845</td>\n",
              "      <td>0.808588</td>\n",
              "      <td>0.821879</td>\n",
              "      <td>0.795966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.607700</td>\n",
              "      <td>0.512416</td>\n",
              "      <td>0.856031</td>\n",
              "      <td>0.843052</td>\n",
              "      <td>0.843764</td>\n",
              "      <td>0.850921</td>\n",
              "      <td>0.836259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>345</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.537280</td>\n",
              "      <td>0.829767</td>\n",
              "      <td>0.829835</td>\n",
              "      <td>0.832786</td>\n",
              "      <td>0.837538</td>\n",
              "      <td>0.806756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.495700</td>\n",
              "      <td>0.492236</td>\n",
              "      <td>0.856031</td>\n",
              "      <td>0.851231</td>\n",
              "      <td>0.853224</td>\n",
              "      <td>0.861561</td>\n",
              "      <td>0.836815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>575</td>\n",
              "      <td>0.511300</td>\n",
              "      <td>0.427359</td>\n",
              "      <td>0.875486</td>\n",
              "      <td>0.877702</td>\n",
              "      <td>0.876196</td>\n",
              "      <td>0.883279</td>\n",
              "      <td>0.858439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>0.282600</td>\n",
              "      <td>0.520351</td>\n",
              "      <td>0.866732</td>\n",
              "      <td>0.861145</td>\n",
              "      <td>0.872301</td>\n",
              "      <td>0.858059</td>\n",
              "      <td>0.847997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>805</td>\n",
              "      <td>0.231900</td>\n",
              "      <td>0.500757</td>\n",
              "      <td>0.875486</td>\n",
              "      <td>0.871655</td>\n",
              "      <td>0.870412</td>\n",
              "      <td>0.876533</td>\n",
              "      <td>0.858236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>0.309600</td>\n",
              "      <td>0.480842</td>\n",
              "      <td>0.878405</td>\n",
              "      <td>0.871936</td>\n",
              "      <td>0.874470</td>\n",
              "      <td>0.874482</td>\n",
              "      <td>0.861072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1035</td>\n",
              "      <td>0.266000</td>\n",
              "      <td>0.434851</td>\n",
              "      <td>0.898833</td>\n",
              "      <td>0.892524</td>\n",
              "      <td>0.887829</td>\n",
              "      <td>0.898519</td>\n",
              "      <td>0.884313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.286000</td>\n",
              "      <td>0.424778</td>\n",
              "      <td>0.892996</td>\n",
              "      <td>0.886364</td>\n",
              "      <td>0.890188</td>\n",
              "      <td>0.884881</td>\n",
              "      <td>0.877485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1265</td>\n",
              "      <td>0.121500</td>\n",
              "      <td>0.497170</td>\n",
              "      <td>0.897860</td>\n",
              "      <td>0.890249</td>\n",
              "      <td>0.894741</td>\n",
              "      <td>0.889037</td>\n",
              "      <td>0.883124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1380</td>\n",
              "      <td>0.113000</td>\n",
              "      <td>0.500880</td>\n",
              "      <td>0.896887</td>\n",
              "      <td>0.893222</td>\n",
              "      <td>0.890387</td>\n",
              "      <td>0.896823</td>\n",
              "      <td>0.881960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1495</td>\n",
              "      <td>0.106900</td>\n",
              "      <td>0.511821</td>\n",
              "      <td>0.898833</td>\n",
              "      <td>0.895631</td>\n",
              "      <td>0.892201</td>\n",
              "      <td>0.900529</td>\n",
              "      <td>0.884256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1610</td>\n",
              "      <td>0.084800</td>\n",
              "      <td>0.533654</td>\n",
              "      <td>0.898833</td>\n",
              "      <td>0.895645</td>\n",
              "      <td>0.894204</td>\n",
              "      <td>0.897481</td>\n",
              "      <td>0.884119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1725</td>\n",
              "      <td>0.095900</td>\n",
              "      <td>0.500199</td>\n",
              "      <td>0.897860</td>\n",
              "      <td>0.894389</td>\n",
              "      <td>0.891380</td>\n",
              "      <td>0.897728</td>\n",
              "      <td>0.883040</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 11:23:26,457]\u001b[0m Trial 22 finished with values: [0.4247778058052063, 0.8863639658799652] and parameters: {'learning_rate': 0.00010385370797127183, 'num_train_epochs': 3, 'per_device_train_batch_size': 16, 'weight_decay': 0.004271677459604317}. \u001b[0m\n",
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 64)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=84.0, min_trials=700\n",
            "params: {'learning_rate': 6.2166624718692126e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 64, 'weight_decay': 0.0015762901497597605}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='432' max='432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [432/432 08:46, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>2.031200</td>\n",
              "      <td>1.827934</td>\n",
              "      <td>0.500973</td>\n",
              "      <td>0.334773</td>\n",
              "      <td>0.426345</td>\n",
              "      <td>0.385090</td>\n",
              "      <td>0.430727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>1.691600</td>\n",
              "      <td>1.447881</td>\n",
              "      <td>0.592412</td>\n",
              "      <td>0.422876</td>\n",
              "      <td>0.551617</td>\n",
              "      <td>0.465537</td>\n",
              "      <td>0.533491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>1.344500</td>\n",
              "      <td>1.122832</td>\n",
              "      <td>0.712062</td>\n",
              "      <td>0.623660</td>\n",
              "      <td>0.796724</td>\n",
              "      <td>0.617544</td>\n",
              "      <td>0.670647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>1.051600</td>\n",
              "      <td>0.896437</td>\n",
              "      <td>0.782101</td>\n",
              "      <td>0.733597</td>\n",
              "      <td>0.839707</td>\n",
              "      <td>0.716380</td>\n",
              "      <td>0.751282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.903000</td>\n",
              "      <td>0.752795</td>\n",
              "      <td>0.817121</td>\n",
              "      <td>0.793707</td>\n",
              "      <td>0.851770</td>\n",
              "      <td>0.779115</td>\n",
              "      <td>0.791088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>0.788200</td>\n",
              "      <td>0.655556</td>\n",
              "      <td>0.835603</td>\n",
              "      <td>0.821275</td>\n",
              "      <td>0.848889</td>\n",
              "      <td>0.810631</td>\n",
              "      <td>0.811562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>0.652600</td>\n",
              "      <td>0.597928</td>\n",
              "      <td>0.848249</td>\n",
              "      <td>0.840209</td>\n",
              "      <td>0.854630</td>\n",
              "      <td>0.831831</td>\n",
              "      <td>0.826054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>224</td>\n",
              "      <td>0.617100</td>\n",
              "      <td>0.557745</td>\n",
              "      <td>0.854086</td>\n",
              "      <td>0.842980</td>\n",
              "      <td>0.855996</td>\n",
              "      <td>0.836250</td>\n",
              "      <td>0.832770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>252</td>\n",
              "      <td>0.579300</td>\n",
              "      <td>0.531529</td>\n",
              "      <td>0.858949</td>\n",
              "      <td>0.852363</td>\n",
              "      <td>0.869279</td>\n",
              "      <td>0.841530</td>\n",
              "      <td>0.838329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.582200</td>\n",
              "      <td>0.508288</td>\n",
              "      <td>0.859922</td>\n",
              "      <td>0.849811</td>\n",
              "      <td>0.861276</td>\n",
              "      <td>0.842714</td>\n",
              "      <td>0.839422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>308</td>\n",
              "      <td>0.561000</td>\n",
              "      <td>0.491368</td>\n",
              "      <td>0.866732</td>\n",
              "      <td>0.860350</td>\n",
              "      <td>0.870402</td>\n",
              "      <td>0.852788</td>\n",
              "      <td>0.847200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>336</td>\n",
              "      <td>0.525800</td>\n",
              "      <td>0.483618</td>\n",
              "      <td>0.870623</td>\n",
              "      <td>0.864302</td>\n",
              "      <td>0.873802</td>\n",
              "      <td>0.857507</td>\n",
              "      <td>0.851689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>364</td>\n",
              "      <td>0.522600</td>\n",
              "      <td>0.474507</td>\n",
              "      <td>0.868677</td>\n",
              "      <td>0.861130</td>\n",
              "      <td>0.867701</td>\n",
              "      <td>0.857341</td>\n",
              "      <td>0.849507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>392</td>\n",
              "      <td>0.472400</td>\n",
              "      <td>0.470658</td>\n",
              "      <td>0.866732</td>\n",
              "      <td>0.859852</td>\n",
              "      <td>0.867935</td>\n",
              "      <td>0.854486</td>\n",
              "      <td>0.847218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.450600</td>\n",
              "      <td>0.469155</td>\n",
              "      <td>0.866732</td>\n",
              "      <td>0.859838</td>\n",
              "      <td>0.868306</td>\n",
              "      <td>0.854184</td>\n",
              "      <td>0.847219</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 11:32:25,158]\u001b[0m Trial 23 finished with values: [0.4691549837589264, 0.8598378327769765] and parameters: {'learning_rate': 6.2166624718692126e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 64, 'weight_decay': 0.0015762901497597605}. \u001b[0m\n",
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 64)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=84.0, min_trials=700\n",
            "params: {'learning_rate': 0.00015011643305430197, 'num_train_epochs': 3, 'per_device_train_batch_size': 64, 'weight_decay': 0.0013646844044177492}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='432' max='432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [432/432 08:47, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.069700</td>\n",
              "      <td>0.525594</td>\n",
              "      <td>0.849222</td>\n",
              "      <td>0.842067</td>\n",
              "      <td>0.858859</td>\n",
              "      <td>0.836753</td>\n",
              "      <td>0.828800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.513000</td>\n",
              "      <td>0.486671</td>\n",
              "      <td>0.846304</td>\n",
              "      <td>0.841423</td>\n",
              "      <td>0.845198</td>\n",
              "      <td>0.845176</td>\n",
              "      <td>0.825423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.508800</td>\n",
              "      <td>0.436635</td>\n",
              "      <td>0.865759</td>\n",
              "      <td>0.866791</td>\n",
              "      <td>0.872879</td>\n",
              "      <td>0.864694</td>\n",
              "      <td>0.846733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.450200</td>\n",
              "      <td>0.414641</td>\n",
              "      <td>0.861868</td>\n",
              "      <td>0.859485</td>\n",
              "      <td>0.858872</td>\n",
              "      <td>0.870361</td>\n",
              "      <td>0.843731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.467900</td>\n",
              "      <td>0.433122</td>\n",
              "      <td>0.870623</td>\n",
              "      <td>0.867227</td>\n",
              "      <td>0.871362</td>\n",
              "      <td>0.871153</td>\n",
              "      <td>0.852649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>0.325200</td>\n",
              "      <td>0.411908</td>\n",
              "      <td>0.883268</td>\n",
              "      <td>0.879688</td>\n",
              "      <td>0.876017</td>\n",
              "      <td>0.885959</td>\n",
              "      <td>0.866791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>0.215400</td>\n",
              "      <td>0.379826</td>\n",
              "      <td>0.889105</td>\n",
              "      <td>0.882778</td>\n",
              "      <td>0.882650</td>\n",
              "      <td>0.885114</td>\n",
              "      <td>0.873385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>224</td>\n",
              "      <td>0.249500</td>\n",
              "      <td>0.374854</td>\n",
              "      <td>0.888132</td>\n",
              "      <td>0.884496</td>\n",
              "      <td>0.881285</td>\n",
              "      <td>0.889872</td>\n",
              "      <td>0.872117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>252</td>\n",
              "      <td>0.234100</td>\n",
              "      <td>0.359042</td>\n",
              "      <td>0.892023</td>\n",
              "      <td>0.884298</td>\n",
              "      <td>0.882780</td>\n",
              "      <td>0.888072</td>\n",
              "      <td>0.876681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.269400</td>\n",
              "      <td>0.359644</td>\n",
              "      <td>0.892996</td>\n",
              "      <td>0.887221</td>\n",
              "      <td>0.889691</td>\n",
              "      <td>0.885972</td>\n",
              "      <td>0.877586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>308</td>\n",
              "      <td>0.158400</td>\n",
              "      <td>0.358115</td>\n",
              "      <td>0.897860</td>\n",
              "      <td>0.895355</td>\n",
              "      <td>0.895727</td>\n",
              "      <td>0.897001</td>\n",
              "      <td>0.883291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>336</td>\n",
              "      <td>0.103800</td>\n",
              "      <td>0.407164</td>\n",
              "      <td>0.894942</td>\n",
              "      <td>0.891661</td>\n",
              "      <td>0.892358</td>\n",
              "      <td>0.891636</td>\n",
              "      <td>0.879709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>364</td>\n",
              "      <td>0.083900</td>\n",
              "      <td>0.420679</td>\n",
              "      <td>0.898833</td>\n",
              "      <td>0.893760</td>\n",
              "      <td>0.901776</td>\n",
              "      <td>0.889096</td>\n",
              "      <td>0.884406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>392</td>\n",
              "      <td>0.083400</td>\n",
              "      <td>0.405341</td>\n",
              "      <td>0.894942</td>\n",
              "      <td>0.889417</td>\n",
              "      <td>0.888647</td>\n",
              "      <td>0.890985</td>\n",
              "      <td>0.879705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.072800</td>\n",
              "      <td>0.406094</td>\n",
              "      <td>0.899805</td>\n",
              "      <td>0.895381</td>\n",
              "      <td>0.894430</td>\n",
              "      <td>0.896764</td>\n",
              "      <td>0.885256</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 11:41:24,981]\u001b[0m Trial 24 finished with values: [0.35811516642570496, 0.8953553844888646] and parameters: {'learning_rate': 0.00015011643305430197, 'num_train_epochs': 3, 'per_device_train_batch_size': 64, 'weight_decay': 0.0013646844044177492}. \u001b[0m\n",
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 256)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=21.0, min_trials=700\n",
            "params: {'learning_rate': 6.444068140135576e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 256, 'weight_decay': 0.0074867390620878506}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='108' max='108' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [108/108 08:36, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.821100</td>\n",
              "      <td>1.252208</td>\n",
              "      <td>0.655642</td>\n",
              "      <td>0.511273</td>\n",
              "      <td>0.684884</td>\n",
              "      <td>0.533402</td>\n",
              "      <td>0.606220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.040500</td>\n",
              "      <td>0.696371</td>\n",
              "      <td>0.821012</td>\n",
              "      <td>0.794613</td>\n",
              "      <td>0.839337</td>\n",
              "      <td>0.787431</td>\n",
              "      <td>0.795321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.640800</td>\n",
              "      <td>0.526478</td>\n",
              "      <td>0.852140</td>\n",
              "      <td>0.841640</td>\n",
              "      <td>0.852763</td>\n",
              "      <td>0.835978</td>\n",
              "      <td>0.830802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.537500</td>\n",
              "      <td>0.476835</td>\n",
              "      <td>0.856031</td>\n",
              "      <td>0.853069</td>\n",
              "      <td>0.874534</td>\n",
              "      <td>0.840481</td>\n",
              "      <td>0.836519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.498900</td>\n",
              "      <td>0.413580</td>\n",
              "      <td>0.866732</td>\n",
              "      <td>0.860887</td>\n",
              "      <td>0.861507</td>\n",
              "      <td>0.863398</td>\n",
              "      <td>0.847931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.395400</td>\n",
              "      <td>0.388925</td>\n",
              "      <td>0.879377</td>\n",
              "      <td>0.870009</td>\n",
              "      <td>0.871572</td>\n",
              "      <td>0.871503</td>\n",
              "      <td>0.861997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.292500</td>\n",
              "      <td>0.373065</td>\n",
              "      <td>0.877432</td>\n",
              "      <td>0.869896</td>\n",
              "      <td>0.874264</td>\n",
              "      <td>0.869282</td>\n",
              "      <td>0.859762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.296900</td>\n",
              "      <td>0.367319</td>\n",
              "      <td>0.887160</td>\n",
              "      <td>0.879619</td>\n",
              "      <td>0.881710</td>\n",
              "      <td>0.880658</td>\n",
              "      <td>0.871085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.292600</td>\n",
              "      <td>0.377272</td>\n",
              "      <td>0.884241</td>\n",
              "      <td>0.878499</td>\n",
              "      <td>0.888928</td>\n",
              "      <td>0.872424</td>\n",
              "      <td>0.867481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.328900</td>\n",
              "      <td>0.365929</td>\n",
              "      <td>0.878405</td>\n",
              "      <td>0.869199</td>\n",
              "      <td>0.866794</td>\n",
              "      <td>0.875562</td>\n",
              "      <td>0.861225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.269700</td>\n",
              "      <td>0.359816</td>\n",
              "      <td>0.890078</td>\n",
              "      <td>0.884123</td>\n",
              "      <td>0.888510</td>\n",
              "      <td>0.881207</td>\n",
              "      <td>0.874054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.228100</td>\n",
              "      <td>0.349084</td>\n",
              "      <td>0.892023</td>\n",
              "      <td>0.886287</td>\n",
              "      <td>0.886301</td>\n",
              "      <td>0.887609</td>\n",
              "      <td>0.876522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.203400</td>\n",
              "      <td>0.353653</td>\n",
              "      <td>0.892023</td>\n",
              "      <td>0.883888</td>\n",
              "      <td>0.883474</td>\n",
              "      <td>0.886665</td>\n",
              "      <td>0.876422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.175200</td>\n",
              "      <td>0.353141</td>\n",
              "      <td>0.892023</td>\n",
              "      <td>0.884349</td>\n",
              "      <td>0.885046</td>\n",
              "      <td>0.885351</td>\n",
              "      <td>0.876352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.174600</td>\n",
              "      <td>0.351754</td>\n",
              "      <td>0.894942</td>\n",
              "      <td>0.888900</td>\n",
              "      <td>0.892724</td>\n",
              "      <td>0.886129</td>\n",
              "      <td>0.879616</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 11:50:16,095]\u001b[0m Trial 25 finished with values: [0.3490840792655945, 0.8862868603824068] and parameters: {'learning_rate': 6.444068140135576e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 256, 'weight_decay': 0.0074867390620878506}. \u001b[0m\n",
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 32)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=171.0, min_trials=700\n",
            "params: {'learning_rate': 7.867028121974179e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 32, 'weight_decay': 0.005149244601973092}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='867' max='867' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [867/867 09:00, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>1.834300</td>\n",
              "      <td>1.397810</td>\n",
              "      <td>0.617704</td>\n",
              "      <td>0.449955</td>\n",
              "      <td>0.571764</td>\n",
              "      <td>0.486179</td>\n",
              "      <td>0.564623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>1.157800</td>\n",
              "      <td>0.828618</td>\n",
              "      <td>0.812257</td>\n",
              "      <td>0.782284</td>\n",
              "      <td>0.835080</td>\n",
              "      <td>0.777098</td>\n",
              "      <td>0.785433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>0.756100</td>\n",
              "      <td>0.597633</td>\n",
              "      <td>0.841440</td>\n",
              "      <td>0.834304</td>\n",
              "      <td>0.849573</td>\n",
              "      <td>0.824555</td>\n",
              "      <td>0.818286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>228</td>\n",
              "      <td>0.599100</td>\n",
              "      <td>0.527467</td>\n",
              "      <td>0.848249</td>\n",
              "      <td>0.842275</td>\n",
              "      <td>0.844541</td>\n",
              "      <td>0.848151</td>\n",
              "      <td>0.827574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>285</td>\n",
              "      <td>0.551300</td>\n",
              "      <td>0.468684</td>\n",
              "      <td>0.865759</td>\n",
              "      <td>0.858712</td>\n",
              "      <td>0.866456</td>\n",
              "      <td>0.854867</td>\n",
              "      <td>0.846207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>342</td>\n",
              "      <td>0.484300</td>\n",
              "      <td>0.429419</td>\n",
              "      <td>0.870623</td>\n",
              "      <td>0.865919</td>\n",
              "      <td>0.867161</td>\n",
              "      <td>0.865603</td>\n",
              "      <td>0.851853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>399</td>\n",
              "      <td>0.397700</td>\n",
              "      <td>0.413269</td>\n",
              "      <td>0.876459</td>\n",
              "      <td>0.870159</td>\n",
              "      <td>0.874364</td>\n",
              "      <td>0.868748</td>\n",
              "      <td>0.858489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>456</td>\n",
              "      <td>0.405200</td>\n",
              "      <td>0.411213</td>\n",
              "      <td>0.877432</td>\n",
              "      <td>0.870695</td>\n",
              "      <td>0.869117</td>\n",
              "      <td>0.875537</td>\n",
              "      <td>0.859990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>513</td>\n",
              "      <td>0.396100</td>\n",
              "      <td>0.389139</td>\n",
              "      <td>0.880350</td>\n",
              "      <td>0.874339</td>\n",
              "      <td>0.873234</td>\n",
              "      <td>0.877480</td>\n",
              "      <td>0.863109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.413800</td>\n",
              "      <td>0.392533</td>\n",
              "      <td>0.882296</td>\n",
              "      <td>0.876599</td>\n",
              "      <td>0.882701</td>\n",
              "      <td>0.872857</td>\n",
              "      <td>0.865154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>627</td>\n",
              "      <td>0.358000</td>\n",
              "      <td>0.379808</td>\n",
              "      <td>0.885214</td>\n",
              "      <td>0.879126</td>\n",
              "      <td>0.879709</td>\n",
              "      <td>0.879582</td>\n",
              "      <td>0.868553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>684</td>\n",
              "      <td>0.360600</td>\n",
              "      <td>0.376396</td>\n",
              "      <td>0.880350</td>\n",
              "      <td>0.877014</td>\n",
              "      <td>0.880141</td>\n",
              "      <td>0.874949</td>\n",
              "      <td>0.862899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>741</td>\n",
              "      <td>0.337500</td>\n",
              "      <td>0.379028</td>\n",
              "      <td>0.882296</td>\n",
              "      <td>0.877003</td>\n",
              "      <td>0.880882</td>\n",
              "      <td>0.874907</td>\n",
              "      <td>0.865126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>798</td>\n",
              "      <td>0.290800</td>\n",
              "      <td>0.374083</td>\n",
              "      <td>0.883268</td>\n",
              "      <td>0.878682</td>\n",
              "      <td>0.882376</td>\n",
              "      <td>0.876149</td>\n",
              "      <td>0.866216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>855</td>\n",
              "      <td>0.293200</td>\n",
              "      <td>0.372723</td>\n",
              "      <td>0.881323</td>\n",
              "      <td>0.877066</td>\n",
              "      <td>0.879226</td>\n",
              "      <td>0.875786</td>\n",
              "      <td>0.863994</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 11:59:27,781]\u001b[0m Trial 26 finished with values: [0.3727229833602905, 0.8770659104005859] and parameters: {'learning_rate': 7.867028121974179e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 32, 'weight_decay': 0.005149244601973092}. \u001b[0m\n",
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 256)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=21.0, min_trials=700\n",
            "params: {'learning_rate': 0.00014176483733247038, 'num_train_epochs': 3, 'per_device_train_batch_size': 256, 'weight_decay': 0.0022986715140560997}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='108' max='108' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [108/108 08:35, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.701700</td>\n",
              "      <td>0.937521</td>\n",
              "      <td>0.760700</td>\n",
              "      <td>0.719947</td>\n",
              "      <td>0.809048</td>\n",
              "      <td>0.710992</td>\n",
              "      <td>0.729417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.779300</td>\n",
              "      <td>0.490461</td>\n",
              "      <td>0.852140</td>\n",
              "      <td>0.843478</td>\n",
              "      <td>0.842300</td>\n",
              "      <td>0.848619</td>\n",
              "      <td>0.831440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.516000</td>\n",
              "      <td>0.447428</td>\n",
              "      <td>0.863813</td>\n",
              "      <td>0.856459</td>\n",
              "      <td>0.854030</td>\n",
              "      <td>0.865550</td>\n",
              "      <td>0.844893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.500700</td>\n",
              "      <td>0.431742</td>\n",
              "      <td>0.868677</td>\n",
              "      <td>0.867876</td>\n",
              "      <td>0.884582</td>\n",
              "      <td>0.856491</td>\n",
              "      <td>0.850061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.494200</td>\n",
              "      <td>0.394611</td>\n",
              "      <td>0.867704</td>\n",
              "      <td>0.857681</td>\n",
              "      <td>0.862205</td>\n",
              "      <td>0.858230</td>\n",
              "      <td>0.848844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.309500</td>\n",
              "      <td>0.348409</td>\n",
              "      <td>0.884241</td>\n",
              "      <td>0.875002</td>\n",
              "      <td>0.873104</td>\n",
              "      <td>0.878071</td>\n",
              "      <td>0.867609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.242200</td>\n",
              "      <td>0.363521</td>\n",
              "      <td>0.890078</td>\n",
              "      <td>0.883573</td>\n",
              "      <td>0.890389</td>\n",
              "      <td>0.878665</td>\n",
              "      <td>0.873996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.267000</td>\n",
              "      <td>0.354465</td>\n",
              "      <td>0.895914</td>\n",
              "      <td>0.889303</td>\n",
              "      <td>0.886298</td>\n",
              "      <td>0.893076</td>\n",
              "      <td>0.880944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.252000</td>\n",
              "      <td>0.372987</td>\n",
              "      <td>0.883268</td>\n",
              "      <td>0.878491</td>\n",
              "      <td>0.888592</td>\n",
              "      <td>0.871949</td>\n",
              "      <td>0.866588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.276300</td>\n",
              "      <td>0.337083</td>\n",
              "      <td>0.900778</td>\n",
              "      <td>0.895219</td>\n",
              "      <td>0.893715</td>\n",
              "      <td>0.898985</td>\n",
              "      <td>0.886684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.201300</td>\n",
              "      <td>0.339293</td>\n",
              "      <td>0.896887</td>\n",
              "      <td>0.891883</td>\n",
              "      <td>0.894098</td>\n",
              "      <td>0.891286</td>\n",
              "      <td>0.882068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.146700</td>\n",
              "      <td>0.338549</td>\n",
              "      <td>0.900778</td>\n",
              "      <td>0.895775</td>\n",
              "      <td>0.896780</td>\n",
              "      <td>0.896050</td>\n",
              "      <td>0.886417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.138400</td>\n",
              "      <td>0.345432</td>\n",
              "      <td>0.896887</td>\n",
              "      <td>0.891484</td>\n",
              "      <td>0.893606</td>\n",
              "      <td>0.890493</td>\n",
              "      <td>0.881846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.112200</td>\n",
              "      <td>0.343250</td>\n",
              "      <td>0.900778</td>\n",
              "      <td>0.894765</td>\n",
              "      <td>0.896401</td>\n",
              "      <td>0.893563</td>\n",
              "      <td>0.886284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.112700</td>\n",
              "      <td>0.349476</td>\n",
              "      <td>0.902724</td>\n",
              "      <td>0.897714</td>\n",
              "      <td>0.904377</td>\n",
              "      <td>0.892225</td>\n",
              "      <td>0.888513</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 12:08:17,730]\u001b[0m Trial 27 finished with values: [0.33708322048187256, 0.8952187875367795] and parameters: {'learning_rate': 0.00014176483733247038, 'num_train_epochs': 3, 'per_device_train_batch_size': 256, 'weight_decay': 0.0022986715140560997}. \u001b[0m\n",
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 256)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=21.0, min_trials=700\n",
            "params: {'learning_rate': 6.117647523466384e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 256, 'weight_decay': 0.0038493715778471}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='108' max='108' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [108/108 08:32, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.149800</td>\n",
              "      <td>2.067539</td>\n",
              "      <td>0.308366</td>\n",
              "      <td>0.186884</td>\n",
              "      <td>0.201609</td>\n",
              "      <td>0.237166</td>\n",
              "      <td>0.213683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.032900</td>\n",
              "      <td>1.952708</td>\n",
              "      <td>0.443580</td>\n",
              "      <td>0.294855</td>\n",
              "      <td>0.317538</td>\n",
              "      <td>0.336450</td>\n",
              "      <td>0.360531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.944900</td>\n",
              "      <td>1.845073</td>\n",
              "      <td>0.501946</td>\n",
              "      <td>0.335458</td>\n",
              "      <td>0.375158</td>\n",
              "      <td>0.381180</td>\n",
              "      <td>0.427571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.819800</td>\n",
              "      <td>1.736612</td>\n",
              "      <td>0.548638</td>\n",
              "      <td>0.369788</td>\n",
              "      <td>0.370511</td>\n",
              "      <td>0.421984</td>\n",
              "      <td>0.481352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.736500</td>\n",
              "      <td>1.631032</td>\n",
              "      <td>0.580739</td>\n",
              "      <td>0.405634</td>\n",
              "      <td>0.448358</td>\n",
              "      <td>0.449220</td>\n",
              "      <td>0.519779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.678600</td>\n",
              "      <td>1.534877</td>\n",
              "      <td>0.607004</td>\n",
              "      <td>0.439130</td>\n",
              "      <td>0.451200</td>\n",
              "      <td>0.477212</td>\n",
              "      <td>0.549598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>1.532000</td>\n",
              "      <td>1.449728</td>\n",
              "      <td>0.617704</td>\n",
              "      <td>0.455655</td>\n",
              "      <td>0.562421</td>\n",
              "      <td>0.490685</td>\n",
              "      <td>0.562416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>1.466600</td>\n",
              "      <td>1.373665</td>\n",
              "      <td>0.631323</td>\n",
              "      <td>0.472925</td>\n",
              "      <td>0.572223</td>\n",
              "      <td>0.503630</td>\n",
              "      <td>0.577976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>1.376700</td>\n",
              "      <td>1.309977</td>\n",
              "      <td>0.651751</td>\n",
              "      <td>0.500484</td>\n",
              "      <td>0.585010</td>\n",
              "      <td>0.524468</td>\n",
              "      <td>0.602141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.362100</td>\n",
              "      <td>1.252975</td>\n",
              "      <td>0.661479</td>\n",
              "      <td>0.516158</td>\n",
              "      <td>0.692700</td>\n",
              "      <td>0.537071</td>\n",
              "      <td>0.612733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>1.308200</td>\n",
              "      <td>1.207954</td>\n",
              "      <td>0.674125</td>\n",
              "      <td>0.543670</td>\n",
              "      <td>0.697049</td>\n",
              "      <td>0.556093</td>\n",
              "      <td>0.627150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>1.253800</td>\n",
              "      <td>1.172126</td>\n",
              "      <td>0.691634</td>\n",
              "      <td>0.571849</td>\n",
              "      <td>0.817471</td>\n",
              "      <td>0.576932</td>\n",
              "      <td>0.647429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>1.245600</td>\n",
              "      <td>1.146626</td>\n",
              "      <td>0.704280</td>\n",
              "      <td>0.595684</td>\n",
              "      <td>0.810590</td>\n",
              "      <td>0.595991</td>\n",
              "      <td>0.661875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>1.203200</td>\n",
              "      <td>1.129304</td>\n",
              "      <td>0.713035</td>\n",
              "      <td>0.610749</td>\n",
              "      <td>0.814527</td>\n",
              "      <td>0.608033</td>\n",
              "      <td>0.671926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>1.155200</td>\n",
              "      <td>1.120851</td>\n",
              "      <td>0.716926</td>\n",
              "      <td>0.619600</td>\n",
              "      <td>0.817706</td>\n",
              "      <td>0.614759</td>\n",
              "      <td>0.676462</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 12:17:04,429]\u001b[0m Trial 28 finished with values: [1.1208508014678955, 0.6196000125987274] and parameters: {'learning_rate': 6.117647523466384e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 256, 'weight_decay': 0.0038493715778471}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/optuna/trial/_trial.py:744: UserWarning: Fixed parameter 'learning_rate' with value 0.0005019671120957824 is out of range for distribution LogUniformDistribution(high=0.0005, low=5e-06).\n",
            "  \"for distribution {}.\".format(name, param_value, distribution)\n",
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 32)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=171.0, min_trials=700\n",
            "params: {'learning_rate': 0.00022317303056912185, 'num_train_epochs': 3, 'per_device_train_batch_size': 32, 'weight_decay': 0.0018214061884032904}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='867' max='867' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [867/867 08:59, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.935900</td>\n",
              "      <td>0.596372</td>\n",
              "      <td>0.809339</td>\n",
              "      <td>0.804676</td>\n",
              "      <td>0.823553</td>\n",
              "      <td>0.795881</td>\n",
              "      <td>0.781915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>0.684500</td>\n",
              "      <td>0.578073</td>\n",
              "      <td>0.822957</td>\n",
              "      <td>0.825969</td>\n",
              "      <td>0.837731</td>\n",
              "      <td>0.822056</td>\n",
              "      <td>0.799052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>0.640400</td>\n",
              "      <td>0.536541</td>\n",
              "      <td>0.837549</td>\n",
              "      <td>0.825745</td>\n",
              "      <td>0.815385</td>\n",
              "      <td>0.846772</td>\n",
              "      <td>0.815665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>228</td>\n",
              "      <td>0.570500</td>\n",
              "      <td>0.527781</td>\n",
              "      <td>0.835603</td>\n",
              "      <td>0.831298</td>\n",
              "      <td>0.842077</td>\n",
              "      <td>0.837530</td>\n",
              "      <td>0.813943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>285</td>\n",
              "      <td>0.580800</td>\n",
              "      <td>0.571544</td>\n",
              "      <td>0.825875</td>\n",
              "      <td>0.822172</td>\n",
              "      <td>0.844176</td>\n",
              "      <td>0.817540</td>\n",
              "      <td>0.802476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>342</td>\n",
              "      <td>0.386900</td>\n",
              "      <td>0.510365</td>\n",
              "      <td>0.857004</td>\n",
              "      <td>0.847202</td>\n",
              "      <td>0.846914</td>\n",
              "      <td>0.851917</td>\n",
              "      <td>0.836735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>399</td>\n",
              "      <td>0.294500</td>\n",
              "      <td>0.550787</td>\n",
              "      <td>0.852140</td>\n",
              "      <td>0.842728</td>\n",
              "      <td>0.844907</td>\n",
              "      <td>0.853106</td>\n",
              "      <td>0.831901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>456</td>\n",
              "      <td>0.354300</td>\n",
              "      <td>0.430998</td>\n",
              "      <td>0.875486</td>\n",
              "      <td>0.868410</td>\n",
              "      <td>0.866294</td>\n",
              "      <td>0.875689</td>\n",
              "      <td>0.857803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>513</td>\n",
              "      <td>0.315800</td>\n",
              "      <td>0.566049</td>\n",
              "      <td>0.862840</td>\n",
              "      <td>0.854198</td>\n",
              "      <td>0.866797</td>\n",
              "      <td>0.854604</td>\n",
              "      <td>0.843663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.353500</td>\n",
              "      <td>0.437891</td>\n",
              "      <td>0.890078</td>\n",
              "      <td>0.888138</td>\n",
              "      <td>0.888901</td>\n",
              "      <td>0.888574</td>\n",
              "      <td>0.874197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>627</td>\n",
              "      <td>0.181700</td>\n",
              "      <td>0.447163</td>\n",
              "      <td>0.888132</td>\n",
              "      <td>0.886145</td>\n",
              "      <td>0.886582</td>\n",
              "      <td>0.886399</td>\n",
              "      <td>0.871938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>684</td>\n",
              "      <td>0.156100</td>\n",
              "      <td>0.443987</td>\n",
              "      <td>0.893969</td>\n",
              "      <td>0.891604</td>\n",
              "      <td>0.889164</td>\n",
              "      <td>0.894545</td>\n",
              "      <td>0.878676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>741</td>\n",
              "      <td>0.129800</td>\n",
              "      <td>0.519010</td>\n",
              "      <td>0.891051</td>\n",
              "      <td>0.889663</td>\n",
              "      <td>0.887797</td>\n",
              "      <td>0.892888</td>\n",
              "      <td>0.875367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>798</td>\n",
              "      <td>0.105200</td>\n",
              "      <td>0.498812</td>\n",
              "      <td>0.891051</td>\n",
              "      <td>0.885392</td>\n",
              "      <td>0.888826</td>\n",
              "      <td>0.883721</td>\n",
              "      <td>0.875212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>855</td>\n",
              "      <td>0.092700</td>\n",
              "      <td>0.476346</td>\n",
              "      <td>0.899805</td>\n",
              "      <td>0.899463</td>\n",
              "      <td>0.903186</td>\n",
              "      <td>0.896319</td>\n",
              "      <td>0.885152</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 12:26:15,234]\u001b[0m Trial 29 finished with values: [0.43099766969680786, 0.8684098410427146] and parameters: {'learning_rate': 0.00022317303056912185, 'num_train_epochs': 3, 'per_device_train_batch_size': 32, 'weight_decay': 0.0018214061884032904}. \u001b[0m\n",
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 128)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=42.0, min_trials=700\n",
            "params: {'learning_rate': 6.416450556475228e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 128, 'weight_decay': 0.0014725254027968936}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='216' max='216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [216/216 08:36, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.101300</td>\n",
              "      <td>1.975201</td>\n",
              "      <td>0.431907</td>\n",
              "      <td>0.287238</td>\n",
              "      <td>0.294292</td>\n",
              "      <td>0.330657</td>\n",
              "      <td>0.351324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.908000</td>\n",
              "      <td>1.759971</td>\n",
              "      <td>0.541829</td>\n",
              "      <td>0.361387</td>\n",
              "      <td>0.437513</td>\n",
              "      <td>0.416746</td>\n",
              "      <td>0.474937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.713600</td>\n",
              "      <td>1.541740</td>\n",
              "      <td>0.581712</td>\n",
              "      <td>0.411392</td>\n",
              "      <td>0.551734</td>\n",
              "      <td>0.453541</td>\n",
              "      <td>0.520753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>1.490000</td>\n",
              "      <td>1.346813</td>\n",
              "      <td>0.633268</td>\n",
              "      <td>0.469847</td>\n",
              "      <td>0.573097</td>\n",
              "      <td>0.502092</td>\n",
              "      <td>0.580546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.331100</td>\n",
              "      <td>1.165804</td>\n",
              "      <td>0.694553</td>\n",
              "      <td>0.568865</td>\n",
              "      <td>0.708572</td>\n",
              "      <td>0.577523</td>\n",
              "      <td>0.651397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>1.194600</td>\n",
              "      <td>1.026720</td>\n",
              "      <td>0.755837</td>\n",
              "      <td>0.693080</td>\n",
              "      <td>0.815251</td>\n",
              "      <td>0.683971</td>\n",
              "      <td>0.720525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>1.014700</td>\n",
              "      <td>0.915913</td>\n",
              "      <td>0.788911</td>\n",
              "      <td>0.748272</td>\n",
              "      <td>0.836523</td>\n",
              "      <td>0.728784</td>\n",
              "      <td>0.758439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.935000</td>\n",
              "      <td>0.834182</td>\n",
              "      <td>0.812257</td>\n",
              "      <td>0.783934</td>\n",
              "      <td>0.851147</td>\n",
              "      <td>0.765852</td>\n",
              "      <td>0.784946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>0.856500</td>\n",
              "      <td>0.784169</td>\n",
              "      <td>0.817121</td>\n",
              "      <td>0.796886</td>\n",
              "      <td>0.849553</td>\n",
              "      <td>0.779479</td>\n",
              "      <td>0.790873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.839000</td>\n",
              "      <td>0.728142</td>\n",
              "      <td>0.834630</td>\n",
              "      <td>0.817323</td>\n",
              "      <td>0.850256</td>\n",
              "      <td>0.804625</td>\n",
              "      <td>0.810477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>0.794700</td>\n",
              "      <td>0.703150</td>\n",
              "      <td>0.836576</td>\n",
              "      <td>0.821020</td>\n",
              "      <td>0.854158</td>\n",
              "      <td>0.807102</td>\n",
              "      <td>0.812726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>0.750300</td>\n",
              "      <td>0.674249</td>\n",
              "      <td>0.837549</td>\n",
              "      <td>0.823646</td>\n",
              "      <td>0.851530</td>\n",
              "      <td>0.810818</td>\n",
              "      <td>0.813731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>0.747200</td>\n",
              "      <td>0.660074</td>\n",
              "      <td>0.838521</td>\n",
              "      <td>0.824417</td>\n",
              "      <td>0.853363</td>\n",
              "      <td>0.812708</td>\n",
              "      <td>0.814911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>0.694000</td>\n",
              "      <td>0.649826</td>\n",
              "      <td>0.839494</td>\n",
              "      <td>0.826207</td>\n",
              "      <td>0.855068</td>\n",
              "      <td>0.813685</td>\n",
              "      <td>0.816016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.654700</td>\n",
              "      <td>0.644454</td>\n",
              "      <td>0.845331</td>\n",
              "      <td>0.833125</td>\n",
              "      <td>0.860552</td>\n",
              "      <td>0.820028</td>\n",
              "      <td>0.822704</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 12:35:04,515]\u001b[0m Trial 30 finished with values: [0.6444544792175293, 0.8331249713307459] and parameters: {'learning_rate': 6.416450556475228e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 128, 'weight_decay': 0.0014725254027968936}. \u001b[0m\n",
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 16)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=345.0, min_trials=700\n",
            "params: {'learning_rate': 5.8324041160853215e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 16, 'weight_decay': 0.006827576041735309}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1734' max='1734' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1734/1734 09:24, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.843100</td>\n",
              "      <td>0.532750</td>\n",
              "      <td>0.836576</td>\n",
              "      <td>0.836765</td>\n",
              "      <td>0.836609</td>\n",
              "      <td>0.850768</td>\n",
              "      <td>0.816098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.529900</td>\n",
              "      <td>0.428289</td>\n",
              "      <td>0.865759</td>\n",
              "      <td>0.860436</td>\n",
              "      <td>0.859458</td>\n",
              "      <td>0.864350</td>\n",
              "      <td>0.846503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>345</td>\n",
              "      <td>0.499400</td>\n",
              "      <td>0.527094</td>\n",
              "      <td>0.838521</td>\n",
              "      <td>0.825908</td>\n",
              "      <td>0.825672</td>\n",
              "      <td>0.846990</td>\n",
              "      <td>0.817771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.447300</td>\n",
              "      <td>0.456128</td>\n",
              "      <td>0.866732</td>\n",
              "      <td>0.857312</td>\n",
              "      <td>0.860973</td>\n",
              "      <td>0.865808</td>\n",
              "      <td>0.848573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>575</td>\n",
              "      <td>0.451900</td>\n",
              "      <td>0.389225</td>\n",
              "      <td>0.883268</td>\n",
              "      <td>0.881137</td>\n",
              "      <td>0.886679</td>\n",
              "      <td>0.878379</td>\n",
              "      <td>0.866675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>0.258700</td>\n",
              "      <td>0.406758</td>\n",
              "      <td>0.892996</td>\n",
              "      <td>0.885176</td>\n",
              "      <td>0.892614</td>\n",
              "      <td>0.881055</td>\n",
              "      <td>0.877986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>805</td>\n",
              "      <td>0.209600</td>\n",
              "      <td>0.442535</td>\n",
              "      <td>0.890078</td>\n",
              "      <td>0.885282</td>\n",
              "      <td>0.884096</td>\n",
              "      <td>0.891034</td>\n",
              "      <td>0.875059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>0.268800</td>\n",
              "      <td>0.445921</td>\n",
              "      <td>0.889105</td>\n",
              "      <td>0.887877</td>\n",
              "      <td>0.894502</td>\n",
              "      <td>0.884357</td>\n",
              "      <td>0.873223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1035</td>\n",
              "      <td>0.250300</td>\n",
              "      <td>0.409281</td>\n",
              "      <td>0.892023</td>\n",
              "      <td>0.887750</td>\n",
              "      <td>0.883103</td>\n",
              "      <td>0.894199</td>\n",
              "      <td>0.876551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.250100</td>\n",
              "      <td>0.381521</td>\n",
              "      <td>0.893969</td>\n",
              "      <td>0.892803</td>\n",
              "      <td>0.895660</td>\n",
              "      <td>0.890495</td>\n",
              "      <td>0.878492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1265</td>\n",
              "      <td>0.111000</td>\n",
              "      <td>0.475778</td>\n",
              "      <td>0.896887</td>\n",
              "      <td>0.894370</td>\n",
              "      <td>0.902634</td>\n",
              "      <td>0.887951</td>\n",
              "      <td>0.881872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1380</td>\n",
              "      <td>0.098700</td>\n",
              "      <td>0.451256</td>\n",
              "      <td>0.896887</td>\n",
              "      <td>0.891815</td>\n",
              "      <td>0.891256</td>\n",
              "      <td>0.893358</td>\n",
              "      <td>0.881987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1495</td>\n",
              "      <td>0.105100</td>\n",
              "      <td>0.475503</td>\n",
              "      <td>0.899805</td>\n",
              "      <td>0.897396</td>\n",
              "      <td>0.898360</td>\n",
              "      <td>0.897013</td>\n",
              "      <td>0.885206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1610</td>\n",
              "      <td>0.085100</td>\n",
              "      <td>0.466973</td>\n",
              "      <td>0.898833</td>\n",
              "      <td>0.896160</td>\n",
              "      <td>0.897409</td>\n",
              "      <td>0.895463</td>\n",
              "      <td>0.884099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1725</td>\n",
              "      <td>0.070000</td>\n",
              "      <td>0.463247</td>\n",
              "      <td>0.900778</td>\n",
              "      <td>0.898200</td>\n",
              "      <td>0.898574</td>\n",
              "      <td>0.898379</td>\n",
              "      <td>0.886336</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 12:44:40,344]\u001b[0m Trial 31 finished with values: [0.3815212547779083, 0.8928026814276008] and parameters: {'learning_rate': 5.8324041160853215e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 16, 'weight_decay': 0.006827576041735309}. \u001b[0m\n",
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 16)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=345.0, min_trials=700\n",
            "params: {'learning_rate': 0.0002539606068280479, 'num_train_epochs': 3, 'per_device_train_batch_size': 16, 'weight_decay': 0.008215865853457857}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1734' max='1734' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1734/1734 09:22, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>1.161200</td>\n",
              "      <td>1.033520</td>\n",
              "      <td>0.698444</td>\n",
              "      <td>0.666042</td>\n",
              "      <td>0.742248</td>\n",
              "      <td>0.685758</td>\n",
              "      <td>0.664033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>1.010000</td>\n",
              "      <td>0.829171</td>\n",
              "      <td>0.785992</td>\n",
              "      <td>0.767893</td>\n",
              "      <td>0.791669</td>\n",
              "      <td>0.753971</td>\n",
              "      <td>0.754555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>345</td>\n",
              "      <td>1.109100</td>\n",
              "      <td>1.483737</td>\n",
              "      <td>0.586576</td>\n",
              "      <td>0.604665</td>\n",
              "      <td>0.698901</td>\n",
              "      <td>0.614592</td>\n",
              "      <td>0.544750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>1.060400</td>\n",
              "      <td>1.216273</td>\n",
              "      <td>0.647860</td>\n",
              "      <td>0.642867</td>\n",
              "      <td>0.738755</td>\n",
              "      <td>0.668977</td>\n",
              "      <td>0.616861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>575</td>\n",
              "      <td>1.037700</td>\n",
              "      <td>1.195467</td>\n",
              "      <td>0.654669</td>\n",
              "      <td>0.665643</td>\n",
              "      <td>0.687130</td>\n",
              "      <td>0.695546</td>\n",
              "      <td>0.639437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>0.801200</td>\n",
              "      <td>1.152508</td>\n",
              "      <td>0.663424</td>\n",
              "      <td>0.602792</td>\n",
              "      <td>0.613240</td>\n",
              "      <td>0.645257</td>\n",
              "      <td>0.628946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>805</td>\n",
              "      <td>0.716300</td>\n",
              "      <td>1.001908</td>\n",
              "      <td>0.722763</td>\n",
              "      <td>0.715651</td>\n",
              "      <td>0.725675</td>\n",
              "      <td>0.747907</td>\n",
              "      <td>0.691943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>0.783800</td>\n",
              "      <td>1.133106</td>\n",
              "      <td>0.700389</td>\n",
              "      <td>0.663119</td>\n",
              "      <td>0.666141</td>\n",
              "      <td>0.691200</td>\n",
              "      <td>0.667194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1035</td>\n",
              "      <td>0.689800</td>\n",
              "      <td>1.006042</td>\n",
              "      <td>0.716926</td>\n",
              "      <td>0.715276</td>\n",
              "      <td>0.748028</td>\n",
              "      <td>0.728877</td>\n",
              "      <td>0.686748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.756100</td>\n",
              "      <td>0.937207</td>\n",
              "      <td>0.756809</td>\n",
              "      <td>0.762420</td>\n",
              "      <td>0.776849</td>\n",
              "      <td>0.760182</td>\n",
              "      <td>0.723093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1265</td>\n",
              "      <td>0.564000</td>\n",
              "      <td>0.981871</td>\n",
              "      <td>0.772374</td>\n",
              "      <td>0.775148</td>\n",
              "      <td>0.789303</td>\n",
              "      <td>0.781555</td>\n",
              "      <td>0.743310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1380</td>\n",
              "      <td>0.507300</td>\n",
              "      <td>1.052814</td>\n",
              "      <td>0.757782</td>\n",
              "      <td>0.756189</td>\n",
              "      <td>0.769913</td>\n",
              "      <td>0.779515</td>\n",
              "      <td>0.731543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1495</td>\n",
              "      <td>0.521500</td>\n",
              "      <td>0.935434</td>\n",
              "      <td>0.714981</td>\n",
              "      <td>0.700912</td>\n",
              "      <td>0.725990</td>\n",
              "      <td>0.737375</td>\n",
              "      <td>0.690361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1610</td>\n",
              "      <td>0.401700</td>\n",
              "      <td>0.693917</td>\n",
              "      <td>0.855058</td>\n",
              "      <td>0.847427</td>\n",
              "      <td>0.850637</td>\n",
              "      <td>0.844885</td>\n",
              "      <td>0.833852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1725</td>\n",
              "      <td>0.415900</td>\n",
              "      <td>0.694846</td>\n",
              "      <td>0.849222</td>\n",
              "      <td>0.842492</td>\n",
              "      <td>0.843773</td>\n",
              "      <td>0.842279</td>\n",
              "      <td>0.827390</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 12:54:14,063]\u001b[0m Trial 32 finished with values: [0.6939172744750977, 0.8474266079747198] and parameters: {'learning_rate': 0.0002539606068280479, 'num_train_epochs': 3, 'per_device_train_batch_size': 16, 'weight_decay': 0.008215865853457857}. \u001b[0m\n",
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 256)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=21.0, min_trials=700\n",
            "params: {'learning_rate': 0.000246009921598085, 'num_train_epochs': 3, 'per_device_train_batch_size': 256, 'weight_decay': 0.004778813284793992}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='108' max='108' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [108/108 08:34, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.753900</td>\n",
              "      <td>0.897408</td>\n",
              "      <td>0.779183</td>\n",
              "      <td>0.757483</td>\n",
              "      <td>0.806688</td>\n",
              "      <td>0.748621</td>\n",
              "      <td>0.750510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.745300</td>\n",
              "      <td>0.519193</td>\n",
              "      <td>0.837549</td>\n",
              "      <td>0.829429</td>\n",
              "      <td>0.831084</td>\n",
              "      <td>0.835973</td>\n",
              "      <td>0.814710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.571600</td>\n",
              "      <td>0.486066</td>\n",
              "      <td>0.854086</td>\n",
              "      <td>0.846543</td>\n",
              "      <td>0.848755</td>\n",
              "      <td>0.854843</td>\n",
              "      <td>0.834658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.538300</td>\n",
              "      <td>0.423711</td>\n",
              "      <td>0.860895</td>\n",
              "      <td>0.856066</td>\n",
              "      <td>0.859162</td>\n",
              "      <td>0.856724</td>\n",
              "      <td>0.841031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.547000</td>\n",
              "      <td>0.417605</td>\n",
              "      <td>0.866732</td>\n",
              "      <td>0.861636</td>\n",
              "      <td>0.869490</td>\n",
              "      <td>0.861906</td>\n",
              "      <td>0.848131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.339000</td>\n",
              "      <td>0.358419</td>\n",
              "      <td>0.875486</td>\n",
              "      <td>0.870781</td>\n",
              "      <td>0.874704</td>\n",
              "      <td>0.870559</td>\n",
              "      <td>0.857638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.224300</td>\n",
              "      <td>0.383850</td>\n",
              "      <td>0.888132</td>\n",
              "      <td>0.885755</td>\n",
              "      <td>0.896175</td>\n",
              "      <td>0.877752</td>\n",
              "      <td>0.871890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.285500</td>\n",
              "      <td>0.375343</td>\n",
              "      <td>0.882296</td>\n",
              "      <td>0.877531</td>\n",
              "      <td>0.880071</td>\n",
              "      <td>0.876216</td>\n",
              "      <td>0.865257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.250300</td>\n",
              "      <td>0.341824</td>\n",
              "      <td>0.891051</td>\n",
              "      <td>0.887760</td>\n",
              "      <td>0.894365</td>\n",
              "      <td>0.883175</td>\n",
              "      <td>0.875209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.298200</td>\n",
              "      <td>0.347304</td>\n",
              "      <td>0.890078</td>\n",
              "      <td>0.886133</td>\n",
              "      <td>0.885498</td>\n",
              "      <td>0.888612</td>\n",
              "      <td>0.874282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.198800</td>\n",
              "      <td>0.366192</td>\n",
              "      <td>0.890078</td>\n",
              "      <td>0.886190</td>\n",
              "      <td>0.890424</td>\n",
              "      <td>0.884175</td>\n",
              "      <td>0.874268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.130600</td>\n",
              "      <td>0.354807</td>\n",
              "      <td>0.895914</td>\n",
              "      <td>0.892265</td>\n",
              "      <td>0.891750</td>\n",
              "      <td>0.893413</td>\n",
              "      <td>0.880794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.121000</td>\n",
              "      <td>0.357884</td>\n",
              "      <td>0.896887</td>\n",
              "      <td>0.893716</td>\n",
              "      <td>0.895156</td>\n",
              "      <td>0.893197</td>\n",
              "      <td>0.881885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.105600</td>\n",
              "      <td>0.377142</td>\n",
              "      <td>0.892023</td>\n",
              "      <td>0.888015</td>\n",
              "      <td>0.892512</td>\n",
              "      <td>0.885252</td>\n",
              "      <td>0.876350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.092900</td>\n",
              "      <td>0.382939</td>\n",
              "      <td>0.890078</td>\n",
              "      <td>0.884633</td>\n",
              "      <td>0.889111</td>\n",
              "      <td>0.881595</td>\n",
              "      <td>0.874075</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 13:03:03,206]\u001b[0m Trial 33 finished with values: [0.34182417392730713, 0.8877602355183684] and parameters: {'learning_rate': 0.000246009921598085, 'num_train_epochs': 3, 'per_device_train_batch_size': 256, 'weight_decay': 0.004778813284793992}. \u001b[0m\n",
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 8)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=693.0, min_trials=700\n",
            "params: {'learning_rate': 2.1308732758030146e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 8, 'weight_decay': 0.0015799166052704611}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3468' max='3468' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3468/3468 10:13, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>231</td>\n",
              "      <td>0.957000</td>\n",
              "      <td>0.548460</td>\n",
              "      <td>0.837549</td>\n",
              "      <td>0.836494</td>\n",
              "      <td>0.836745</td>\n",
              "      <td>0.846769</td>\n",
              "      <td>0.816579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>462</td>\n",
              "      <td>0.495300</td>\n",
              "      <td>0.408218</td>\n",
              "      <td>0.875486</td>\n",
              "      <td>0.872759</td>\n",
              "      <td>0.864340</td>\n",
              "      <td>0.886002</td>\n",
              "      <td>0.858249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>693</td>\n",
              "      <td>0.481200</td>\n",
              "      <td>0.429940</td>\n",
              "      <td>0.875486</td>\n",
              "      <td>0.871243</td>\n",
              "      <td>0.872618</td>\n",
              "      <td>0.875032</td>\n",
              "      <td>0.857912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>924</td>\n",
              "      <td>0.437600</td>\n",
              "      <td>0.416830</td>\n",
              "      <td>0.878405</td>\n",
              "      <td>0.871078</td>\n",
              "      <td>0.868560</td>\n",
              "      <td>0.880325</td>\n",
              "      <td>0.861412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1155</td>\n",
              "      <td>0.463200</td>\n",
              "      <td>0.435911</td>\n",
              "      <td>0.878405</td>\n",
              "      <td>0.876028</td>\n",
              "      <td>0.876549</td>\n",
              "      <td>0.879799</td>\n",
              "      <td>0.861783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1386</td>\n",
              "      <td>0.249100</td>\n",
              "      <td>0.480800</td>\n",
              "      <td>0.881323</td>\n",
              "      <td>0.876900</td>\n",
              "      <td>0.878786</td>\n",
              "      <td>0.880005</td>\n",
              "      <td>0.864881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1617</td>\n",
              "      <td>0.255300</td>\n",
              "      <td>0.430710</td>\n",
              "      <td>0.900778</td>\n",
              "      <td>0.896068</td>\n",
              "      <td>0.894391</td>\n",
              "      <td>0.898755</td>\n",
              "      <td>0.886583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1848</td>\n",
              "      <td>0.272900</td>\n",
              "      <td>0.503600</td>\n",
              "      <td>0.885214</td>\n",
              "      <td>0.880436</td>\n",
              "      <td>0.889968</td>\n",
              "      <td>0.876624</td>\n",
              "      <td>0.868647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2079</td>\n",
              "      <td>0.279300</td>\n",
              "      <td>0.449622</td>\n",
              "      <td>0.894942</td>\n",
              "      <td>0.889486</td>\n",
              "      <td>0.889967</td>\n",
              "      <td>0.890320</td>\n",
              "      <td>0.879710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2310</td>\n",
              "      <td>0.293600</td>\n",
              "      <td>0.442454</td>\n",
              "      <td>0.888132</td>\n",
              "      <td>0.884806</td>\n",
              "      <td>0.889650</td>\n",
              "      <td>0.882228</td>\n",
              "      <td>0.872097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2541</td>\n",
              "      <td>0.150100</td>\n",
              "      <td>0.473723</td>\n",
              "      <td>0.899805</td>\n",
              "      <td>0.895439</td>\n",
              "      <td>0.896644</td>\n",
              "      <td>0.894806</td>\n",
              "      <td>0.885213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2772</td>\n",
              "      <td>0.149200</td>\n",
              "      <td>0.502964</td>\n",
              "      <td>0.893969</td>\n",
              "      <td>0.889324</td>\n",
              "      <td>0.887255</td>\n",
              "      <td>0.892152</td>\n",
              "      <td>0.878666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3003</td>\n",
              "      <td>0.124500</td>\n",
              "      <td>0.514392</td>\n",
              "      <td>0.894942</td>\n",
              "      <td>0.890277</td>\n",
              "      <td>0.892768</td>\n",
              "      <td>0.889201</td>\n",
              "      <td>0.879685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3234</td>\n",
              "      <td>0.106800</td>\n",
              "      <td>0.525491</td>\n",
              "      <td>0.894942</td>\n",
              "      <td>0.890364</td>\n",
              "      <td>0.894057</td>\n",
              "      <td>0.888076</td>\n",
              "      <td>0.879642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3465</td>\n",
              "      <td>0.101400</td>\n",
              "      <td>0.520516</td>\n",
              "      <td>0.896887</td>\n",
              "      <td>0.891380</td>\n",
              "      <td>0.893272</td>\n",
              "      <td>0.890765</td>\n",
              "      <td>0.881976</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 13:13:27,600]\u001b[0m Trial 34 finished with values: [0.40821805596351624, 0.8727594906409757] and parameters: {'learning_rate': 2.1308732758030146e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 8, 'weight_decay': 0.0015799166052704611}. \u001b[0m\n",
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 256)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=21.0, min_trials=700\n",
            "params: {'learning_rate': 0.0004838555339703523, 'num_train_epochs': 3, 'per_device_train_batch_size': 256, 'weight_decay': 0.001767055979370236}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='108' max='108' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [108/108 08:34, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.288100</td>\n",
              "      <td>2.141694</td>\n",
              "      <td>0.143969</td>\n",
              "      <td>0.035486</td>\n",
              "      <td>0.049405</td>\n",
              "      <td>0.115741</td>\n",
              "      <td>0.031844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.931700</td>\n",
              "      <td>1.760860</td>\n",
              "      <td>0.361868</td>\n",
              "      <td>0.194583</td>\n",
              "      <td>0.155798</td>\n",
              "      <td>0.294484</td>\n",
              "      <td>0.303916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.993700</td>\n",
              "      <td>2.172541</td>\n",
              "      <td>0.242218</td>\n",
              "      <td>0.084884</td>\n",
              "      <td>0.062729</td>\n",
              "      <td>0.164683</td>\n",
              "      <td>0.137884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.982100</td>\n",
              "      <td>2.296722</td>\n",
              "      <td>0.243191</td>\n",
              "      <td>0.083605</td>\n",
              "      <td>0.060925</td>\n",
              "      <td>0.165344</td>\n",
              "      <td>0.139344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>2.129300</td>\n",
              "      <td>1.900215</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.088930</td>\n",
              "      <td>0.079438</td>\n",
              "      <td>0.185171</td>\n",
              "      <td>0.148385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.960500</td>\n",
              "      <td>1.852681</td>\n",
              "      <td>0.302529</td>\n",
              "      <td>0.132600</td>\n",
              "      <td>0.099608</td>\n",
              "      <td>0.211942</td>\n",
              "      <td>0.194210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>1.901600</td>\n",
              "      <td>2.064027</td>\n",
              "      <td>0.254864</td>\n",
              "      <td>0.100521</td>\n",
              "      <td>0.094733</td>\n",
              "      <td>0.184728</td>\n",
              "      <td>0.188748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>2.288300</td>\n",
              "      <td>2.190271</td>\n",
              "      <td>0.219844</td>\n",
              "      <td>0.086679</td>\n",
              "      <td>0.104649</td>\n",
              "      <td>0.149471</td>\n",
              "      <td>0.162240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>2.069200</td>\n",
              "      <td>2.046475</td>\n",
              "      <td>0.234436</td>\n",
              "      <td>0.095847</td>\n",
              "      <td>0.103209</td>\n",
              "      <td>0.159392</td>\n",
              "      <td>0.181391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>2.034100</td>\n",
              "      <td>1.980591</td>\n",
              "      <td>0.255837</td>\n",
              "      <td>0.104140</td>\n",
              "      <td>0.093774</td>\n",
              "      <td>0.173942</td>\n",
              "      <td>0.197068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>2.029800</td>\n",
              "      <td>2.077641</td>\n",
              "      <td>0.293774</td>\n",
              "      <td>0.103324</td>\n",
              "      <td>0.071311</td>\n",
              "      <td>0.199735</td>\n",
              "      <td>0.206446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>1.928000</td>\n",
              "      <td>1.839619</td>\n",
              "      <td>0.307393</td>\n",
              "      <td>0.144704</td>\n",
              "      <td>0.128027</td>\n",
              "      <td>0.213047</td>\n",
              "      <td>0.231694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>1.875400</td>\n",
              "      <td>1.858112</td>\n",
              "      <td>0.309339</td>\n",
              "      <td>0.123300</td>\n",
              "      <td>0.094439</td>\n",
              "      <td>0.212091</td>\n",
              "      <td>0.213346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>1.839500</td>\n",
              "      <td>1.783870</td>\n",
              "      <td>0.313230</td>\n",
              "      <td>0.142058</td>\n",
              "      <td>0.116725</td>\n",
              "      <td>0.226427</td>\n",
              "      <td>0.222181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>1.797400</td>\n",
              "      <td>1.802747</td>\n",
              "      <td>0.302529</td>\n",
              "      <td>0.113750</td>\n",
              "      <td>0.095458</td>\n",
              "      <td>0.217148</td>\n",
              "      <td>0.222337</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "\u001b[32m[I 2022-01-29 13:22:18,360]\u001b[0m Trial 35 finished with values: [1.7608602046966553, 0.19458316444269771] and parameters: {'learning_rate': 0.0004838555339703523, 'num_train_epochs': 3, 'per_device_train_batch_size': 256, 'weight_decay': 0.001767055979370236}. \u001b[0m\n",
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 32)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=171.0, min_trials=700\n",
            "params: {'learning_rate': 5.055557515444166e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 32, 'weight_decay': 0.007704930286495619}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='867' max='867' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [867/867 08:59, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>1.038700</td>\n",
              "      <td>0.504185</td>\n",
              "      <td>0.856031</td>\n",
              "      <td>0.839092</td>\n",
              "      <td>0.848376</td>\n",
              "      <td>0.842076</td>\n",
              "      <td>0.836190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>0.496400</td>\n",
              "      <td>0.418576</td>\n",
              "      <td>0.860895</td>\n",
              "      <td>0.850330</td>\n",
              "      <td>0.848861</td>\n",
              "      <td>0.855644</td>\n",
              "      <td>0.841240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>0.443600</td>\n",
              "      <td>0.413712</td>\n",
              "      <td>0.867704</td>\n",
              "      <td>0.863406</td>\n",
              "      <td>0.858815</td>\n",
              "      <td>0.870930</td>\n",
              "      <td>0.849061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>228</td>\n",
              "      <td>0.419000</td>\n",
              "      <td>0.403482</td>\n",
              "      <td>0.870623</td>\n",
              "      <td>0.865785</td>\n",
              "      <td>0.863475</td>\n",
              "      <td>0.875567</td>\n",
              "      <td>0.853309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>285</td>\n",
              "      <td>0.427300</td>\n",
              "      <td>0.394584</td>\n",
              "      <td>0.877432</td>\n",
              "      <td>0.874872</td>\n",
              "      <td>0.883306</td>\n",
              "      <td>0.868619</td>\n",
              "      <td>0.859601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>342</td>\n",
              "      <td>0.240900</td>\n",
              "      <td>0.350949</td>\n",
              "      <td>0.886187</td>\n",
              "      <td>0.883019</td>\n",
              "      <td>0.883535</td>\n",
              "      <td>0.883128</td>\n",
              "      <td>0.869690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>399</td>\n",
              "      <td>0.200100</td>\n",
              "      <td>0.361852</td>\n",
              "      <td>0.894942</td>\n",
              "      <td>0.891806</td>\n",
              "      <td>0.894693</td>\n",
              "      <td>0.890752</td>\n",
              "      <td>0.879844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>456</td>\n",
              "      <td>0.228300</td>\n",
              "      <td>0.364814</td>\n",
              "      <td>0.898833</td>\n",
              "      <td>0.895090</td>\n",
              "      <td>0.897584</td>\n",
              "      <td>0.895133</td>\n",
              "      <td>0.884220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>513</td>\n",
              "      <td>0.212800</td>\n",
              "      <td>0.379509</td>\n",
              "      <td>0.892023</td>\n",
              "      <td>0.885443</td>\n",
              "      <td>0.883578</td>\n",
              "      <td>0.892761</td>\n",
              "      <td>0.876994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.258100</td>\n",
              "      <td>0.336947</td>\n",
              "      <td>0.900778</td>\n",
              "      <td>0.894476</td>\n",
              "      <td>0.896648</td>\n",
              "      <td>0.894032</td>\n",
              "      <td>0.886387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>627</td>\n",
              "      <td>0.122300</td>\n",
              "      <td>0.355591</td>\n",
              "      <td>0.901751</td>\n",
              "      <td>0.898977</td>\n",
              "      <td>0.899830</td>\n",
              "      <td>0.899873</td>\n",
              "      <td>0.887693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>684</td>\n",
              "      <td>0.107000</td>\n",
              "      <td>0.379559</td>\n",
              "      <td>0.902724</td>\n",
              "      <td>0.898598</td>\n",
              "      <td>0.901675</td>\n",
              "      <td>0.896162</td>\n",
              "      <td>0.888542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>741</td>\n",
              "      <td>0.099800</td>\n",
              "      <td>0.381090</td>\n",
              "      <td>0.900778</td>\n",
              "      <td>0.896665</td>\n",
              "      <td>0.901043</td>\n",
              "      <td>0.894207</td>\n",
              "      <td>0.886375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>798</td>\n",
              "      <td>0.075100</td>\n",
              "      <td>0.396366</td>\n",
              "      <td>0.898833</td>\n",
              "      <td>0.894247</td>\n",
              "      <td>0.895915</td>\n",
              "      <td>0.893759</td>\n",
              "      <td>0.884136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>855</td>\n",
              "      <td>0.072500</td>\n",
              "      <td>0.397643</td>\n",
              "      <td>0.899805</td>\n",
              "      <td>0.894400</td>\n",
              "      <td>0.897144</td>\n",
              "      <td>0.893058</td>\n",
              "      <td>0.885292</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 13:31:29,007]\u001b[0m Trial 36 finished with values: [0.336946964263916, 0.8944764602762665] and parameters: {'learning_rate': 5.055557515444166e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 32, 'weight_decay': 0.007704930286495619}. \u001b[0m\n",
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 8)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=693.0, min_trials=700\n",
            "params: {'learning_rate': 0.0001213956551304579, 'num_train_epochs': 3, 'per_device_train_batch_size': 8, 'weight_decay': 0.004980760292979186}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3468' max='3468' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3468/3468 10:11, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>231</td>\n",
              "      <td>1.005800</td>\n",
              "      <td>0.854318</td>\n",
              "      <td>0.734436</td>\n",
              "      <td>0.726425</td>\n",
              "      <td>0.771102</td>\n",
              "      <td>0.735279</td>\n",
              "      <td>0.702844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>462</td>\n",
              "      <td>0.769400</td>\n",
              "      <td>0.624968</td>\n",
              "      <td>0.829767</td>\n",
              "      <td>0.824237</td>\n",
              "      <td>0.847810</td>\n",
              "      <td>0.813396</td>\n",
              "      <td>0.805942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>693</td>\n",
              "      <td>0.742200</td>\n",
              "      <td>0.669259</td>\n",
              "      <td>0.823930</td>\n",
              "      <td>0.816848</td>\n",
              "      <td>0.830567</td>\n",
              "      <td>0.829094</td>\n",
              "      <td>0.801379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>924</td>\n",
              "      <td>0.696000</td>\n",
              "      <td>0.619986</td>\n",
              "      <td>0.821984</td>\n",
              "      <td>0.813830</td>\n",
              "      <td>0.813301</td>\n",
              "      <td>0.827854</td>\n",
              "      <td>0.798229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1155</td>\n",
              "      <td>0.713900</td>\n",
              "      <td>0.590808</td>\n",
              "      <td>0.839494</td>\n",
              "      <td>0.834079</td>\n",
              "      <td>0.843496</td>\n",
              "      <td>0.832529</td>\n",
              "      <td>0.817522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1386</td>\n",
              "      <td>0.459000</td>\n",
              "      <td>0.799877</td>\n",
              "      <td>0.838521</td>\n",
              "      <td>0.839789</td>\n",
              "      <td>0.852380</td>\n",
              "      <td>0.841509</td>\n",
              "      <td>0.817586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1617</td>\n",
              "      <td>0.449500</td>\n",
              "      <td>0.606432</td>\n",
              "      <td>0.863813</td>\n",
              "      <td>0.860312</td>\n",
              "      <td>0.863990</td>\n",
              "      <td>0.869220</td>\n",
              "      <td>0.846603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1848</td>\n",
              "      <td>0.476100</td>\n",
              "      <td>0.612100</td>\n",
              "      <td>0.864786</td>\n",
              "      <td>0.864214</td>\n",
              "      <td>0.877521</td>\n",
              "      <td>0.856419</td>\n",
              "      <td>0.845450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2079</td>\n",
              "      <td>0.418300</td>\n",
              "      <td>0.599132</td>\n",
              "      <td>0.875486</td>\n",
              "      <td>0.870009</td>\n",
              "      <td>0.879852</td>\n",
              "      <td>0.863937</td>\n",
              "      <td>0.857974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2310</td>\n",
              "      <td>0.456300</td>\n",
              "      <td>0.590836</td>\n",
              "      <td>0.867704</td>\n",
              "      <td>0.860504</td>\n",
              "      <td>0.877566</td>\n",
              "      <td>0.851618</td>\n",
              "      <td>0.849452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2541</td>\n",
              "      <td>0.285100</td>\n",
              "      <td>0.609004</td>\n",
              "      <td>0.886187</td>\n",
              "      <td>0.882954</td>\n",
              "      <td>0.894424</td>\n",
              "      <td>0.875564</td>\n",
              "      <td>0.869766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2772</td>\n",
              "      <td>0.218600</td>\n",
              "      <td>0.612207</td>\n",
              "      <td>0.884241</td>\n",
              "      <td>0.877093</td>\n",
              "      <td>0.874190</td>\n",
              "      <td>0.883418</td>\n",
              "      <td>0.867766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3003</td>\n",
              "      <td>0.246500</td>\n",
              "      <td>0.573597</td>\n",
              "      <td>0.892996</td>\n",
              "      <td>0.885689</td>\n",
              "      <td>0.884302</td>\n",
              "      <td>0.887916</td>\n",
              "      <td>0.877534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3234</td>\n",
              "      <td>0.189900</td>\n",
              "      <td>0.556026</td>\n",
              "      <td>0.891051</td>\n",
              "      <td>0.886478</td>\n",
              "      <td>0.885033</td>\n",
              "      <td>0.888839</td>\n",
              "      <td>0.875247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3465</td>\n",
              "      <td>0.173800</td>\n",
              "      <td>0.552101</td>\n",
              "      <td>0.890078</td>\n",
              "      <td>0.885508</td>\n",
              "      <td>0.887137</td>\n",
              "      <td>0.885063</td>\n",
              "      <td>0.874055</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 13:41:51,814]\u001b[0m Trial 37 finished with values: [0.552101194858551, 0.8855075042466827] and parameters: {'learning_rate': 0.0001213956551304579, 'num_train_epochs': 3, 'per_device_train_batch_size': 8, 'weight_decay': 0.004980760292979186}. \u001b[0m\n",
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 256)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=21.0, min_trials=700\n",
            "params: {'learning_rate': 5.392686865617486e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 256, 'weight_decay': 0.004339891033507396}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='108' max='108' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [108/108 08:31, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.156700</td>\n",
              "      <td>2.082201</td>\n",
              "      <td>0.287938</td>\n",
              "      <td>0.172366</td>\n",
              "      <td>0.190799</td>\n",
              "      <td>0.220505</td>\n",
              "      <td>0.190490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.050000</td>\n",
              "      <td>1.978758</td>\n",
              "      <td>0.422179</td>\n",
              "      <td>0.278948</td>\n",
              "      <td>0.312731</td>\n",
              "      <td>0.317954</td>\n",
              "      <td>0.336108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.971100</td>\n",
              "      <td>1.882225</td>\n",
              "      <td>0.475681</td>\n",
              "      <td>0.317459</td>\n",
              "      <td>0.337079</td>\n",
              "      <td>0.359884</td>\n",
              "      <td>0.395979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.857000</td>\n",
              "      <td>1.784520</td>\n",
              "      <td>0.530156</td>\n",
              "      <td>0.357851</td>\n",
              "      <td>0.366209</td>\n",
              "      <td>0.405409</td>\n",
              "      <td>0.459887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.783000</td>\n",
              "      <td>1.689674</td>\n",
              "      <td>0.566148</td>\n",
              "      <td>0.388216</td>\n",
              "      <td>0.424876</td>\n",
              "      <td>0.436117</td>\n",
              "      <td>0.502313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.737500</td>\n",
              "      <td>1.602826</td>\n",
              "      <td>0.582685</td>\n",
              "      <td>0.416298</td>\n",
              "      <td>0.441156</td>\n",
              "      <td>0.455914</td>\n",
              "      <td>0.520850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>1.601400</td>\n",
              "      <td>1.525726</td>\n",
              "      <td>0.602140</td>\n",
              "      <td>0.435701</td>\n",
              "      <td>0.447825</td>\n",
              "      <td>0.474848</td>\n",
              "      <td>0.544347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>1.541800</td>\n",
              "      <td>1.456419</td>\n",
              "      <td>0.615759</td>\n",
              "      <td>0.451065</td>\n",
              "      <td>0.564660</td>\n",
              "      <td>0.487330</td>\n",
              "      <td>0.560086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>1.459900</td>\n",
              "      <td>1.398000</td>\n",
              "      <td>0.632296</td>\n",
              "      <td>0.471328</td>\n",
              "      <td>0.578336</td>\n",
              "      <td>0.502064</td>\n",
              "      <td>0.579553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.450100</td>\n",
              "      <td>1.346997</td>\n",
              "      <td>0.642996</td>\n",
              "      <td>0.486573</td>\n",
              "      <td>0.577872</td>\n",
              "      <td>0.514570</td>\n",
              "      <td>0.591648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>1.402400</td>\n",
              "      <td>1.306845</td>\n",
              "      <td>0.650778</td>\n",
              "      <td>0.502586</td>\n",
              "      <td>0.688427</td>\n",
              "      <td>0.525809</td>\n",
              "      <td>0.600349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>1.350500</td>\n",
              "      <td>1.273730</td>\n",
              "      <td>0.664397</td>\n",
              "      <td>0.522717</td>\n",
              "      <td>0.693668</td>\n",
              "      <td>0.542011</td>\n",
              "      <td>0.616054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>1.342200</td>\n",
              "      <td>1.248931</td>\n",
              "      <td>0.674125</td>\n",
              "      <td>0.531739</td>\n",
              "      <td>0.698470</td>\n",
              "      <td>0.550922</td>\n",
              "      <td>0.627377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>1.303200</td>\n",
              "      <td>1.232167</td>\n",
              "      <td>0.674125</td>\n",
              "      <td>0.532197</td>\n",
              "      <td>0.698321</td>\n",
              "      <td>0.551148</td>\n",
              "      <td>0.627319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>1.257700</td>\n",
              "      <td>1.224004</td>\n",
              "      <td>0.676070</td>\n",
              "      <td>0.540467</td>\n",
              "      <td>0.699395</td>\n",
              "      <td>0.555264</td>\n",
              "      <td>0.629559</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "\u001b[32m[I 2022-01-29 13:50:38,467]\u001b[0m Trial 38 finished with values: [1.2240041494369507, 0.54046694494783] and parameters: {'learning_rate': 5.392686865617486e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 256, 'weight_decay': 0.004339891033507396}. \u001b[0m\n",
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 32)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=171.0, min_trials=700\n",
            "params: {'learning_rate': 0.00014702055839263257, 'num_train_epochs': 3, 'per_device_train_batch_size': 32, 'weight_decay': 0.001630971369462351}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='867' max='867' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [867/867 08:58, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.898000</td>\n",
              "      <td>0.660651</td>\n",
              "      <td>0.798638</td>\n",
              "      <td>0.785095</td>\n",
              "      <td>0.793563</td>\n",
              "      <td>0.806144</td>\n",
              "      <td>0.773463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>0.570400</td>\n",
              "      <td>0.525529</td>\n",
              "      <td>0.828794</td>\n",
              "      <td>0.822005</td>\n",
              "      <td>0.837418</td>\n",
              "      <td>0.822530</td>\n",
              "      <td>0.804911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>0.565000</td>\n",
              "      <td>0.471834</td>\n",
              "      <td>0.856031</td>\n",
              "      <td>0.846651</td>\n",
              "      <td>0.847570</td>\n",
              "      <td>0.857199</td>\n",
              "      <td>0.836550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>228</td>\n",
              "      <td>0.476600</td>\n",
              "      <td>0.456308</td>\n",
              "      <td>0.859922</td>\n",
              "      <td>0.854037</td>\n",
              "      <td>0.856160</td>\n",
              "      <td>0.863981</td>\n",
              "      <td>0.840907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>285</td>\n",
              "      <td>0.548900</td>\n",
              "      <td>0.414406</td>\n",
              "      <td>0.879377</td>\n",
              "      <td>0.875806</td>\n",
              "      <td>0.881667</td>\n",
              "      <td>0.871774</td>\n",
              "      <td>0.861747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>342</td>\n",
              "      <td>0.324600</td>\n",
              "      <td>0.464491</td>\n",
              "      <td>0.866732</td>\n",
              "      <td>0.853801</td>\n",
              "      <td>0.859490</td>\n",
              "      <td>0.856289</td>\n",
              "      <td>0.847959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>399</td>\n",
              "      <td>0.254000</td>\n",
              "      <td>0.446613</td>\n",
              "      <td>0.872568</td>\n",
              "      <td>0.866990</td>\n",
              "      <td>0.869521</td>\n",
              "      <td>0.870346</td>\n",
              "      <td>0.854727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>456</td>\n",
              "      <td>0.297000</td>\n",
              "      <td>0.423336</td>\n",
              "      <td>0.880350</td>\n",
              "      <td>0.872010</td>\n",
              "      <td>0.876185</td>\n",
              "      <td>0.872445</td>\n",
              "      <td>0.863396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>513</td>\n",
              "      <td>0.249200</td>\n",
              "      <td>0.409162</td>\n",
              "      <td>0.890078</td>\n",
              "      <td>0.883529</td>\n",
              "      <td>0.882581</td>\n",
              "      <td>0.890833</td>\n",
              "      <td>0.875127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.301200</td>\n",
              "      <td>0.376262</td>\n",
              "      <td>0.891051</td>\n",
              "      <td>0.887757</td>\n",
              "      <td>0.892161</td>\n",
              "      <td>0.884052</td>\n",
              "      <td>0.875117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>627</td>\n",
              "      <td>0.134100</td>\n",
              "      <td>0.436217</td>\n",
              "      <td>0.887160</td>\n",
              "      <td>0.884540</td>\n",
              "      <td>0.886444</td>\n",
              "      <td>0.883405</td>\n",
              "      <td>0.870764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>684</td>\n",
              "      <td>0.121200</td>\n",
              "      <td>0.429660</td>\n",
              "      <td>0.893969</td>\n",
              "      <td>0.886121</td>\n",
              "      <td>0.888247</td>\n",
              "      <td>0.885479</td>\n",
              "      <td>0.878773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>741</td>\n",
              "      <td>0.096200</td>\n",
              "      <td>0.480427</td>\n",
              "      <td>0.897860</td>\n",
              "      <td>0.888999</td>\n",
              "      <td>0.896151</td>\n",
              "      <td>0.886100</td>\n",
              "      <td>0.883315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>798</td>\n",
              "      <td>0.085000</td>\n",
              "      <td>0.451435</td>\n",
              "      <td>0.886187</td>\n",
              "      <td>0.879511</td>\n",
              "      <td>0.882254</td>\n",
              "      <td>0.877881</td>\n",
              "      <td>0.869572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>855</td>\n",
              "      <td>0.066300</td>\n",
              "      <td>0.445000</td>\n",
              "      <td>0.892023</td>\n",
              "      <td>0.885096</td>\n",
              "      <td>0.886896</td>\n",
              "      <td>0.884428</td>\n",
              "      <td>0.876363</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-29 13:59:48,269]\u001b[0m Trial 39 finished with values: [0.37626218795776367, 0.8877572152080605] and parameters: {'learning_rate': 0.00014702055839263257, 'num_train_epochs': 3, 'per_device_train_batch_size': 32, 'weight_decay': 0.001630971369462351}. \u001b[0m\n",
            "fixed params: [('num_train_epochs', 3), ('per_device_train_batch_size', 64)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=84.0, min_trials=700\n",
            "params: {'learning_rate': 4.131270983914418e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 64, 'weight_decay': 0.0012996858167884208}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='282' max='432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [282/432 05:42 < 03:03, 0.82 it/s, Epoch 1.95/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.362900</td>\n",
              "      <td>0.675870</td>\n",
              "      <td>0.818093</td>\n",
              "      <td>0.799567</td>\n",
              "      <td>0.836014</td>\n",
              "      <td>0.785481</td>\n",
              "      <td>0.792426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.593600</td>\n",
              "      <td>0.461886</td>\n",
              "      <td>0.860895</td>\n",
              "      <td>0.855798</td>\n",
              "      <td>0.859355</td>\n",
              "      <td>0.860816</td>\n",
              "      <td>0.841852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.419125</td>\n",
              "      <td>0.866732</td>\n",
              "      <td>0.859106</td>\n",
              "      <td>0.871995</td>\n",
              "      <td>0.856416</td>\n",
              "      <td>0.848141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.431200</td>\n",
              "      <td>0.379825</td>\n",
              "      <td>0.885214</td>\n",
              "      <td>0.877976</td>\n",
              "      <td>0.878065</td>\n",
              "      <td>0.882681</td>\n",
              "      <td>0.869133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.447100</td>\n",
              "      <td>0.372712</td>\n",
              "      <td>0.884241</td>\n",
              "      <td>0.878633</td>\n",
              "      <td>0.875935</td>\n",
              "      <td>0.883313</td>\n",
              "      <td>0.867716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>0.297800</td>\n",
              "      <td>0.352463</td>\n",
              "      <td>0.882296</td>\n",
              "      <td>0.878974</td>\n",
              "      <td>0.882626</td>\n",
              "      <td>0.876395</td>\n",
              "      <td>0.865196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>0.234600</td>\n",
              "      <td>0.347910</td>\n",
              "      <td>0.886187</td>\n",
              "      <td>0.879943</td>\n",
              "      <td>0.875481</td>\n",
              "      <td>0.885816</td>\n",
              "      <td>0.869860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>224</td>\n",
              "      <td>0.256600</td>\n",
              "      <td>0.341687</td>\n",
              "      <td>0.890078</td>\n",
              "      <td>0.884916</td>\n",
              "      <td>0.882210</td>\n",
              "      <td>0.889309</td>\n",
              "      <td>0.874318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>252</td>\n",
              "      <td>0.250600</td>\n",
              "      <td>0.333345</td>\n",
              "      <td>0.893969</td>\n",
              "      <td>0.889003</td>\n",
              "      <td>0.892741</td>\n",
              "      <td>0.886495</td>\n",
              "      <td>0.878534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.266300</td>\n",
              "      <td>0.329047</td>\n",
              "      <td>0.898833</td>\n",
              "      <td>0.893913</td>\n",
              "      <td>0.896075</td>\n",
              "      <td>0.892224</td>\n",
              "      <td>0.884057</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import optuna\n",
        "from optuna.storages import RDBStorage\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "db_path = \"/content/gdrive/My Drive/Colab Notebooks/nlp-classification/\"\n",
        "db_name = \"10kgnad_optuna\"\n",
        "# study_name = checkpoint + \"_multi_epoch234\"\n",
        "# study_name = checkpoint + \"_loss-f1_bs32_epoch23\"\n",
        "# study_name = checkpoint + \"_loss-f1_bs32_ep3_pad\"\n",
        "study_name = checkpoint + \"_bs8-256_ep3_len128\"\n",
        "\n",
        "# automatically change the state of a stale trial to TrialState.FAIL from TrialState.RUNNING\n",
        "storage = RDBStorage(url=f\"sqlite:///{db_path}{db_name}.db\", heartbeat_interval=60, grace_period=120)\n",
        "\n",
        "# https://stackoverflow.com/questions/59129812/how-to-avoid-cuda-out-of-memory-in-pytorch\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "# multi objective study\n",
        "# https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/002_multi_objective.html#sphx-glr-tutorial-20-recipes-002-multi-objective-py\n",
        "study = optuna.create_study(study_name=study_name,\n",
        "                            directions=[\"minimize\", \"maximize\"],\n",
        "                            # storage=f\"sqlite:///{db_path}{db_name}.db\",\n",
        "                            storage=storage,\n",
        "                            load_if_exists=True,)\n",
        "\n",
        "# ------ prime with parameters\n",
        "def lr_sample(min, max, dist=0.1, jitter=0.1):\n",
        "    min_log = np.log10(min)\n",
        "    max_log = np.log10(max)\n",
        "    n = int((max_log - min_log) / dist)\n",
        "    return np.logspace(min_log, max_log, n) * np.random.uniform(1-jitter, 1+jitter, size=n)\n",
        "\n",
        "def lr_pairs():\n",
        "    lrs = []\n",
        "    min_lr = 5e-6\n",
        "    max_lr = 5e-4\n",
        "    for bs in [8, 16, 32, 64, 128, 256]:\n",
        "        lrs.extend((bs, lr) for lr in lr_sample(min_lr, max_lr, dist=0.05, jitter=0.05))\n",
        "    random.shuffle(lrs)\n",
        "    return lrs\n",
        "\n",
        "for bs, lr in lr_pairs():\n",
        "    study.enqueue_trial(\n",
        "        {\n",
        "                \"learning_rate\": lr,\n",
        "                \"per_device_train_batch_size\": bs,\n",
        "            }\n",
        "        )\n",
        "\n",
        "# give some hyperparameters that are presumably good\n",
        "# for bs in [8,16,32,64,128]:\n",
        "#     for lr in np.exp(np.linspace(np.log(7e-6), np.log(2e-4), 15)):\n",
        "#         study.enqueue_trial(\n",
        "#             {\n",
        "#                 \"learning_rate\": lr,\n",
        "#                 \"per_device_train_batch_size\": bs,\n",
        "#             }\n",
        "#         )\n",
        "# study.enqueue_trial(\n",
        "#     {\n",
        "#         \"learning_rate\": 5.1e-5,\n",
        "#         \"per_device_train_batch_size\": 32,\n",
        "#     }\n",
        "# )\n",
        "# study.enqueue_trial(\n",
        "#     {\n",
        "#         \"learning_rate\": 5.8e-5,\n",
        "#         \"per_device_train_batch_size\": 32,\n",
        "#     }\n",
        "# )\n",
        "\n",
        "\n",
        "study.optimize(objective, n_trials=200, callbacks=[best_model_callback])\n",
        "\n",
        "# study.best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaHTHuP9W6Dn"
      },
      "outputs": [],
      "source": [
        "!ls -lahtr $project_name"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## New Optimization Code"
      ],
      "metadata": {
        "id": "WmIwj5olOKAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q optuna transformers datasets >/dev/null"
      ],
      "metadata": {
        "id": "xS4c2CltzlSk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3_9Al_rbBrla"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainerCallback\n",
        "from optuna.trial import Trial, TrialState\n",
        "from optuna.study._study_direction import StudyDirection\n",
        "import pandas as pd\n",
        "\n",
        "# https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/trainer_callback.py#L505\n",
        "# https://huggingface.co/docs/transformers/main_classes/callback#transformers.TrainerCallback\n",
        "\n",
        "import logging\n",
        "logging.getLogger(__name__).setLevel(logging.INFO)\n",
        "log = logging.getLogger(__name__)\n",
        "\n",
        "class TrialLogAndPruningCallback(TrainerCallback):\n",
        "    \"\"\"Stores eval metrics at each evaluation step in the trial user attrs.\"\"\"\n",
        "    def __init__(self, trial: Trial, objectives=None, warmup_steps=0, min_trials=7):\n",
        "        self.study = trial.study\n",
        "        self.trial = trial\n",
        "        self.param_keys = [\"num_train_epochs\", \"batch_size\"]\n",
        "        self.param_vals = [trial.params[k] for k in self.param_keys]\n",
        "\n",
        "        log.warning(f\"fixed params: {list(zip(self.param_keys, self.param_vals))}\")\n",
        "\n",
        "        if objectives == None:\n",
        "            self.objectives = [\"eval_loss\"]\n",
        "        else:\n",
        "            self.objectives = objectives\n",
        "        self._warmup_steps = warmup_steps\n",
        "        self._min_trials = max(1, int(min_trials))\n",
        "\n",
        "        log.warning(f\"objectives: {self.objectives}, directions: {self.study.directions}, warmup={self._warmup_steps}, min_trials={self._min_trials}\")\n",
        "        log.warning(f\"params: {trial.params}\")\n",
        "        \n",
        "\n",
        "    def _filter_trials(self, complete_trials):\n",
        "        \"\"\"Select only trials with same parameter values\"\"\"\n",
        "        # values = [self.trial.params[k] for k in keys]\n",
        "        return [t for t in complete_trials if self.param_vals == [t.params[k] for k in self.param_keys]]\n",
        "\n",
        "    def _prune(self, step: int, metrics) -> bool:\n",
        "        \"\"\"Median Pruning on multiple objectives.\"\"\"\n",
        "        if step < self._warmup_steps:\n",
        "            # log.warning(f\"less than warmup steps {step}<{self._warmup_steps}\")\n",
        "            return False\n",
        "\n",
        "        # get all completed trials\n",
        "        complete_trials = self.study.get_trials(deepcopy=False,\n",
        "                                                states=[TrialState.COMPLETE])\n",
        "        # only compare trials with same batch size and epochs\n",
        "        complete_trials = self._filter_trials(complete_trials)\n",
        "        n_trials = len(complete_trials)\n",
        "\n",
        "        # check minimal number of trial required\n",
        "        if n_trials < self._min_trials:\n",
        "            # log.warning(f\"less than min trials {n_trials}<{self._min_trials}\")\n",
        "            return False\n",
        "\n",
        "        # log.warning(f\"checking {step}: {metrics}\")\n",
        "\n",
        "        # sanity check\n",
        "        has_metrics = [o in metrics.keys() for o in self.objectives]\n",
        "        if not all(has_metrics):\n",
        "            log.warning(f\"missing objective metrics {list(zip(self.objectives, has_metrics))}\")\n",
        "\n",
        "        # extract metrics from trials\n",
        "        # print(f\"fetching metrics of {n_trials} complete trials\")\n",
        "        trial_metrics = []\n",
        "        for t in complete_trials:\n",
        "            # print(str(step), \"in keys?\", str(step) in t.user_attrs.keys(), t.user_attrs.keys())\n",
        "            if str(step) in t.user_attrs.keys():\n",
        "                trial_metrics.append(t.user_attrs[str(step)])\n",
        "        n_metrics = len(trial_metrics)\n",
        "\n",
        "        # compute median for each metric over all trials\n",
        "        median = pd.DataFrame(trial_metrics).median()\n",
        "\n",
        "        # log.warning(f\"median of {n_metrics}/{n_trials}: {median.to_dict()}\")\n",
        "\n",
        "        # compare current metric value with median\n",
        "        prune_state = []\n",
        "        for i, o in enumerate(self.objectives):\n",
        "            if self.study.directions[i] == StudyDirection.MAXIMIZE:\n",
        "                prune_state.append(metrics[o] <= median[o])\n",
        "            else:\n",
        "                prune_state.append(metrics[o] > median[o])\n",
        "        \n",
        "        met = \",\".join([f\"{m}={metrics[m]:.4}/{median[m]:.4}\" for m in self.objectives])\n",
        "        print(f\"prune? step={step}, warmup={self._warmup_steps}, complete_trials={n_trials}, metrics={n_metrics} -> {met}; {prune_state}\")\n",
        "\n",
        "        # all metrics must be marked for pruning\n",
        "        return all(prune_state)\n",
        "    \n",
        "    def on_evaluate(self, args, state, control, lr_scheduler, metrics, **kwargs):\n",
        "        step = state.global_step\n",
        "        values = {**metrics, \"lr\": lr_scheduler.get_last_lr()[-1]}\n",
        "        self.trial.set_user_attr(str(step), values)\n",
        "\n",
        "        # pruning\n",
        "        if self._prune(step, metrics):\n",
        "            print(f\"pruning trial at step {step}\")\n",
        "            # control.should_training_stop = True  # not needed\n",
        "            raise optuna.TrialPruned()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "\n",
        "class Interval:\n",
        "    def __init__(self, min, max, log=False):\n",
        "        self.min = min\n",
        "        self.max = max\n",
        "        self.log = log\n",
        "\n",
        "def uniform(min, max):\n",
        "    return Interval(min, max, log=False)\n",
        "\n",
        "def log_uniform(min, max):\n",
        "    return Interval(min, max, log=True)\n",
        "\n",
        "class Space:\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        self.params = kwargs\n",
        "    \n",
        "    def _suggest(self, trial, key, val):\n",
        "        if val is None or isinstance(val, (bool, float, int, str)):\n",
        "            return trial.suggest_categorical(key, [val])\n",
        "        if isinstance(val, list):\n",
        "            return trial.suggest_categorical(key, val)\n",
        "        if isinstance(val, Interval):\n",
        "            return trial.suggest_float(key, val.min, val.max, log=val.log)\n",
        "\n",
        "    def suggest(self, trial: optuna.trial.Trial):\n",
        "        return {k:self._suggest(trial, k, v) for k, v in self.params.items()}\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import logging as trlog\n",
        "from datasets import DatasetDict, Dataset, load_dataset\n",
        "from datasets import logging as dslog\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, matthews_corrcoef, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "# hide progress bar when downloading dataset - needs workaround!\n",
        "dslog.get_verbosity = lambda : logging.NOTSET\n",
        "trlog.get_verbosity = lambda : logging.NOTSET\n",
        "\n",
        "def load_data():\n",
        "    base_url = \"https://raw.githubusercontent.com/tblock/10kGNAD/master/{}.csv\"\n",
        "    data_files = {x: base_url.format(x) for x in [\"train\", \"test\"]}\n",
        "    dataset = (load_dataset('csv',\n",
        "                            data_files=data_files,\n",
        "                            sep=\";\",\n",
        "                            quotechar=\"'\",\n",
        "                            names=[\"label\", \"text\"]).\n",
        "            class_encode_column(\"label\"))\n",
        "    label_names = dataset[\"train\"].features[\"label\"].names\n",
        "    return dataset, label_names\n",
        "\n",
        "\n",
        "def sample_data(ds: DatasetDict, train_size: float, columns=[\"text\", \"label\"]):\n",
        "    \"\"\"Create a stratified sample of the train dataset.\"\"\"\n",
        "    X = ds[\"train\"][columns[0]]\n",
        "    y = ds[\"train\"][columns[1]]\n",
        "    X_train, _, y_train, _ = train_test_split(X, y, train_size=train_size, random_state=42, stratify=y)\n",
        "    return DatasetDict({\n",
        "        \"train\": Dataset.from_dict({\"label\": y_train, \"text\": X_train}),\n",
        "        \"test\": ds[\"test\"]\n",
        "    })\n",
        "\n",
        "data_cache = {}\n",
        "\n",
        "def prepare_data(tokenizer, params) -> DatasetDict:\n",
        "\n",
        "    train_size = params[\"train_size\"]\n",
        "    max_seq_length = params[\"max_seq_length\"]\n",
        "    identifier = f\"{train_size}/{max_seq_length}\"\n",
        "\n",
        "    if identifier in data_cache:\n",
        "        print(f\"USING CACHED DATASET for {identifier}\")\n",
        "        return data_cache[identifier]\n",
        "    else:\n",
        "        print(\"GENERATING DATASET\")\n",
        "\n",
        "    dataset, label_names = load_data()\n",
        "\n",
        "    if train_size is not None and train_size < 1.0:\n",
        "        dataset = sample_data(dataset, train_size)\n",
        "\n",
        "    if max_seq_length is None:\n",
        "        max_seq_length = getattr(tokenizer, \"model_max_length\")\n",
        "    \n",
        "    # TODO better use a partial function\n",
        "    def preprocess_function(examples):\n",
        "        return tokenizer(examples[\"text\"], truncation=True, max_length=max_seq_length)\n",
        "    \n",
        "    mapped_data = dataset.map(preprocess_function, batched=True).remove_columns(\"text\")\n",
        "\n",
        "    print(\"STORING DATASET\")\n",
        "    data_cache[identifier] = (mapped_data, label_names)\n",
        "    return mapped_data, label_names\n",
        "\n",
        "\n",
        "def create_training_args(params: dict, config: dict) -> TrainingArguments:\n",
        "    \n",
        "    # output_dir = config[\"output_dir\"]\n",
        "    train_rows = config[\"train_rows\"]\n",
        "    base_batch_size = config[\"base_batch_size\"]\n",
        "    eval_rounds_per_epoch = config[\"eval_rounds_per_epoch\"]\n",
        "\n",
        "    # calculate gradient_accumulation and evaluation steps\n",
        "    bs = params[\"batch_size\"]\n",
        "    gradient_accumulation_steps = bs // base_batch_size\n",
        "    eval_steps = train_rows / bs // eval_rounds_per_epoch\n",
        "    if (train_rows / bs < eval_rounds_per_epoch):\n",
        "        raise ValueError(f\"batch size {bs} is too big for {train_rows} examples and {eval_rounds_per_epoch} eval rounds!\")\n",
        "\n",
        "    return TrainingArguments(\n",
        "        output_dir=config[\"output_dir\"],\n",
        "        report_to=[],\n",
        "        log_level=\"error\",\n",
        "        disable_tqdm=False,\n",
        "\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=eval_steps,\n",
        "        logging_steps=eval_steps,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=eval_steps,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        greater_is_better=False,\n",
        "\n",
        "        # hyperparameters\n",
        "        num_train_epochs=params[\"num_train_epochs\"],\n",
        "        learning_rate=params[\"learning_rate\"],\n",
        "        per_device_train_batch_size=base_batch_size,\n",
        "        per_device_eval_batch_size=base_batch_size,\n",
        "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "        weight_decay=params[\"weight_decay\"],\n",
        "\n",
        "        # fp16=True,  # fp16 needs apex. but disabled on Tesla P100 by pytorch\n",
        "    )\n",
        "\n",
        "\n",
        "def init_model(checkpoint, label_names):\n",
        "    \"\"\"A function that instantiates the model to be used.\"\"\"\n",
        "\n",
        "    # We want to include the label names and save them together with the model.\n",
        "    # The only way to do this is to create a Config and put them in. \n",
        "    config = AutoConfig.from_pretrained(\n",
        "            checkpoint,\n",
        "            num_labels=len(label_names),\n",
        "            id2label={i: label for i, label in enumerate(label_names)},\n",
        "            label2id={label: i for i, label in enumerate(label_names)},\n",
        "            )\n",
        "\n",
        "    return AutoModelForSequenceClassification.from_pretrained(checkpoint, config=config)\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    \"\"\"The function that will be used to compute metrics at evaluation.\n",
        "    Must take a :class:`~transformers.EvalPrediction` and return a dictionary\n",
        "    string to metric values.\"\"\"\n",
        "    logits, labels = eval_preds\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    # precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "    return {\n",
        "        \"acc\": accuracy_score(labels, preds),\n",
        "        \"f1\": f1_score(labels, preds, average='macro'),\n",
        "        \"precision\": precision_score(labels, preds, average='macro'),\n",
        "        \"recall\": recall_score(labels, preds, average='macro'),\n",
        "        \"mcc\": matthews_corrcoef(labels, preds),\n",
        "        }\n",
        "\n",
        "def objective(trial):\n",
        "\n",
        "    # suggest hyperparameters\n",
        "    hp = space.suggest(trial)\n",
        "    print(hp)\n",
        "\n",
        "    checkpoint = hp[\"model\"]\n",
        "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "    # prepare dataset\n",
        "    tokenized_dataset, label_names = prepare_data(tokenizer, hp)\n",
        "    # print(f\"LABEL NAMES {label_names}\")\n",
        "\n",
        "\n",
        "    project_name = \"test\"\n",
        "    best_model_dir = \"best_model\"\n",
        "\n",
        "    config = {\n",
        "        \"output_dir\": project_name,\n",
        "        \"base_batch_size\": 8,\n",
        "        \"eval_rounds_per_epoch\": 5,\n",
        "        \"train_rows\": tokenized_dataset[\"train\"].num_rows\n",
        "    }\n",
        "\n",
        "    ## TODO: calculate batch size and aggregations steps separately\n",
        "\n",
        "    # create training args\n",
        "    training_args = create_training_args(hp, config)\n",
        "    # print(args)\n",
        "\n",
        "    # https://stackoverflow.com/questions/59129812/how-to-avoid-cuda-out-of-memory-in-pytorch\n",
        "    import torch\n",
        "    torch.cuda.empty_cache()\n",
        "    import gc\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "    # prepare Trainer\n",
        "    trainer = Trainer(\n",
        "        model_init=lambda x: init_model(checkpoint, label_names),\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset[\"train\"],\n",
        "        eval_dataset=tokenized_dataset[\"test\"],\n",
        "        tokenizer=tokenizer,\n",
        "        # data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[TrialLogAndPruningCallback(trial, objectives=[\"eval_loss\", \"eval_f1\"], min_trials=700, warmup_steps=training_args.eval_steps*3)]\n",
        "    )\n",
        "\n",
        "    # train model and save best model from evaluations\n",
        "    # needs 'load_best_model_at_end=True'\n",
        "    trainer.train()\n",
        "    trainer.save_model(f\"{project_name}/{best_model_dir}\")\n",
        "\n",
        "    result = trainer.evaluate(eval_dataset=tokenized_dataset[\"test\"])\n",
        "\n",
        "    # store eval metrics in trial\n",
        "    trial.set_user_attr(\"eval_result\", result)\n",
        "    \n",
        "    # return result[\"eval_loss\"]\n",
        "    return result[\"eval_loss\"], result[\"eval_f1\"]\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def lr_sample(min, max, dist=0.1, jitter=0.1):\n",
        "    min_log = np.log10(min)\n",
        "    max_log = np.log10(max)\n",
        "    n = int((max_log - min_log) / dist)\n",
        "    return np.logspace(min_log, max_log, n) * np.random.uniform(1-jitter, 1+jitter, size=n)\n",
        "\n",
        "def lr_pairs(min_lr, max_lr, hp_values, dist=0.1, jitter=0.1, shuffle=True):\n",
        "    lrs = []\n",
        "    for val in hp_values:\n",
        "        lrs.extend((val, lr) for lr in lr_sample(min_lr, max_lr, dist=dist, jitter=jitter))\n",
        "    if shuffle:\n",
        "        random.shuffle(lrs)\n",
        "    return lrs\n",
        "\n",
        "def prepare_trials(study, min_lr, max_lr, hp_name, hp_values, dist=0.1, jitter=0.1, shuffle=True):\n",
        "    # ------ prime with parameters\n",
        "\n",
        "    for val, lr in lr_pairs(min_lr, max_lr, hp_values, dist=0.1, jitter=0.1, shuffle=shuffle):\n",
        "        study.enqueue_trial(\n",
        "            {\n",
        "                    \"learning_rate\": lr,\n",
        "                    hp_name: val,\n",
        "                }\n",
        "            )"
      ],
      "metadata": {
        "id": "6fCQMfY2j3hg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from optuna.storages import RDBStorage\n",
        "\n",
        "db_path = \"/content/gdrive/My Drive/Colab Notebooks/nlp-classification/\"\n",
        "db_name = \"10kgnad_optuna\"\n",
        "# automatically change the state of a stale trial to TrialState.FAIL from TrialState.RUNNING\n",
        "storage = RDBStorage(url=f\"sqlite:///{db_path}{db_name}.db\", heartbeat_interval=60, grace_period=120)\n",
        "\n",
        "space = Space(\n",
        "    model = \"deepset/gbert-base\",\n",
        "    train_size = [0.25, 0.5, 1.0],\n",
        "    data_collator = False,\n",
        "    max_seq_length = 128,\n",
        "    batch_size = [64],\n",
        "    num_train_epochs = 2,\n",
        "    learning_rate = log_uniform(5e-6, 5e-4),\n",
        "    weight_decay = log_uniform(1e-3, 1e-2),\n",
        ")\n",
        "\n",
        "# TODO define optimization metrics\n",
        "\n",
        "study_name = space.params[\"model\"] + \"_ds25-100_bs64_ep2_len128\"\n",
        "print(\"STUDY\", study_name)\n",
        "\n",
        "# multi objective study\n",
        "# https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/002_multi_objective.html#sphx-glr-tutorial-20-recipes-002-multi-objective-py\n",
        "study = optuna.create_study(study_name=study_name,\n",
        "                            directions=[\"minimize\", \"maximize\"],\n",
        "                            storage=storage,\n",
        "                            load_if_exists=True,)\n",
        "\n",
        "# prime trials\n",
        "lr = space.params[\"learning_rate\"]\n",
        "key = \"train_size\"\n",
        "# prepare_trials(study, lr.min, lr.max, key, space.params[key], dist=0.05, jitter=0.05)\n",
        "\n",
        "study.optimize(objective, n_trials=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CW1ts2ZGw6Ho",
        "outputId": "fa7dd5da-39b9-42e1-8b87-b156d209f98e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-30 23:00:06,780]\u001b[0m Using an existing study with name 'deepset/gbert-base_ds25-100_bs64_ep2_len128' instead of creating a new one.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STUDY deepset/gbert-base_ds25-100_bs64_ep2_len128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:258: ExperimentalWarning: enqueue_trial is experimental (supported from v1.2.0). The interface can change in the future.\n",
            "/usr/local/lib/python3.7/dist-packages/optuna/study/study.py:857: ExperimentalWarning: create_trial is experimental (supported from v2.0.0). The interface can change in the future.\n",
            "  create_trial(state=TrialState.WAITING, system_attrs={\"fixed_params\": params})\n",
            "/usr/local/lib/python3.7/dist-packages/optuna/study/study.py:857: ExperimentalWarning: add_trial is experimental (supported from v2.0.0). The interface can change in the future.\n",
            "  create_trial(state=TrialState.WAITING, system_attrs={\"fixed_params\": params})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'deepset/gbert-base', 'train_size': 0.25, 'data_collator': False, 'max_seq_length': 128, 'batch_size': 64, 'num_train_epochs': 2, 'learning_rate': 1.2276295732827837e-05, 'weight_decay': 0.0027240014343563727}\n",
            "USING CACHED DATASET for 0.25/128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "fixed params: [('num_train_epochs', 2), ('batch_size', 64)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=21.0, min_trials=700\n",
            "params: {'model': 'deepset/gbert-base', 'train_size': 0.25, 'data_collator': False, 'max_seq_length': 128, 'batch_size': 64, 'num_train_epochs': 2, 'learning_rate': 1.2276295732827837e-05, 'weight_decay': 0.0027240014343563727}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='72' max='72' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [72/72 02:41, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.152000</td>\n",
              "      <td>2.028398</td>\n",
              "      <td>0.377432</td>\n",
              "      <td>0.243727</td>\n",
              "      <td>0.281115</td>\n",
              "      <td>0.279799</td>\n",
              "      <td>0.278619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.965900</td>\n",
              "      <td>1.891984</td>\n",
              "      <td>0.450389</td>\n",
              "      <td>0.286981</td>\n",
              "      <td>0.304725</td>\n",
              "      <td>0.332783</td>\n",
              "      <td>0.368156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.868700</td>\n",
              "      <td>1.744822</td>\n",
              "      <td>0.536965</td>\n",
              "      <td>0.358967</td>\n",
              "      <td>0.359316</td>\n",
              "      <td>0.413068</td>\n",
              "      <td>0.467602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.772400</td>\n",
              "      <td>1.616001</td>\n",
              "      <td>0.562257</td>\n",
              "      <td>0.396550</td>\n",
              "      <td>0.426373</td>\n",
              "      <td>0.439915</td>\n",
              "      <td>0.498063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.610900</td>\n",
              "      <td>1.509369</td>\n",
              "      <td>0.574903</td>\n",
              "      <td>0.416915</td>\n",
              "      <td>0.541455</td>\n",
              "      <td>0.454308</td>\n",
              "      <td>0.514970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.505300</td>\n",
              "      <td>1.398851</td>\n",
              "      <td>0.626459</td>\n",
              "      <td>0.465426</td>\n",
              "      <td>0.568618</td>\n",
              "      <td>0.497825</td>\n",
              "      <td>0.572091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>1.389800</td>\n",
              "      <td>1.325756</td>\n",
              "      <td>0.646887</td>\n",
              "      <td>0.494456</td>\n",
              "      <td>0.691836</td>\n",
              "      <td>0.518642</td>\n",
              "      <td>0.595966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>1.354900</td>\n",
              "      <td>1.272181</td>\n",
              "      <td>0.658560</td>\n",
              "      <td>0.516576</td>\n",
              "      <td>0.694346</td>\n",
              "      <td>0.534474</td>\n",
              "      <td>0.609345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>1.312400</td>\n",
              "      <td>1.233800</td>\n",
              "      <td>0.675097</td>\n",
              "      <td>0.549253</td>\n",
              "      <td>0.810627</td>\n",
              "      <td>0.557734</td>\n",
              "      <td>0.628266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.267800</td>\n",
              "      <td>1.216491</td>\n",
              "      <td>0.679961</td>\n",
              "      <td>0.563215</td>\n",
              "      <td>0.799230</td>\n",
              "      <td>0.566810</td>\n",
              "      <td>0.633656</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-30 23:03:04,601]\u001b[0m Trial 3 finished with values: [1.2164912223815918, 0.5632148867503006] and parameters: {'model': 'deepset/gbert-base', 'train_size': 0.25, 'data_collator': False, 'max_seq_length': 128, 'batch_size': 64, 'num_train_epochs': 2, 'learning_rate': 1.2276295732827837e-05, 'weight_decay': 0.0027240014343563727}. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'deepset/gbert-base', 'train_size': 0.25, 'data_collator': False, 'max_seq_length': 128, 'batch_size': 64, 'num_train_epochs': 2, 'learning_rate': 4.613658538714951e-05, 'weight_decay': 0.008837330426901786}\n",
            "USING CACHED DATASET for 0.25/128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "fixed params: [('num_train_epochs', 2), ('batch_size', 64)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=21.0, min_trials=700\n",
            "params: {'model': 'deepset/gbert-base', 'train_size': 0.25, 'data_collator': False, 'max_seq_length': 128, 'batch_size': 64, 'num_train_epochs': 2, 'learning_rate': 4.613658538714951e-05, 'weight_decay': 0.008837330426901786}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='72' max='72' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [72/72 02:41, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.067800</td>\n",
              "      <td>1.821143</td>\n",
              "      <td>0.492218</td>\n",
              "      <td>0.306595</td>\n",
              "      <td>0.320390</td>\n",
              "      <td>0.376718</td>\n",
              "      <td>0.425593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.603700</td>\n",
              "      <td>1.283495</td>\n",
              "      <td>0.678016</td>\n",
              "      <td>0.547608</td>\n",
              "      <td>0.599946</td>\n",
              "      <td>0.559153</td>\n",
              "      <td>0.636958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.192200</td>\n",
              "      <td>0.900568</td>\n",
              "      <td>0.793774</td>\n",
              "      <td>0.775325</td>\n",
              "      <td>0.832911</td>\n",
              "      <td>0.754353</td>\n",
              "      <td>0.763913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.904100</td>\n",
              "      <td>0.708131</td>\n",
              "      <td>0.807393</td>\n",
              "      <td>0.796384</td>\n",
              "      <td>0.813955</td>\n",
              "      <td>0.796824</td>\n",
              "      <td>0.781653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.718000</td>\n",
              "      <td>0.622202</td>\n",
              "      <td>0.829767</td>\n",
              "      <td>0.820943</td>\n",
              "      <td>0.846130</td>\n",
              "      <td>0.807739</td>\n",
              "      <td>0.805020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.584100</td>\n",
              "      <td>0.563245</td>\n",
              "      <td>0.843385</td>\n",
              "      <td>0.835451</td>\n",
              "      <td>0.855622</td>\n",
              "      <td>0.824631</td>\n",
              "      <td>0.820557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.500200</td>\n",
              "      <td>0.544211</td>\n",
              "      <td>0.842412</td>\n",
              "      <td>0.834631</td>\n",
              "      <td>0.858554</td>\n",
              "      <td>0.822545</td>\n",
              "      <td>0.819649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.470100</td>\n",
              "      <td>0.513218</td>\n",
              "      <td>0.857004</td>\n",
              "      <td>0.849946</td>\n",
              "      <td>0.859465</td>\n",
              "      <td>0.845100</td>\n",
              "      <td>0.836257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.444600</td>\n",
              "      <td>0.493780</td>\n",
              "      <td>0.859922</td>\n",
              "      <td>0.853480</td>\n",
              "      <td>0.860051</td>\n",
              "      <td>0.849334</td>\n",
              "      <td>0.839460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.460100</td>\n",
              "      <td>0.487019</td>\n",
              "      <td>0.856031</td>\n",
              "      <td>0.849585</td>\n",
              "      <td>0.854713</td>\n",
              "      <td>0.846588</td>\n",
              "      <td>0.835013</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-30 23:06:00,741]\u001b[0m Trial 4 finished with values: [0.4870188534259796, 0.8495850771306106] and parameters: {'model': 'deepset/gbert-base', 'train_size': 0.25, 'data_collator': False, 'max_seq_length': 128, 'batch_size': 64, 'num_train_epochs': 2, 'learning_rate': 4.613658538714951e-05, 'weight_decay': 0.008837330426901786}. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'deepset/gbert-base', 'train_size': 1.0, 'data_collator': False, 'max_seq_length': 128, 'batch_size': 64, 'num_train_epochs': 2, 'learning_rate': 0.00011006711630444298, 'weight_decay': 0.0033087965136340096}\n",
            "USING CACHED DATASET for 1.0/128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "fixed params: [('num_train_epochs', 2), ('batch_size', 64)]\n",
            "objectives: ['eval_loss', 'eval_f1'], directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MAXIMIZE: 2>], warmup=84.0, min_trials=700\n",
            "params: {'model': 'deepset/gbert-base', 'train_size': 1.0, 'data_collator': False, 'max_seq_length': 128, 'batch_size': 64, 'num_train_epochs': 2, 'learning_rate': 0.00011006711630444298, 'weight_decay': 0.0033087965136340096}\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='195' max='288' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [195/288 03:47 < 01:49, 0.85 it/s, Epoch 1.35/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.068000</td>\n",
              "      <td>0.552434</td>\n",
              "      <td>0.832685</td>\n",
              "      <td>0.822455</td>\n",
              "      <td>0.844592</td>\n",
              "      <td>0.815563</td>\n",
              "      <td>0.810710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.496300</td>\n",
              "      <td>0.446340</td>\n",
              "      <td>0.860895</td>\n",
              "      <td>0.857209</td>\n",
              "      <td>0.862571</td>\n",
              "      <td>0.859055</td>\n",
              "      <td>0.842293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.453000</td>\n",
              "      <td>0.393846</td>\n",
              "      <td>0.873541</td>\n",
              "      <td>0.869858</td>\n",
              "      <td>0.869688</td>\n",
              "      <td>0.871670</td>\n",
              "      <td>0.855276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.416800</td>\n",
              "      <td>0.413256</td>\n",
              "      <td>0.869650</td>\n",
              "      <td>0.865183</td>\n",
              "      <td>0.866581</td>\n",
              "      <td>0.869679</td>\n",
              "      <td>0.851737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.447000</td>\n",
              "      <td>0.396629</td>\n",
              "      <td>0.880350</td>\n",
              "      <td>0.875680</td>\n",
              "      <td>0.876563</td>\n",
              "      <td>0.876831</td>\n",
              "      <td>0.863092</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>0.268500</td>\n",
              "      <td>0.366865</td>\n",
              "      <td>0.883268</td>\n",
              "      <td>0.879788</td>\n",
              "      <td>0.878630</td>\n",
              "      <td>0.883650</td>\n",
              "      <td>0.866637</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "21c2_10kGNAD_huggingface_optuna_config.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNQxu7TCa0zw5i7W4FHi28c",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2f44a82be16843898f3c900d46444283": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_78c34095c4a7444cbb13c82fe40c43cf",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_896fec95fec1463e8f04549e3f2896b9",
              "IPY_MODEL_48bb699727e9424b93d9b9b123d3faa1",
              "IPY_MODEL_9f6de51a6bde47ca94e0e3508e40a7a4"
            ]
          }
        },
        "78c34095c4a7444cbb13c82fe40c43cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "896fec95fec1463e8f04549e3f2896b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_00b892279f274a648322e5ec06d4cff1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_43f99edc070e4f9984176f1b6b846894"
          }
        },
        "48bb699727e9424b93d9b9b123d3faa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ef0f1a2945754ec6986756e5f145d708",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9d6d41a6ede34027a06d8561cd62c0a7"
          }
        },
        "9f6de51a6bde47ca94e0e3508e40a7a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a17bdc26ac854fc98a07b3fb5dc270c1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2/2 [00:00&lt;00:00,  6.86it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_06cc5207581741d39baa9b6624b451b6"
          }
        },
        "00b892279f274a648322e5ec06d4cff1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "43f99edc070e4f9984176f1b6b846894": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ef0f1a2945754ec6986756e5f145d708": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9d6d41a6ede34027a06d8561cd62c0a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a17bdc26ac854fc98a07b3fb5dc270c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "06cc5207581741d39baa9b6624b451b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}