{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ece3d79a-c209-43fb-be99-0ade934dec2f",
   "metadata": {
    "id": "AsTaSRJekM0L"
   },
   "source": [
    "## Prepare Dataset\n",
    "\n",
    "Using [10k German News Articles Dataset](https://tblock.github.io/10kGNAD/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fe88710-3f3e-482c-94f6-ff24ec101835",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26fe6bc6-3c81-465b-a194-93253a34537c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91b981d8-126c-473e-a47d-50c27d8b6471",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class TextDatasetWithLabels:\n",
    "    \"\"\"A data container that simplifies data preprocessing for NLP models.\"\"\"\n",
    "    \n",
    "    _datasets = {}\n",
    "    \n",
    "    def __init__(self, train: pd.DataFrame, dev: pd.DataFrame = None, test: pd.DataFrame = None, text_column: str= None, label_column: str=None):\n",
    "        if train is None:\n",
    "            raise Exception(\"need at least a training set!\")\n",
    "        self._datasets[\"train\"] = train\n",
    "        self._datasets[\"dev\"] = dev\n",
    "        self._datasets[\"test\"] = test\n",
    "        self._text_column = text_column\n",
    "        self._label_column = label_column\n",
    "        # TODO check that text_column and label_column are present in datasets\n",
    "    \n",
    "    @classmethod\n",
    "    def read_csv(cls, train, dev=None, test=None, sep=\",\", quotechar='\"', columns: list=None, text_column: str=\"text\", label_column: str=\"labels\"):\n",
    "        \"\"\"Read data from csv files.\"\"\"\n",
    "        train = pd.read_csv(train, sep=sep, quotechar=quotechar, names=columns)\n",
    "        if dev is not None:\n",
    "            dev = pd.read_csv(dev, sep=sep, quotechar=quotechar, names=columns)\n",
    "        if test is not None:\n",
    "            test = pd.read_csv(test, sep=sep, quotechar=quotechar, names=columns)\n",
    "        return TextDatasetWithLabels(train, dev, test, text_column, label_column)\n",
    "    \n",
    "    def to_csv(self, train_file, dev_file=None, test_file=None, label_first=True, header=True):\n",
    "        columns = [self._text_column, self._label_column]\n",
    "        if label_first:\n",
    "            columns.reverse()\n",
    "        \n",
    "        if train_file is not None:\n",
    "            self._datasets[\"train\"][columns].to_csv(train_file, header=header, index=False)\n",
    "        if dev_file is not None:\n",
    "            self._datasets[\"dev\"][columns].to_csv(dev_file, header=header, index=False)\n",
    "        if test_file is not None:\n",
    "            self._datasets[\"test\"][columns].to_csv(test_file, header=header, index=False)\n",
    "            \n",
    "    def to_single_csv(self, file, ml_use=[\"training\", \"validation\", \"test\"], mluse_first=True, label_first=True, header=True):\n",
    "        \"\"\"Concatenate datasets in single file and mark them properly in extra column 'ml_use'\"\"\"\n",
    "        columns = [self._text_column, self._label_column]\n",
    "        if label_first:\n",
    "            columns.reverse()\n",
    "        if mluse_first:\n",
    "            columns = [\"ml_use\"] + columns\n",
    "        else:\n",
    "            columns.append(\"ml_use\")\n",
    "        \n",
    "        sets = []\n",
    "        sets.append(self._datasets[\"train\"].assign(ml_use=ml_use[0]))\n",
    "        if self._datasets[\"dev\"] is not None:\n",
    "            sets.append(self._datasets[\"dev\"].assign(ml_use=ml_use[1]))\n",
    "        if self._datasets[\"test\"] is not None:\n",
    "            sets.append(self._datasets[\"test\"].assign(ml_use=ml_use[2]))\n",
    "        pd.concat(sets).reset_index(drop=True)[columns].to_csv(file, header=header, index=False)\n",
    "    \n",
    "    def info(self):\n",
    "        \"\"\"Prints basic information about the data.\"\"\"\n",
    "        dfs = [(name, self._datasets[name]) for name in [\"train\", \"dev\", \"test\"]]\n",
    "        counts = pd.DataFrame([(name, 0 if df is None else df.shape[0]) for name, df in dfs], columns=[\"name\", \"count\"])\n",
    "        total = counts[\"count\"].sum()\n",
    "        for i, (name, count) in counts.iterrows():\n",
    "            percent = '' if count == 0 else f\"({count/total:.1%})\"\n",
    "            count = 'n/a' if count == 0 else f\"{count:,}\"\n",
    "            print(f\"{name:<5}: {count:>8} {percent:>5}\")\n",
    "    \n",
    "    def clean(self, keep_testdata=True):\n",
    "        \"\"\"Check for duplicates.\"\"\"\n",
    "        \n",
    "        hashes = {}\n",
    "        remove = []\n",
    "        \n",
    "        for name in [\"train\", \"dev\", \"test\"]:\n",
    "            df = self._datasets[name]\n",
    "            if df is None:\n",
    "                continue\n",
    "            for idx, row in df.iterrows():\n",
    "                hash = hashlib.md5(row[self._text_column].encode('utf-8')).hexdigest()\n",
    "                if hash in hashes:\n",
    "                    prev_name, prev_idx = hashes.get(hash)\n",
    "                    prev_label = self._datasets[prev_name][self._label_column][prev_idx]\n",
    "                    label = self._datasets[name][self._label_column][idx]\n",
    "                    print(f\"found duplicate ({prev_name}, {prev_idx}, {prev_label}) - ({name}, {idx}, {label})\")\n",
    "                    if prev_label != label:\n",
    "                        print(\"same text but different labels - removing both\")\n",
    "                        remove.append((prev_name, prev_idx))\n",
    "                        remove.append((name, idx))\n",
    "                        hashes.pop(hash)\n",
    "                    elif prev_name != name and name == \"test\" and keep_testdata:\n",
    "                        print(\"duplicates in different datasets - keeping test data, removing other\")\n",
    "                        remove.append((prev_name, prev_idx))\n",
    "                        hashes[hash] = (name, idx)\n",
    "                    else:\n",
    "                        print(\"duplicate entries - removing later one\")\n",
    "                        remove.append((name, idx))\n",
    "                else:\n",
    "                    hashes[hash] = (name, idx)\n",
    "        \n",
    "        print(\"removing \", remove)\n",
    "        remove_df = pd.DataFrame(remove, columns=[\"name\", \"id\"])\n",
    "        for name in remove_df.name.unique():\n",
    "        #     print(name, remove_df[lambda x: x.name == name].id.values)\n",
    "            self._datasets[name] = self._datasets[name].drop(remove_df[lambda x: x.name == name].id.values)\n",
    "    \n",
    "    def create_dev_set(self, random_state: int = 42, ratio=\"auto\"):\n",
    "        \n",
    "        if self._datasets[\"dev\"] is not None:\n",
    "            print(\"dev set already exists!\")\n",
    "            return\n",
    "\n",
    "        label_col = self._label_column\n",
    "        X = self._datasets[\"train\"].drop(label_col, axis=1)\n",
    "        y = self._datasets[\"train\"][label_col]\n",
    "        \n",
    "        if ratio == \"auto\":\n",
    "            if self._datasets[\"test\"] is not None:\n",
    "                # make same size as test dataset\n",
    "                ratio = self._datasets[\"test\"].shape[0] / X.shape[0]\n",
    "            else:\n",
    "                ratio = 0.1\n",
    "        \n",
    "        X_train, X_dev, y_train, y_dev = train_test_split(X, y, stratify=y, test_size=ratio, random_state=random_state)\n",
    "        \n",
    "        column_order = self._datasets[\"train\"].columns\n",
    "        self._datasets[\"train\"] = pd.concat([X_train, y_train], axis=1)[column_order]\n",
    "        self._datasets[\"dev\"] = pd.concat([X_dev, y_dev], axis=1)[column_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d41db6b-23d4-4ad0-926d-6d0784a55eaf",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "873c5541-9f47-45d6-bf2e-3a6ae699c896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:    9,245 (90.0%)\n",
      "dev  :      n/a      \n",
      "test :    1,028 (10.0%)\n"
     ]
    }
   ],
   "source": [
    "text_ds = TextDatasetWithLabels.read_csv(train=\"https://github.com/tblock/10kGNAD/blob/master/train.csv?raw=true\",\n",
    "                                         test=\"https://github.com/tblock/10kGNAD/blob/master/test.csv?raw=true\",\n",
    "                                         sep=\";\", quotechar=\"'\",\n",
    "                                         columns=[\"labels\", \"text\"]\n",
    "                                        )\n",
    "text_ds.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e857e09-df28-4c80-a9e2-f33ca7146e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found duplicate (train, 1187, Panorama) - (train, 6411, International)\n",
      "same text but different labels - removing both\n",
      "found duplicate (train, 2785, Inland) - (test, 728, Inland)\n",
      "duplicates in different datasets - keeping test data, removing other\n",
      "removing  [('train', 1187), ('train', 6411), ('train', 2785)]\n"
     ]
    }
   ],
   "source": [
    "text_ds.clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8fe091d-ef04-4a23-9893-da093c7ac507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:    9,242 (90.0%)\n",
      "dev  :      n/a      \n",
      "test :    1,028 (10.0%)\n"
     ]
    }
   ],
   "source": [
    "text_ds.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6777f98d-6337-4aab-9e36-79b588a33702",
   "metadata": {},
   "source": [
    "## Save Training and Test Data for AWS Comprehend\n",
    "\n",
    "* AWS Comprehend does not support a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aa57d69-6a37-4f2e-a897-0f0bd1a8220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train and test set for AWS Comprehend (label, text)\n",
    "text_ds.to_csv(train_file=\"data/10kgnad_aws_comprehend_train.csv\",\n",
    "               test_file=\"data/10kgnad_aws_comprehend_test.csv\",\n",
    "               label_first=True, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01665fbb-b5e3-4cfd-a7fb-c637f3c9c8db",
   "metadata": {},
   "source": [
    "## Create a Dev Set (Validation Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50b2db1d-2cc8-45ea-8fb7-cf9e6237b300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:    8,214 (80.0%)\n",
      "dev  :    1,028 (10.0%)\n",
      "test :    1,028 (10.0%)\n"
     ]
    }
   ],
   "source": [
    "text_ds.create_dev_set()\n",
    "text_ds.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed51b01-f4cc-4396-88d8-462f1db2065e",
   "metadata": {},
   "source": [
    "## Save Training, Dev and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d327bb06-a8bd-40cd-ba7a-dd87140c4447",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ds.to_csv(train_file=\"data/10kgnad_train.csv\",\n",
    "               dev_file=\"data/10kgnad_valid.csv\",\n",
    "               test_file=\"data/10kgnad_test.csv\",\n",
    "               label_first=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36942b6c-ddcc-4c12-bbff-296be10a2893",
   "metadata": {},
   "source": [
    "## Save combined Training, Dev and Test Set for GCP Auto ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a072299b-8853-44e3-a2d6-9edf0f34b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ds.to_single_csv(\"data/10kgnad_gcp_auto_ml.csv\", mluse_first=True, label_first=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
