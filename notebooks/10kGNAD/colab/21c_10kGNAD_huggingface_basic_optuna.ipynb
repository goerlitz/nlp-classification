{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goerlitz/nlp-classification/blob/main/notebooks/10kGNAD/colab/21c_10kGNAD_huggingface_basic_optuna.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrveYRYD2E9-"
      },
      "source": [
        "# Hyperparameter Optimization with HuggingFace Transformers\n",
        "\n",
        "Adapted from https://huggingface.co/docs/transformers/custom_datasets#sequence-classification-with-imdb-reviews\n",
        "\n",
        "Things we need\n",
        "* a tokenizer\n",
        "* tokenized input data\n",
        "* a pretrained model\n",
        "* evaluation metrics\n",
        "* training parameters\n",
        "* a Trainer instance\n",
        "\n",
        "Notes\n",
        "* [class labels can be included in the model config](https://github.com/huggingface/transformers/pull/2945#issuecomment-781986506) (a bit hacky)\n",
        "* [fp16 is disabled on tesla P100 GPU in pytorch](https://discuss.pytorch.org/t/cnn-fp16-slower-than-fp32-on-tesla-p100/12146)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JH5VyX6w9AIM"
      },
      "source": [
        "## Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "L2BZ9NGbmTkF"
      },
      "outputs": [],
      "source": [
        "checkpoint = \"distilbert-base-german-cased\"\n",
        "# checkpoint = \"deepset/gbert-base\"\n",
        "# checkpoint = \"deepset/gelectra-base\"\n",
        "\n",
        "project_name = f'10kgnad_hf__{checkpoint.replace(\"/\", \"_\")}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKqjLlGpWdsi"
      },
      "source": [
        "### Connect Google Drive\n",
        "\n",
        "Will be used to save results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YXxAbtZWcxA",
        "outputId": "8a8e5c72-6c08-4343-e13c-1e4a3a6ed5fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "M3n8HqXwEyK6"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# define model path\n",
        "root_path = Path('/content/gdrive/My Drive/')\n",
        "base_path = root_path / 'Colab Notebooks/nlp-classification/'\n",
        "model_path = base_path / 'models'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5F2oddvs2rNv"
      },
      "source": [
        "## Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVWRNvQf1cnR",
        "outputId": "b0290709-7a5d-4a33-db44-bcf8dc39b338"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Jan  7 17:14:45 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwlw_kI3nHLW"
      },
      "source": [
        "### Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QhKj625lBso",
        "outputId": "cbfce7b2-10b4-4951-c12b-b444aa78c404"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "optuna==2.10.0\n",
            "transformers==4.15.0\n",
            "torch @ https://download.pytorch.org/whl/cu111/torch-1.10.0%2Bcu111-cp37-cp37m-linux_x86_64.whl\n",
            "torchaudio @ https://download.pytorch.org/whl/cu111/torchaudio-0.10.0%2Bcu111-cp37-cp37m-linux_x86_64.whl\n",
            "torchsummary==1.5.1\n",
            "torchtext==0.11.0\n",
            "torchvision @ https://download.pytorch.org/whl/cu111/torchvision-0.11.1%2Bcu111-cp37-cp37m-linux_x86_64.whl\n",
            "CPU times: user 113 ms, sys: 54.1 ms, total: 167 ms\n",
            "Wall time: 11.1 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "!pip install -q -U transformers datasets >/dev/null\n",
        "!pip install -q -U optuna >/dev/null\n",
        "\n",
        "# check installed version\n",
        "!pip freeze | grep optuna        # optuna==2.10.0\n",
        "!pip freeze | grep transformers  # transformers==4.15.0\n",
        "!pip freeze | grep torch         # torch==1.10.0+cu111"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vbpLzC7DRoR5"
      },
      "outputs": [],
      "source": [
        "from transformers import logging\n",
        "\n",
        "# hide progress bar when downloading tokenizer and model (a workaround!)\n",
        "logging.get_verbosity = lambda : logging.NOTSET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64Q8bFMM7UYg"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "9afad0219a1d41b994bf564665992083",
            "73071c64cc0045a6a0f05095ba202b61",
            "994ab1930d3c4724b095fc6676a7bd98",
            "7204e2676cb6489dbb7c72305c673d5e",
            "1441b5271ca5428db602efadfe865451",
            "55662134eda24351b29d0b4c482e3192",
            "5cd8ed4c341844cfadc7542d9ca4916e",
            "70d1321199444dfbb2d546eb29305ed4",
            "553ffaaf454e4be994b4ed4144c58916",
            "933231612ec641caaaacf053612ca979",
            "86772280985a488d997c4e7e24be0e48"
          ]
        },
        "id": "4aYzOljo7W9X",
        "outputId": "5c8d9f54-5704-4789-9bb8-371ed75b2a2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/gnad10/3a8445be65795ad88270af4d797034c3d99f70f8352ca658c586faf1cf960881 (last modified on Thu Jan  6 23:08:20 2022) since it couldn't be found locally at gnad10., or remotely on the Hugging Face Hub.\n",
            "Using custom data configuration default\n",
            "Reusing dataset gnad10 (/root/.cache/huggingface/datasets/gnad10/default/1.1.0/3a8445be65795ad88270af4d797034c3d99f70f8352ca658c586faf1cf960881)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9afad0219a1d41b994bf564665992083",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "gnad10k = load_dataset(\"gnad10\")\n",
        "label_names = gnad10k[\"train\"].features[\"label\"].names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkcJyc-I8bLP",
        "outputId": "21b35b55-2187-46f1-901e-eae0b40cecc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 9245\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 1028\n",
            "    })\n",
            "})\n",
            "labels: ['Web', 'Panorama', 'International', 'Wirtschaft', 'Sport', 'Inland', 'Etat', 'Wissenschaft', 'Kultur']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': 4,\n",
              " 'text': '21-Jähriger fällt wohl bis Saisonende aus. Wien – Rapid muss wohl bis Saisonende auf Offensivspieler Thomas Murg verzichten. Der im Winter aus Ried gekommene 21-Jährige erlitt beim 0:4-Heimdebakel gegen Admira Wacker Mödling am Samstag einen Teilriss des Innenbandes im linken Knie, wie eine Magnetresonanz-Untersuchung am Donnerstag ergab. Murg erhielt eine Schiene, muss aber nicht operiert werden. Dennoch steht ihm eine mehrwöchige Pause bevor.'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "print(gnad10k)\n",
        "print(\"labels:\", label_names)\n",
        "gnad10k[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EUOwFbFPXCy"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "* Loading the same Tokenizer that was used with the pretrained model.\n",
        "* Define function to tokenize the text (with truncation to max input length of model.\n",
        "* Run the tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7AJ7SGjPaZ0",
        "outputId": "1d849984-b2a1-4fa1-b7d8-31ebb22fd046"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/gnad10/default/1.1.0/3a8445be65795ad88270af4d797034c3d99f70f8352ca658c586faf1cf960881/cache-5d66d7a004b32c63.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/gnad10/default/1.1.0/3a8445be65795ad88270af4d797034c3d99f70f8352ca658c586faf1cf960881/cache-1e7aaca04dbb52e2.arrow\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True)\n",
        "\n",
        "tokenized_gnad10k = gnad10k.map(preprocess_function, batched=True).remove_columns(\"text\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdmFxnHlSV6x"
      },
      "source": [
        "### Use Dynamic Padding\n",
        "\n",
        "Apply panding only on longest text in batch - this is more efficient than applying padding on the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DhEWeeg2SVCe"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eqe_fA8muv5T"
      },
      "source": [
        "## Model Setup\n",
        "\n",
        "We want to include the label names and save them together with the model.\n",
        "The only way to do this is to create a Config and put them in. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kHPXNqxCgyxQ"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
        "\n",
        "config = AutoConfig.from_pretrained(\n",
        "        checkpoint,\n",
        "        num_labels=len(label_names),\n",
        "        id2label={i: label for i, label in enumerate(label_names)},\n",
        "        label2id={label: i for i, label in enumerate(label_names)},\n",
        "        )\n",
        "\n",
        "def model_init(trial: optuna.Trial):\n",
        "    \"\"\"A function that instantiates the model to be used.\"\"\"\n",
        "    return AutoModelForSequenceClassification.from_pretrained(checkpoint, config=config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufbNYVSIxuhv"
      },
      "source": [
        "### Define Evaluation Metrics\n",
        "\n",
        "The funtion that computes the metrics needs to be passed to the Trainer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Hpj1PZYjxtqS"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, matthews_corrcoef\n",
        "import numpy as np\n",
        "from typing import Dict\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    \"\"\"The function that will be used to compute metrics at evaluation.\n",
        "    Must take a :class:`~transformers.EvalPrediction` and return a dictionary\n",
        "    string to metric values.\"\"\"\n",
        "    logits, labels = eval_preds\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        \"acc\": accuracy_score(labels, preds),\n",
        "        \"f1\": f1_score(labels, preds, average='macro'),\n",
        "        \"precision\": precision_score(labels, preds, average='macro'),\n",
        "        \"recall\": recall_score(labels, preds, average='macro'),\n",
        "        \"mcc\": matthews_corrcoef(labels, preds),\n",
        "        }\n",
        "\n",
        "\n",
        "# def objective(metrics: Dict[str, float]):\n",
        "#     \"\"\"A function computing the main optimization objective from the metrics\n",
        "#     returned by the :obj:`compute_metrics` method.\n",
        "#     To be used in :obj:`Trainer.hyperparameter_search`.\"\"\"\n",
        "#     return metrics[\"eval_loss\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9HHIeJrM3_I"
      },
      "source": [
        "## Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "3_9Al_rbBrla"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainerCallback\n",
        "from optuna.trial import TrialState\n",
        "from optuna.study._study_direction import StudyDirection\n",
        "import pandas as pd\n",
        "\n",
        "# https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/trainer_callback.py#L505\n",
        "\n",
        "\n",
        "\n",
        "class MultiObjectiveMedianPrunerCallback(TrainerCallback):\n",
        "    def __init__(self,\n",
        "                 objectives,\n",
        "                 n_startup_trials: int = 5,\n",
        "                 n_warmup_steps: int = 0,\n",
        "                 interval_steps: int = 1,\n",
        "                 n_min_trials: int = 1,):\n",
        "        self.ojectives = objectives\n",
        "        self._n_startup_trials = n_startup_trials\n",
        "        self._n_warmup_steps = n_warmup_steps\n",
        "        self._interval_steps = interval_steps\n",
        "        self._n_min_trials = n_min_trials\n",
        "    \n",
        "    def prune(self, study: \"optuna.study.Study\", trial: \"optuna.trial.FrozenTrial\") -> bool:\n",
        "        # get completed trials\n",
        "        complete_trials = study.get_trials(deepcopy=False,\n",
        "                                           states=[TrialState.COMPLETE])\n",
        "        # complete_trials = [t for t in all_trials\n",
        "        #                    if t.state == TrialState.COMPLETE]\n",
        "        n_trials = len(complete_trials)\n",
        "\n",
        "        step = trial.last_step\n",
        "        print(f\"prune? step={step}, complete_trials={n_trials}\")\n",
        "\n",
        "\n",
        "        return False\n",
        "        \n",
        "    \n",
        "    # def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
        "    #     print(f\"pruning check \")\n",
        "    #     # TODO: use set_user_attrs instead of report\n",
        "    #     self.trial.report(metrics[self.metric], step=state.global_step)\n",
        "\n",
        "\n",
        "# https://huggingface.co/docs/transformers/main_classes/callback#transformers.TrainerCallback\n",
        "\n",
        "class TrialLogAndPruningCallback(TrainerCallback):\n",
        "    \"\"\"Stores eval metrics at each evaluation step in the trial user attrs.\"\"\"\n",
        "    def __init__(self, trial: optuna.Trial, objectives=None, warmup_steps=0, min_trials=7):\n",
        "        self.trial = trial\n",
        "        if objectives == None:\n",
        "            self.objectives = [\"eval_loss\"]\n",
        "        else:\n",
        "            self.objectives = objectives\n",
        "        self._warmup_steps = warmup_steps\n",
        "        self._min_trials = min_trials\n",
        "\n",
        "    def _filter_trials(self, complete_trials):\n",
        "        \"\"\"Select only trials with same parameter values\"\"\"\n",
        "        keys = [\"num_train_epochs\", \"per_device_train_batch_size\"]\n",
        "        values = [self.trial.params[k] for k in keys]\n",
        "        return [t for t in complete_trials if values == [t.params[k] for k in keys]]\n",
        "\n",
        "    def _prune(self, step: int, metrics) -> bool:\n",
        "        \"\"\"Median Pruning on multiple objectives.\"\"\"\n",
        "        if step < self._warmup_steps:\n",
        "            return False\n",
        "\n",
        "        study = self.trial.study\n",
        "        complete_trials = study.get_trials(deepcopy=False,\n",
        "                                           states=[TrialState.COMPLETE])\n",
        "        # do not compare trials with different batch sizes and epochs\n",
        "        complete_trials = self._filter_trials(complete_trials)\n",
        "        n_trials = len(complete_trials)\n",
        "\n",
        "        if n_trials < self._min_trials:\n",
        "            return False\n",
        "\n",
        "        trial_metrics = []\n",
        "        for t in complete_trials:\n",
        "            if str(step) in t.user_attrs.keys():\n",
        "                trial_metrics.append(t.user_attrs[str(step)])\n",
        "        n_metrics = len(trial_metrics)\n",
        "\n",
        "        median = pd.DataFrame(trial_metrics).median()\n",
        "\n",
        "        directions = study.directions\n",
        "        prune_state = []\n",
        "        for i, o in enumerate(self.objectives):\n",
        "            if directions[i] == StudyDirection.MAXIMIZE:\n",
        "                prune_state.append(metrics[o] <= median[o])\n",
        "            else:\n",
        "                prune_state.append(metrics[o] > median[o])\n",
        "        \n",
        "        met = \",\".join([f\"{m}={metrics[m]:.4}/{median[m]:.4}\" for m in self.objectives])\n",
        "        print(f\"prune? step={step}, warmup={self._warmup_steps}, complete_trials={n_trials}, metrics={n_metrics} -> {met}; {prune_state}\")\n",
        "\n",
        "        # all metrics must be marked for pruning\n",
        "        # return all(prune_state)\n",
        "        return False\n",
        "    \n",
        "    def on_evaluate(self, args, state, control, lr_scheduler, metrics, **kwargs):\n",
        "        step = state.global_step\n",
        "        values = {**metrics, \"lr\": lr_scheduler.get_last_lr()[-1]}\n",
        "        self.trial.set_user_attr(step, values)\n",
        "\n",
        "        # pruning\n",
        "        if self._prune(step, metrics):\n",
        "            print(f\"pruning trial at step {step}\")\n",
        "            # control.should_training_stop = True  # not needed\n",
        "            raise optuna.TrialPruned()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "YqjuJ-2tRMYQ"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "import shutil\n",
        "\n",
        "def hp_space(trial: optuna.Trial):\n",
        "    \"\"\"A function that defines the hyperparameter search space.\n",
        "    To be used in :obj:`Trainer.hyperparameter_search`.\"\"\"\n",
        "    return {\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 3e-5, 1e-4, log=True),  # distilbert\n",
        "        # \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-4, log=True),\n",
        "        # \"learning_rate\": trial.suggest_float(\"learning_rate\", 6e-5, 2e-4, log=True),  # electra\n",
        "        \"num_train_epochs\": trial.suggest_categorical(\"num_train_epochs\", [2, 3]),\n",
        "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [32]),\n",
        "        # \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-3, 1e-2, log=True),\n",
        "        \"weight_decay\": trial.suggest_categorical(\"weight_decay\", [1e-3, 0.0]),\n",
        "        # \"label_smoothing_factor\": trial.suggest_float(\"label_smoothing_factor\", 0.0, 0.1),\n",
        "    }\n",
        "\n",
        "best_model_dir = \"best_model_trainer\"\n",
        "\n",
        "def best_model_callback(study, trial):\n",
        "    \"\"\"Save the model from a best trial\"\"\"\n",
        "    for t in study.best_trials:\n",
        "        if t.number == trial.number:\n",
        "            print(\"This is a new besttrial\", trial.number)\n",
        "        \n",
        "            out_filename = model_path / f\"{project_name}_t{trial.number}\"\n",
        "            shutil.make_archive(out_filename, 'zip', f\"{project_name}/{best_model_dir}\")\n",
        "\n",
        "def objective(trial: optuna.Trial):\n",
        "\n",
        "    # get hyperparameters choice\n",
        "    hp = hp_space(trial)\n",
        "    lr = hp[\"learning_rate\"]\n",
        "    bs = hp[\"per_device_train_batch_size\"]\n",
        "    epochs = hp[\"num_train_epochs\"]\n",
        "    weight_decay = hp[\"weight_decay\"]\n",
        "    # label_smoothing_factor = hp[\"label_smoothing_factor\"]\n",
        "\n",
        "    eval_rounds_per_epoch = 5\n",
        "    eval_steps = gnad10k[\"train\"].num_rows / bs // eval_rounds_per_epoch\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=str(project_name),\n",
        "        report_to=[],\n",
        "        log_level=\"error\",\n",
        "        disable_tqdm=False,\n",
        "\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=eval_steps,\n",
        "        logging_steps=eval_steps,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=eval_steps,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        greater_is_better=False,\n",
        "\n",
        "        # hyperparameters\n",
        "        num_train_epochs=epochs,\n",
        "        learning_rate=lr,\n",
        "        per_device_train_batch_size=bs,\n",
        "        per_device_eval_batch_size=bs,\n",
        "        weight_decay=weight_decay,\n",
        "        # label_smoothing_factor=label_smoothing_factor,\n",
        "\n",
        "        # fp16=True,  # fp16 is disabled on Tesla P100 by pytorch\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model_init=model_init,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_gnad10k[\"train\"],\n",
        "        eval_dataset=tokenized_gnad10k[\"test\"],\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[TrialLogAndPruningCallback(trial, objectives=[\"eval_loss\", \"eval_f1\"], warmup_steps=eval_steps*4)]\n",
        "        # callbacks=[TrialPruningCallback(trial)]\n",
        "    )\n",
        "\n",
        "    # train model and save best model from evaluations\n",
        "    # needs 'load_best_model_at_end=True'\n",
        "    trainer.train()\n",
        "    trainer.save_model(f\"{project_name}/{best_model_dir}\")\n",
        "\n",
        "    result = trainer.evaluate(eval_dataset=tokenized_gnad10k[\"test\"])\n",
        "\n",
        "    # store eval metrics in trial\n",
        "    trial.set_user_attr(\"eval_result\", result)\n",
        "    \n",
        "    # return result[\"eval_loss\"]\n",
        "    return result[\"eval_loss\"], result[\"eval_f1\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9knsvQTUxjWi"
      },
      "source": [
        "## Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "SNx_dukVxizp",
        "outputId": "aa7515f7-cc69-412a-cc6c-694dc0a3f626"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-07 21:20:33,957]\u001b[0m Using an existing study with name 'distilbert-base-german-cased_loss-f1_bs32_epoch23' instead of creating a new one.\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='414' max='578' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [414/578 07:29 < 02:58, 0.92 it/s, Epoch 1.43/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>1.246900</td>\n",
              "      <td>0.698054</td>\n",
              "      <td>0.792802</td>\n",
              "      <td>0.781168</td>\n",
              "      <td>0.815487</td>\n",
              "      <td>0.772654</td>\n",
              "      <td>0.764949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>0.654000</td>\n",
              "      <td>0.505349</td>\n",
              "      <td>0.838521</td>\n",
              "      <td>0.836773</td>\n",
              "      <td>0.856238</td>\n",
              "      <td>0.824729</td>\n",
              "      <td>0.815229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>0.515800</td>\n",
              "      <td>0.436459</td>\n",
              "      <td>0.860895</td>\n",
              "      <td>0.860254</td>\n",
              "      <td>0.862906</td>\n",
              "      <td>0.863638</td>\n",
              "      <td>0.841829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>228</td>\n",
              "      <td>0.467300</td>\n",
              "      <td>0.411757</td>\n",
              "      <td>0.862840</td>\n",
              "      <td>0.863616</td>\n",
              "      <td>0.859840</td>\n",
              "      <td>0.871995</td>\n",
              "      <td>0.843409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>285</td>\n",
              "      <td>0.410300</td>\n",
              "      <td>0.403604</td>\n",
              "      <td>0.867704</td>\n",
              "      <td>0.866437</td>\n",
              "      <td>0.875247</td>\n",
              "      <td>0.865410</td>\n",
              "      <td>0.849197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>342</td>\n",
              "      <td>0.306600</td>\n",
              "      <td>0.380824</td>\n",
              "      <td>0.882296</td>\n",
              "      <td>0.880217</td>\n",
              "      <td>0.887779</td>\n",
              "      <td>0.875088</td>\n",
              "      <td>0.865543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>399</td>\n",
              "      <td>0.307900</td>\n",
              "      <td>0.341928</td>\n",
              "      <td>0.892996</td>\n",
              "      <td>0.889060</td>\n",
              "      <td>0.892112</td>\n",
              "      <td>0.887245</td>\n",
              "      <td>0.877482</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prune? step=228, warmup=228.0, complete_trials=95, metrics=95 -> eval_loss=0.4118/0.4225,eval_f1=0.8636/0.8662; [False, True]\n",
            "prune? step=285, warmup=228.0, complete_trials=95, metrics=95 -> eval_loss=0.4036/0.4049,eval_f1=0.8664/0.8689; [False, True]\n",
            "prune? step=342, warmup=228.0, complete_trials=95, metrics=95 -> eval_loss=0.3808/0.3764,eval_f1=0.8802/0.8764; [True, False]\n",
            "prune? step=399, warmup=228.0, complete_trials=95, metrics=95 -> eval_loss=0.3419/0.3637,eval_f1=0.8891/0.8798; [False, False]\n"
          ]
        }
      ],
      "source": [
        "db_path = \"/content/gdrive/My Drive/Colab Notebooks/nlp-classification/\"\n",
        "db_name = \"10kgnad_optuna\"\n",
        "# study_name = checkpoint + \"_multi_epoch234\"\n",
        "study_name = checkpoint + \"_loss-f1_bs32_epoch23\"\n",
        "\n",
        "# multi objective study\n",
        "# https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/002_multi_objective.html#sphx-glr-tutorial-20-recipes-002-multi-objective-py\n",
        "study = optuna.create_study(study_name=study_name,\n",
        "                            directions=[\"minimize\", \"maximize\"],\n",
        "                            # pruner=MultiObjectiveMedianPrunerCallback([\"eval_loss\", \"eval_f1\"]),\n",
        "                            storage=f\"sqlite:///{db_path}{db_name}.db\",\n",
        "                            load_if_exists=True,)\n",
        "\n",
        "# give some hyperparameters that are presumably good\n",
        "# study.enqueue_trial(\n",
        "#     {\n",
        "#         \"learning_rate\": 8e-5,\n",
        "#         \"weight_decay\": 1e-3,\n",
        "#         \"label_smoothing_factor\": 0.0,\n",
        "#     }\n",
        "# )\n",
        "# study.enqueue_trial(\n",
        "#     {\n",
        "#         \"learning_rate\": 7e-5,\n",
        "#         \"weight_decay\": 1e-3,\n",
        "#         \"label_smoothing_factor\": 1e-5,\n",
        "#     }\n",
        "# )\n",
        "\n",
        "# https://stackoverflow.com/questions/59129812/how-to-avoid-cuda-out-of-memory-in-pytorch\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "study.optimize(objective, n_trials=100, callbacks=[best_model_callback])\n",
        "\n",
        "# study.best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaHTHuP9W6Dn"
      },
      "outputs": [],
      "source": [
        "!ls -lahtr 10kgnad_hf__distilbert-base-german-cased/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBZsS24YbFpy"
      },
      "source": [
        "## Hyperparameter Tuning\n",
        "\n",
        "https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer.hyperparameter_search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phuJdMD6aaLP"
      },
      "outputs": [],
      "source": [
        "# disable transformer warnings like \"Some weights of the model checkpoint ...\"\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=str(project_name),\n",
        "    report_to=[],\n",
        "    log_level=\"error\",\n",
        "    disable_tqdm=False,\n",
        "\n",
        "    evaluation_strategy=\"steps\",\n",
        "    # eval_steps=eval_steps,\n",
        "    save_strategy=\"steps\",\n",
        "    # save_steps=eval_steps,\n",
        "    # load_best_model_at_end=False,\n",
        "    # metric_for_best_model=\"eval_loss\",\n",
        "    # greater_is_better=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model_init=model_init,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_gnad10k[\"train\"],\n",
        "    eval_dataset=tokenized_gnad10k[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "\n",
        "# Default objective is the sum of all metrics\n",
        "# when metrics are provided, so we have to maximize it.\n",
        "# best = trainer.hyperparameter_search(\n",
        "#     hp_space=hp_space,\n",
        "#     compute_objective=objective,\n",
        "#     n_trials=2\n",
        "# )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "21c_10kGNAD_huggingface_basic_optuna.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPOI0aGEQDKztKJg1IO8Pgh",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9afad0219a1d41b994bf564665992083": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_73071c64cc0045a6a0f05095ba202b61",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_994ab1930d3c4724b095fc6676a7bd98",
              "IPY_MODEL_7204e2676cb6489dbb7c72305c673d5e",
              "IPY_MODEL_1441b5271ca5428db602efadfe865451"
            ]
          }
        },
        "73071c64cc0045a6a0f05095ba202b61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "994ab1930d3c4724b095fc6676a7bd98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_55662134eda24351b29d0b4c482e3192",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5cd8ed4c341844cfadc7542d9ca4916e"
          }
        },
        "7204e2676cb6489dbb7c72305c673d5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_70d1321199444dfbb2d546eb29305ed4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_553ffaaf454e4be994b4ed4144c58916"
          }
        },
        "1441b5271ca5428db602efadfe865451": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_933231612ec641caaaacf053612ca979",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2/2 [00:00&lt;00:00, 35.84it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_86772280985a488d997c4e7e24be0e48"
          }
        },
        "55662134eda24351b29d0b4c482e3192": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5cd8ed4c341844cfadc7542d9ca4916e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "70d1321199444dfbb2d546eb29305ed4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "553ffaaf454e4be994b4ed4144c58916": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "933231612ec641caaaacf053612ca979": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "86772280985a488d997c4e7e24be0e48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}