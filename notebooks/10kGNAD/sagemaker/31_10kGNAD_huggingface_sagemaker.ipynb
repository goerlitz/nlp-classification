{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8adc4051-3d2c-49f4-a8a2-0bb1eea5c338",
   "metadata": {},
   "source": [
    "# Model Training on Sagemaker\n",
    "\n",
    "* https://huggingface.co/docs/sagemaker/train\n",
    "* https://github.com/huggingface/notebooks/blob/master/sagemaker/01_getting_started_pytorch/sagemaker-notebook.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb55b173-2390-4daa-9771-c0cb8f0a72f7",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5155b954-c9c5-49d6-a915-3be9df5aa1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"distilbert-base-german-cased\"\n",
    "\n",
    "# checkpoint = \"deepset/gbert-base\"\n",
    "\n",
    "# checkpoint = \"deepset/gelectra-base\"\n",
    "\n",
    "project_name = \"10kgnad_huggingface__\" + checkpoint.replace(\"/\", \"_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c8b432-7dee-4726-98d1-62a87d63661c",
   "metadata": {},
   "source": [
    "## Sagemaker Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "429ae805-add7-4845-b015-602ac52044e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb7228d-cd89-477a-8a18-5c057f242d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c291ce4-dd34-4876-ad1a-505d228fba56",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6d4df95-7a3e-4d0a-b949-82d9575cb2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DIR=data\n",
      "2021-11-27 17:50:35 URL:https://raw.githubusercontent.com/tblock/10kGNAD/master/train.csv [24405789/24405789] -> \"data/train.csv\" [1]\n",
      "2021-11-27 17:50:37 URL:https://raw.githubusercontent.com/tblock/10kGNAD/master/test.csv [2755020/2755020] -> \"data/test.csv\" [1]\n",
      "\n",
      "2,7M Nov 27 17:50 test.csv\n",
      " 24M Nov 27 17:50 train.csv\n"
     ]
    }
   ],
   "source": [
    "%env DIR=data\n",
    "\n",
    "!mkdir -p $DIR\n",
    "!wget -nc https://github.com/tblock/10kGNAD/blob/master/train.csv?raw=true -nv -O $DIR/train.csv\n",
    "!wget -nc https://github.com/tblock/10kGNAD/blob/master/test.csv?raw=true -nv -O $DIR/test.csv\n",
    "!ls -lAh $DIR | cut -d \" \" -f 5-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8d100a1-65ab-49c0-b7d4-3a9911ce32a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-d3ae6c12d0953d9e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/goerlitz/.cache/huggingface/datasets/csv/default-d3ae6c12d0953d9e/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29059342a30642ada23bc4b0e43f28fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c6230971a04f71b8db8d9662fdcf21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/goerlitz/.cache/huggingface/datasets/csv/default-d3ae6c12d0953d9e/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b8cf01a89aa4e42a1774d067c32b4a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['labels', 'text'],\n",
      "        num_rows: 9245\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['labels', 'text'],\n",
      "        num_rows: 1028\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sport</td>\n",
       "      <td>21-Jähriger fällt wohl bis Saisonende aus. Wie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kultur</td>\n",
       "      <td>Erfundene Bilder zu Filmen, die als verloren g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Web</td>\n",
       "      <td>Der frischgekürte CEO Sundar Pichai setzt auf ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wirtschaft</td>\n",
       "      <td>Putin: \"Einigung, dass wir Menge auf Niveau vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Inland</td>\n",
       "      <td>Estland sieht den künftigen österreichischen P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       labels                                               text\n",
       "0       Sport  21-Jähriger fällt wohl bis Saisonende aus. Wie...\n",
       "1      Kultur  Erfundene Bilder zu Filmen, die als verloren g...\n",
       "2         Web  Der frischgekürte CEO Sundar Pichai setzt auf ...\n",
       "3  Wirtschaft  Putin: \"Einigung, dass wir Menge auf Niveau vo...\n",
       "4      Inland  Estland sieht den künftigen österreichischen P..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install -q --upgrade datasets\n",
    "\n",
    "import os\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "dataset = load_dataset(\"csv\",\n",
    "                       data_files={\"train\": os.getenv(\"DIR\")+\"/train.csv\",\n",
    "                                   \"test\": os.getenv(\"DIR\")+\"/test.csv\"},\n",
    "                       sep=\";\", quotechar=\"'\", names=[\"labels\", \"text\"]\n",
    "                       )\n",
    "\n",
    "print(dataset)\n",
    "display(dataset['train'].to_pandas().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cebf1e67-6487-49fe-992f-49224efa486b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec5ed68b468a40e5a55552bf34eedbf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "192e3b77d4474e478b72ec2a38347f3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def encode_labels(ds: DatasetDict):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(ds['train']['labels'])\n",
    "\n",
    "    def encode(data):\n",
    "        return {'labels': le.transform(data['labels'])}\n",
    "\n",
    "    return dataset.map(encode, batched=True), le\n",
    "\n",
    "encoded_ds, le = encode_labels(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e972731f-1188-4956-8ceb-a1417d8be309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3f5348387c41ebb0350e7c032917c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd4354065eb470eaf980b4f0611bf82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/464 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38fa474554df49ffaefbf147a6171c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/234k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9465cd495e934a9c844437f08126cd42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/468k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b76f76e9d8be4b7ab4fb3eef221cb92e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78fd64a0408d484bb34e06fb3577dd7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['attention_mask', 'input_ids', 'labels'],\n",
      "        num_rows: 9245\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['attention_mask', 'input_ids', 'labels'],\n",
      "        num_rows: 1028\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[102, 1735, 232, 19231, 693, 5844, 2134, 378, ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[102, 11806, 646, 30881, 4195, 205, 13165, 818...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[102, 351, 13236, 124, 7847, 123, 26074, 12309...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[102, 16679, 853, 224, 12205, 818, 377, 268, 5...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[102, 18600, 2671, 190, 13458, 13239, 30882, 5...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      attention_mask  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                           input_ids  labels  \n",
       "0  [102, 1735, 232, 19231, 693, 5844, 2134, 378, ...       5  \n",
       "1  [102, 11806, 646, 30881, 4195, 205, 13165, 818...       3  \n",
       "2  [102, 351, 13236, 124, 7847, 123, 26074, 12309...       6  \n",
       "3  [102, 16679, 853, 224, 12205, 818, 377, 268, 5...       7  \n",
       "4  [102, 18600, 2671, 190, 13458, 13239, 30882, 5...       1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install -q --upgrade transformers\n",
    "\n",
    "from transformers import AdamW, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize(data):\n",
    "    # return tokenizer(data['text'], truncation=True, padding=True)\n",
    "    return tokenizer(data['text'], padding='max_length', truncation=True)\n",
    "\n",
    "\n",
    "tokenized_ds = encoded_ds.map(tokenize, batched=True).remove_columns('text')\n",
    "\n",
    "print(tokenized_ds)\n",
    "display(tokenized_ds['train'].to_pandas().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f08048-8d36-4a5f-8a8f-322ae2ac90f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()  \n",
    "\n",
    "base_path = f's3://{sess.default_bucket()}/{s3_prefix}'\n",
    "\n",
    "for data in [\"train\", \"test\"]:\n",
    "    # save dataset to s3\n",
    "    input_path = f'{base_path}/{data}'\n",
    "    tokenized_ds[data].save_to_disk(input_path, fs=s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb4afde-2e82-4d95-b30f-3f68b0c55e55",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec822b31-4ae4-44ab-a74d-c2b3bc761ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters which are passed to the training job\n",
    "hyperparameters={'epochs': 1,\n",
    "                 'per_device_train_batch_size': 32,\n",
    "                 'model_name_or_path': checkpoint\n",
    "                 }\n",
    "\n",
    "\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='train.py',\n",
    "    source_dir='./scripts',\n",
    "    instance_type='ml.p3.2xlarge',\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    transformers_version='4.6',\n",
    "    pytorch_version='1.7',\n",
    "    py_version='py36',\n",
    "    hyperparameters = hyperparameters)\n",
    "\n",
    "huggingface_estimator.fit({'train': f'{base_path}/train',\n",
    "                           'test': f'{base_path}/test'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5684e869-6e73-49b1-a16b-89f18c93a4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# container image used for training job\n",
    "print(f\"container image used for training job: \\n{huggingface_estimator.image_uri}\\n\")\n",
    "\n",
    "# s3 uri where the trained model is located\n",
    "print(f\"s3 uri where the trained model is located: \\n{huggingface_estimator.model_data}\\n\")\n",
    "\n",
    "# latest training job name for this estimator\n",
    "print(f\"latest training job name for this estimator: \\n{huggingface_estimator.latest_training_job.name}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d0e7db-2e93-4bea-80ba-4f5d3422187f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the logs of the training job\n",
    "huggingface_estimator.sagemaker_session.logs_for_job(huggingface_estimator.latest_training_job.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
